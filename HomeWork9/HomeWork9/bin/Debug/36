

<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <link rel="canonical" href=""/>
    <meta name="applicable-device" content="pc">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="renderer" content="webkit"/>
    <meta name="force-rendering" content="webkit"/>
    <meta name="csdn-baidu-search" content="{}">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="report" content='{"pid":"tags","spm":"1000.2123"}'>
    <meta name="referrer" content="always">
    <meta http-equiv="Cache-Control" content="no-siteapp"/>
    <link rel="alternate" media="handheld" href="#"/>
    <meta name="shenma-site-verification" content="5a59773ab8077d4a62bf469ab966a63b_1497598848">
    <meta name="csdn-baidu-search" content=''>
    <title>卷积神经网络 - CSDN</title>
    <link href="https://g.csdnimg.cn/static/logo/favicon32.ico" rel="shortcut icon" type="image/x-icon"/>
    <meta name="keywords" content="卷积神经网络">
    <meta name="description" content="csdn已为您找到关于卷积神经网络相关内容，包含卷积神经网络相关文档代码介绍、相关教程视频课程，以及相关卷积神经网络问答内容。为您解决当下相关问题，如果想了解更详细卷积神经网络内容，请点击详情链接进行了解，或者注册账号与客服人员联系给您提供相关内容的帮助，以下是为您准备的相关内容。">
    <meta name="csdnFooter" content='{"type": "3", "el": ".persion_article"}'>
    <link rel="stylesheet" href="//csdnimg.cn/public/common/libs/bootstrap/css/bootstrap.css">
    <script src="https://g.csdnimg.cn/??lib/jquery/1.12.4/jquery.min.js"></script>
    <script src="https://g.csdnimg.cn/collection-box/2.0.4/collection-box.js"></script>
    <script src="https://g.csdnimg.cn/login-box/1.1.4/login-box.js" type="text/javascript"></script>
    <script>
        window.csdn = window.csdn ? window.csdn : {};
        window.csdn.toolbarIsBlack = false;
    </script>
    <script src="//g.csdnimg.cn/fixed-sidebar/1.1.6/fixed-sidebar.js" type="text/javascript"></script>
    <script src="//g.csdnimg.cn/common/csdn-report/report.js" type="text/javascript"></script>
    <script src="//g.csdnimg.cn/baidu-search/1.0.9/baidu-search.js" type="text/javascript"></script>
    <style>
        .MathJax, .MathJax_Message, .MathJax_Preview {
            display: none
        }
    </style>
</head>
<body>
<link rel="stylesheet" href="https://csdnimg.cn/public/common/toolbar/content_toolbar_css/content_toolbar.css">
<script id="toolbar-tpl-scriptId" src="https://g.csdnimg.cn/common/csdn-toolbar/csdn-toolbar.js" type="text/javascript">
</script>

<link rel="stylesheet" href='https://csdnimg.cn/release/aggregate/css/pc-949af63466.min.css'>
<link rel="stylesheet" href='https://csdnimg.cn/release/aggregate/css/pc_common-c5dae373c3.min.css'>
<style type="text/css">
    .qa-item .qa-item-ft .icon {
        display: inline-block;
        width: 16px;
        height: 16px;
        vertical-align: sub;
        margin-right: 6px;
        background-repeat: no-repeat;
        background-image: url(data:image/png;base64,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);
        background-size: 32px 48px;
    }
</style>
<script type="text/javascript">
    var mod_page = "1"
    var mod_keywords = "卷积神经网络"

    var insert_baidu_first =
    true
    var insert_baidu_count = 10

        //搜索热词
        window.toolbarSearchExt = JSON.stringify({
            tag: [mod_keywords],
            title: "",
            landingWord: [],
            queryWord: ""
        })
</script>

    <script>
        var isContainBaidu = "1";
    </script>


<div class="main-tag-container">
    <div class="con-l-tag">
        <div class="top-banner hide"></div>
            <script>
                window.baiduCollection = {
                    title: "卷积神经网络 - CSDN",
                    desc: "csdn已为您找到关于卷积神经网络相关内容，包含卷积神经网络相关文档代码介绍、相关教程视频课程，以及相关卷积神经网络问答内容。为您解决当下相关问题，如果想了解更详细卷积神经网络内容，请点击详情链接进行了解，或者注册账号与客服人员联系给您提供相关内容的帮助，以下是为您准备的相关内容。",
                }
            </script>
            <div class="baike-container">
                <div class="baike-view">
                    <div class="baike-l">
                            <img class="inner-img" src="https://img-search.csdnimg.cn/bkimg/48540923dd54564e223d3494bdde9c82d0584fc7/aggregate_page" alt="">
                    </div>
                    <div class="baike-r">
                        <div class="b-title">
                            <span>卷积神经网络</span>
                            <a class="collect-a" href="javascript:;">订阅</a>
                        </div>
                        <div class="b-description baike-intro" id="baike-intro">
                            <span>卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一
[1-2] 
。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”
[3] 
。对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络
[4] 
；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于计算机视觉、自然语言处理等领域
[2] 
。卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征，例如像素和音频进行学习、有稳定的效果且对数据没有额外的特征工程（feature engineering）要求
[1-2] 
。</span>
                            <a href="javascript:;">
                                <span>展开全文</span>
                                <img src='https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png' alt="">
                            </a>
                        </div>
                        <div class="b-description baike-all" id="baike-all">
                            <span>卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一
[1-2] 
。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）”
[3] 
。对卷积神经网络的研究始于二十世纪80至90年代，时间延迟网络和LeNet-5是最早出现的卷积神经网络
[4] 
；在二十一世纪后，随着深度学习理论的提出和数值计算设备的改进，卷积神经网络得到了快速发展，并被应用于计算机视觉、自然语言处理等领域
[2] 
。卷积神经网络仿造生物的视知觉（visual perception）机制构建，可以进行监督学习和非监督学习，其隐含层内的卷积核参数共享和层间连接的稀疏性使得卷积神经网络能够以较小的计算量对格点化（grid-like topology）特征，例如像素和音频进行学习、有稳定的效果且对数据没有额外的特征工程（feature engineering）要求
[1-2] 
。</span>
                        </div>
                    </div>
                </div>
                        <div class="basic-info-wrap">
                            <div class="title-basic">信息</div>
                            <div class="basic-info">
                                    <dl class="basicInfo-block basicInfo-right">
                                            <dt> 提出时间</dt>
                                            <dd>1987-1989年
[5]</dd>
                                            <dt>外文名</dt>
                                            <dd>Convolutional Neural Network, CNN</dd>
                                            <dt>提出者</dt>
                                            <dd>Yann LeCun，Wei Zhang，Alexander Waibel 等</dd>
                                            <dt>类    型</dt>
                                            <dd>机器学习算法，神经网络算法</dd>
                                    </dl>
                                    <dl class="basicInfo-block basicInfo-left">
                                            <dt>中文名</dt>
                                            <dd>卷积神经网络</dd>
                                            <dt>学    科</dt>
                                            <dd>人工智能</dd>
                                            <dt>应    用</dt>
                                            <dd>计算机视觉，自然语言处理</dd>
                                    </dl>
                            </div>
                        </div>
                    <div class="basic-info-content">
                        <div class="title-basic">卷积神经网络历史</div>
                        <div class="conetent">
                            



对卷积神经网络的研究可追溯至日本学者福岛邦彦（Kunihiko Fukushima）提出的neocognitron模型。在其1979
[7-8] 
和1980年
[9] 
发表的论文中，福岛仿造生物的视觉皮层（visual cortex）设计了以“neocognitron”命名的神经网络。neocognitron是一个具有深度结构的神经网络，并且是最早被提出的深度学习算法之一
[10] 
，其隐含层由S层（Simple-layer）和C层（Complex-layer）交替构成。其中S层单元在感受野（receptive field）内对图像特征进行提取，C层单元接收和响应不同感受野返回的相同特征
[9] 
。neocognitron的S层-C层组合能够进行特征提取和筛选，部分实现了卷积神经网络中卷积层（convolution layer）和池化层（pooling layer）的功能，被认为是启发了卷积神经网络的开创性研究
[11] 
。第一个卷积神经网络是1987年由Alexander Waibel等提出的时间延迟网络（Time Delay Neural Network, TDNN）
[12] 
。TDNN是一个应用于语音识别问题的卷积神经网络，使用FFT预处理的语音信号作为输入，其隐含层由2个一维卷积核组成，以提取频率域上的平移不变特征
[13] 
。由于在TDNN出现之前，人工智能领域在反向传播算法（Back-Propagation, BP）的研究中取得了突破性进展
[14] 
，因此TDNN得以使用BP框架内进行学习。在原作者的比较试验中，TDNN的表现超过了同等条件下的隐马尔可夫模型（Hidden Markov Model, HMM），而后者是二十世纪80年代语音识别的主流算法
[13] 
。1988年，Wei Zhang提出了第一个二维卷积神经网络：平移不变人工神经网络（SIANN），并将其应用于检测医学影像
[3] 
。独立于Zhang (1988)，Yann LeCun在1989年同样构建了应用于计算机视觉问题的卷积神经网络，即LeNet的最初版本
[5] 
。LeNet包含两个卷积层，2个全连接层，共计6万个学习参数，规模远超TDNN和SIANN，且在结构上与现代的卷积神经网络十分接近
[11] 
。LeCun (1989)
[5] 
对权重进行随机初始化后使用了随机梯度下降（Stochastic Gradient Descent, SGD）进行学习，这一策略被其后的深度学习研究所保留。此外，LeCun (1989)在论述其网络结构时首次使用了“卷积”一词
[5] 
，“卷积神经网络”也因此得名。LeCun (1989)
[5] 
的工作在1993年由贝尔实验室（AT&T Bell Laboratories）完成代码开发并被部署于NCR（National Cash Register Coporation）的支票读取系统
[11] 
。但总体而言，由于数值计算能力有限、学习样本不足，加上同一时期以支持向量机（Support Vector Machine, SVM）为代表的核学习（kernel learning）方法的兴起，这一时期为各类图像处理问题设计的卷积神经网络停留在了研究阶段，应用端的推广较少
[2] 
。在LeNet的基础上，1998年Yann LeCun及其合作者构建了更加完备的卷积神经网络LeNet-5并在手写数字的识别问题中取得成功
[15] 
。LeNet-5沿用了LeCun (1989) 的学习策略并在原有设计中加入了池化层对输入特征进行筛选
[15] 
。LeNet-5及其后产生的变体定义了现代卷积神经网络的基本结构，其构筑中交替出现的卷积层-池化层被认为能够提取输入图像的平移不变特征
[16] 
。LeNet-5的成功使卷积神经网络的应用得到关注，微软在2003年使用卷积神经网络开发了光学字符读取（Optical Character Recognition, OCR）系统
[17] 
。其它基于卷积神经网络的应用研究也得到展开，包括人像识别
[18] 
、手势识别
[19] 
等。在2006年深度学习理论被提出后
[20] 
，卷积神经网络的表征学习能力得到了关注，并随着数值计算设备的更新得到发展
[2] 
。自2012年的AlexNet
[21] 
开始，得到GPU计算集群支持的复杂卷积神经网络多次成为ImageNet大规模视觉识别竞赛（ImageNet Large Scale Visual Recognition Challenge, ILSVRC）
[22] 
的优胜算法，包括2013年的ZFNet
[23] 
、2014年的VGGNet、GoogLeNet
[24] 
和2015年的ResNet
[25] 
。
                        </div>
                    </div>
                <div class="hide-content" href="javascript:;">收起全文 <img src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png"
                                                                        alt=""></div>
            </div>


        <div class="content-list">
            <div class="ag-tab-bar">
                <div class="ag-tb-lt">
                    <div class="tab-hd-item active" type="hot">精华内容</div>
                    <div class="tab-hd-item" type="download"
                         data-report-click='{"spm":"3001.5558","extend1":"聚合页下载TAB"}'>下载资源
                    </div>
                    <div class="tab-hd-item" type="qa" data-report-click='{"spm":"3001.5197","extend1":"聚合页问答TAB"}'>
                        问答
                    </div>
                </div>
                <div class="ag-tb-rt">
                    <div class="icon-link" data-report-query="spm=3001.5142"
                         data-report-click='{"spm":"3001.5142","dest":"https://so.csdn.net/search?mode=1&q=卷积神经网络&spm=1000.2123.3001.5142"}'>
                    </div>
                    <div class="tab-rt-btn">
                        <a class="btn-ask" href="javascript:void(0);"
                           data-report-click='{"spm":"3001.5198","dest":"https://ask.csdn.net/?word=卷积神经网络"}'>我要提问</a>
                    </div>
                </div>
            </div>

            <div class="tab-bd wrap-main-list active">
                <ul class="main-ul">
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/xinzhangyanxiang/article/details/41596663"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/xinzhangyanxiang/article/details/41596663?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-1-41596663.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-1-41596663.pc_agg_rank_aggregation\&quot;}","index": "0","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/xinzhangyanxiang/article/details/41596663?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-1-41596663.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-1-41596663.pc_agg_rank_aggregation\&quot;}","index": "0","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-1-41596663.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2><em>卷积神经网络</em></h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2014-11-29 16:20:41</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    自今年七月份以来，一直在实验室负责<em>卷积神经网络</em>（Convolutional Neural Network，CNN），期间配置和使用过theano和cuda-convnet、cuda-convnet2。为了增进CNN的理解和使用，特写此博文，以其与人交流，互有增益。...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"><h1>卷积神经网络</h1>

<p>转载请注明：http://blog.csdn.net/stdcoutzyx/article/details/41596663</p>

<p>自今年七月份以来，一直在实验室负责卷积神经网络（Convolutional Neural Network，CNN），期间配置和使用过theano和cuda-convnet、cuda-convnet2。为了增进CNN的理解和使用，特写此博文，以其与人交流，互有增益。正文之前，先说几点自己对于CNN的感触。先明确一点就是，Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。</p>

<ul><li>
	<p>第一点，在学习Deep learning和CNN之前，总以为它们是很了不得的知识，总以为它们能解决很多问题，学习了之后，才知道它们不过与其他机器学习算法如svm等相似，仍然可以把它当做一个分类器，仍然可以像使用一个黑盒子那样使用它。</p>
	</li>
	<li>
	<p>第二点，Deep Learning强大的地方就是可以利用网络中间某一层的输出当做是数据的另一种表达，从而可以将其认为是经过网络学习到的特征。基于该特征，可以进行进一步的相似度比较等。</p>
	</li>
	<li>
	<p>第三点，Deep Learning算法能够有效的关键其实是大规模的数据，这一点原因在于每个DL都有众多的参数，少量数据无法将参数训练充分。</p>
	</li>
</ul><p>接下来话不多说，直接奔入主题开始CNN之旅。</p>

<h1>1. 神经网络</h1>

<p>首先介绍神经网络，这一步的详细可以参考资源1。简要介绍下。神经网络的每个单元如下：</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014113905946.png" /></p>

<p> </p>

<p>其对应的公式如下：</p>

<p style="text-align:center;"><img alt="" height="23" src="https://img-blog.csdnimg.cn/20201014113921776.png" width="326" /></p>

<p> </p>

<p>其中，该单元也可以被称作是Logistic回归模型。当将多个单元组合起来并具有分层结构时，就形成了神经网络模型。下图展示了一个具有一个隐含层的神经网络。</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014113935943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<p> </p>

<p>其对应的公式如下：</p>

<p style="text-align:center;"><img alt="" height="128" src="https://img-blog.csdnimg.cn/20201014113952379.png" width="474" /></p>

<p> </p>

<p>比较类似的，可以拓展到有2,3,4,5，…个隐含层。</p>

<p>神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。关于训练算法，本文暂不涉及。</p>

<h1>2 卷积神经网络</h1>

<p>在图像处理中，往往把图像表示为像素的向量，比如一个1000×1000的图像，可以表示为一个1000000的向量。在上一节中提到的神经网络中，如果隐含层数目与输入层一样，即也是1000000时，那么输入层到隐含层的参数数据为1000000×1000000=10^12，这样就太多了，基本没法训练。所以图像处理要想练成神经网络大法，必先减少参数加快速度。就跟辟邪剑谱似的，普通人练得很挫，一旦自宫后内力变强剑法变快，就变的很牛了。</p>

<h2>2.1 局部感知</h2>

<p>卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。如下图所示：左图为全连接，右图为局部连接。</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114009559.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<p> </p>

<p>在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的万分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。</p>

<h2>2.2 参数共享</h2>

<p>但其实这样的话参数仍然过多，那么就启动第二级神器，即权值共享。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100了。</p>

<p>怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。</p>

<p>更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8x8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8x8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8x8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>

<p>如下图所示，展示了一个3×<em>3的卷积核在5×</em>5的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来。</p>

<p style="text-align:center;"><img alt="" height="384" src="https://img-blog.csdnimg.cn/20201014114046469.gif" width="526" /></p>

<p> </p>

<h2>2.3 多卷积核</h2>

<p>上面所述只有100个参数时，表明只有1个10*10的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示：</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114101735.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<p> </p>

<p>上图右，不同颜色表明不同的卷积核。每个卷积核都会将图像生成为另一幅图像。比如两个卷积核就可以将生成两幅图像，这两幅图像可以看做是一张图像的不同的通道。如下图所示，下图有个小错误，即将w1改为w0，w2改为w1即可。下文中仍以w1和w2称呼它们。</p>

<p>下图展示了在四个通道上的卷积操作，有两个卷积核，生成两个通道。其中需要注意的是，四个通道上每个通道对应一个卷积核，先将w2忽略，只看w1，那么在w1的某位置（i,j）处的值，是由四个通道上（i,j）处的卷积结果相加然后再取激活函数值得到的。</p>

<p style="text-align:center;"><img alt="" height="58" src="https://img-blog.csdnimg.cn/20201014114118243.png" width="408" /></p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114132619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<p> </p>

<p> </p>

<p> </p>

<p>所以，在上图由4个通道卷积得到2个通道的过程中，参数的数目为4×2×2×2个，其中4表示4个通道，第一个2表示生成2个通道，最后的2×2表示卷积核大小。</p>

<h2>2.4 Down-pooling</h2>

<p>在通过卷积获得了特征 (features) 之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如 softmax 分类器，但这样做面临计算量的挑战。例如：对于一个 96X96 像素的图像，假设我们已经学习得到了400个定义在8X8输入上的特征，每一个特征和图像卷积都会得到一个 (96 − 8 + 1) × (96 − 8 + 1) = 7921 维的卷积特征，由于有 400 个特征，所以每个样例 (example) 都会得到一个 7921 × 400 = 3,168,400 维的卷积特征向量。学习一个拥有超过 3 百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。</p>

<p>为了解决这个问题，首先回忆一下，我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)。这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。这种聚合的操作就叫做池化 (pooling)，有时也称为平均池化或者最大池化 (取决于计算池化的方法)。</p>

<p style="text-align:center;"><img alt="" height="461" src="https://img-blog.csdnimg.cn/20201014114200208.gif" width="800" /></p>

<p> </p>

<p>至此，卷积神经网络的基本结构和原理已经阐述完毕。</p>

<h2>2.5 多层卷积</h2>

<p>在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化。</p>

<h1>3 ImageNet-2010网络结构</h1>

<p>ImageNet LSVRC是一个图片分类的比赛，其训练集包括127W+张图片，验证集有5W张图片，测试集有15W张图片。本文截取2010年Alex Krizhevsky的CNN结构进行说明，该结构在2010年取得冠军，top-5错误率为15.3%。值得一提的是，在今年的ImageNet LSVRC比赛中，取得冠军的GoogNet已经达到了top-5错误率6.67%。可见，深度学习的提升空间还很巨大。</p>

<p>下图即为Alex的CNN结构图。需要注意的是，该模型采用了2-GPU并行结构，即第1、2、4、5卷积层都是将模型参数分为2部分进行训练的。在这里，更进一步，并行结构分为数据并行与模型并行。数据并行是指在不同的GPU上，模型结构相同，但将训练数据进行切分，分别训练得到不同的模型，然后再将模型进行融合。而模型并行则是，将若干层的模型参数进行切分，不同的GPU上使用相同的数据进行训练，得到的结果直接连接作为下一层的输入。</p>

<p> </p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114217422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<pre>
<code>上图模型的基本参数为：
</code></pre>

<ul><li>输入：224×224大小的图片，3通道</li>
	<li>第一层卷积：11×11大小的卷积核96个，每个GPU上48个。</li>
	<li>第一层max-pooling：2×2的核。</li>
	<li>第二层卷积：5×5卷积核256个，每个GPU上128个。</li>
	<li>第二层max-pooling：2×2的核。</li>
	<li>第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。</li>
	<li>第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。</li>
	<li>第五层卷积：3×3的卷积核256个，两个GPU上个128个。</li>
	<li>第五层max-pooling：2×2的核。</li>
	<li>第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。</li>
	<li>第二层全连接：4096维</li>
	<li>Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。</li>
</ul><h1>4 DeepID网络结构</h1>

<p>DeepID网络结构是香港中文大学的Sun Yi开发出来用来学习人脸特征的卷积神经网络。每张输入的人脸被表示为160维的向量，学习到的向量经过其他模型进行分类，在人脸验证试验上得到了97.45%的正确率，更进一步的，原作者改进了CNN，又得到了99.15%的正确率。</p>

<p>如下图所示，该结构与ImageNet的具体参数类似，所以只解释一下不同的部分吧。</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114234977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" /></p>

<p> </p>

<p>上图中的结构，在最后只有一层全连接层，然后就是softmax层了。论文中就是以该全连接层作为图像的表示。在全连接层，以第四层卷积和第三层max-pooling的输出作为全连接层的输入，这样可以学习到局部的和全局的特征。</p>

<h1>5 参考资源</h1>

<ul><li>[1] http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B 栀子花对Stanford深度学习研究团队的深度学习教程的翻译</li>
	<li>[2] http://blog.csdn.net/zouxy09/article/details/14222605 csdn博主zouxy09深度学习教程系列</li>
	<li>[3] http://deeplearning.net/tutorial/ theano实现deep learning</li>
	<li>[4] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>
	<li>[5] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.</li>
</ul><p>更多内容欢迎关注微信公众后【雨石记】。</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdnimg.cn/20201014114310118.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3hpbnpoYW5neWFueGlhbmc=,size_16,color_FFFFFF,t_70" width="600" /></p>

<p> </p>
</pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                                        <a href="javascript:;" class="readmore_btn"
                                           data-linkUrl="https://blog.csdn.net/xinzhangyanxiang/article/details/41596663"
                                           data-so-type="blog">
                                            <span>展开全文</span>
                                            <img src='https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png' alt="">
                                        </a>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/v_JULY_v/article/details/51812459"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-2-51812459.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-2-51812459.pc_agg_rank_aggregation\&quot;}","index": "1","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/v_JULY_v/article/details/51812459?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-2-51812459.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-2-51812459.pc_agg_rank_aggregation\&quot;}","index": "1","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-2-51812459.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2>CNN笔记：通俗理解<em>卷积神经网络</em></h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2016-07-02 22:14:50</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    通俗理解<em>卷积神经网络</em>（cs231n与5月dl班课程笔记）                1 前言   2012年我在北京组织过8期machine learning读书会，那时“机器学习”非常火，很多人都对其抱有巨大的热情。当我2013年再次来到北京时，有...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"><h2>               通俗理解卷积神经网络（cs231n与5月dl班课程笔记）</h2>

<p> </p>

<p> </p>

<p> </p>

<h2>1 前言</h2>

<p>    2012年我在北京组织过8期machine learning读书会，那时“机器学习”非常火，很多人都对其抱有巨大的热情。当我2013年再次来到北京时，有一个词似乎比“机器学习”更火，那就是“深度学习”。</p>

<p>    本博客内写过一些机器学习相关的文章，但上一篇技术文章“LDA主题模型”还是写于2014年11月份，毕竟自2015年开始创业做在线教育后，太多的杂事、琐碎事，让我一直想再写点技术性文章但每每恨时间抽不开。然由于公司在不断开机器学习、深度学习等相关的在线课程，耳濡目染中，总会顺带着学习学习。</p>

<p>    我虽不参与讲任何课程（我所在公司“<a href="https://www.julyedu.com/" rel="nofollow">七月在线</a>”的所有在线课程都是由目前讲师团队的100多位讲师讲），但依然可以用最最小白的方式 把一些初看复杂的东西抽丝剥茧的通俗写出来。这算重写技术博客的价值所在。</p>

<p>    在dl中，有一个很重要的概念，就是卷积神经网络CNN，基本是入门dl必须搞懂的东西。本文基本根据斯坦福的机器学习公开课、cs231n、与七月在线寒小阳讲的5月<a href="http://www.julyedu.com/course/getDetail/112" rel="nofollow">dl班</a>所写，是一篇课程笔记。</p>

<p>    一开始本文只是想重点讲下CNN中的卷积操作具体是怎么计算怎么操作的，但后面不断补充，包括增加不少自己的理解，故写成了关于卷积神经网络的通俗导论性的文章。有何问题，欢迎不吝指正。</p>

<p> </p>

<p> </p>

<h2>2 人工神经网络</h2>

<h3>2.1 神经元</h3>

<p>    神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。</p>

<p>    举个手写识别的例子，给定一个未知数字，让神经网络识别是什么数字。此时的神经网络的输入由一组被输入图像的像素所激活的输入神经元所定义。在通过非线性激活函数进行非线性变换后，神经元被激活然后被传递到其他神经元。重复这一过程，直到最后一个输出神经元被激活。从而识别当前数字是什么字。</p>

<p>    神经网络的每个神经元如下</p>

<blockquote>
<blockquote>
<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160716131107406" /></p>
</blockquote>
</blockquote>
</blockquote>

<p>    基本wx + b的形式，其中</p>

<ul><li><span style="color:#362e2b;"><span style="color:#362e2b;"><img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" />、<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" />表示</span>输入向量</span></li>
	<li><span style="color:#362e2b;"><img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" />、<img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" />为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重</span></li>
	<li><span style="color:#362e2b;"><span style="color:#362e2b;">b为偏置</span><span style="color:#362e2b;">bias</span></span></li>
	<li><span style="color:#362e2b;">g(z</span><span style="color:#362e2b;">) 为激活函数</span></li>
	<li><span style="color:#362e2b;">a 为输出</span></li>
</ul><p>    如果只是上面这样一说，估计以前没接触过的十有八九又必定迷糊了。事实上，上述简单模型可以追溯到20世纪50/60年代的感知器，可以把感知器理解为一个根据不同因素、以及各个因素的重要性程度而做决策的模型。</p>

<p>    举个例子，这周末北京有一草莓音乐节，那去不去呢？决定你是否去有二个因素，这二个因素可以对应二个输入，分别用x1、x2表示。此外，这二个因素对做决策的影响程度不一样，各自的影响程度用权重w1、w2表示。一般来说，音乐节的演唱嘉宾会非常影响你去不去，唱得好的前提下 即便没人陪同都可忍受，但如果唱得不好还不如你上台唱呢。所以，我们可以如下表示：</p>

<ul><li><img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" />：是否有喜欢的演唱嘉宾。<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> = 1 你喜欢这些嘉宾，<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> = 0 你不喜欢这些嘉宾。嘉宾因素的权重<img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" /> = 7</li>
	<li><img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" />：是否有人陪你同去。<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 1 有人陪你同去，<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 0 没人陪你同去。是否有人陪同的权重<img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" /> = 3。</li>
</ul><p>    这样，咱们的决策模型便建立起来了：g(z) = g( <img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> + b )，g表示激活函数，这里的b可以理解成 为更好达到目标而做调整的偏置项。</p>

<p>    一开始为了简单，人们把激活函数定义成一个线性函数，即对于结果做一个线性变化，比如一个简单的线性激活函数是g(z) = z，输出都是输入的线性变换。后来实际应用中发现，线性激活函数太过局限，于是人们引入了非线性激活函数。</p>

<h3><span style="color:#362e2b;"><span style="color:#362e2b;">2.2 激活函数</span></span></h3>

<p><span style="color:#362e2b;"><span style="color:#362e2b;">    常用的非线性激活函数有sigmoid、<span style="color:#362e2b;">tanh</span>、</span></span><span style="color:#362e2b;"><span style="color:#362e2b;">relu</span>等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的<span style="color:#362e2b;">sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。</span></span></p>

<p><span style="color:#362e2b;">    <span style="color:#362e2b;">sigmoid的函数</span></span><span style="color:#362e2b;">表达式如下</span></p>

<blockquote>
<blockquote>
<blockquote>
<blockquote>
<blockquote><img alt="" class="has" src="https://img-blog.csdn.net/20160703105637734" /></blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

<p><span style="color:#362e2b;">    其中z是一个线性组合，比如z可以等于：b <span style="color:#333333;">+ <img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /><span style="color:#333333;"><span style="color:#333333;">。</span></span></span>通过代入很大的正数或很小的负数到<span style="color:#362e2b;">g(z)</span>函数中可知，其结果趋近于0或1</span><span style="color:#362e2b;">。</span></p>

<p><span style="color:#362e2b;">    因此，sigmoid函数g(z</span><span style="color:#362e2b;">)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）：</span></p>

<blockquote>
<blockquote>
<blockquote>
<p><span style="color:#362e2b;"><span style="color:#362e2b;"><img alt="" class="has" src="https://img-blog.csdn.net/20160703105432793" /></span></span></p>
</blockquote>
</blockquote>
</blockquote>

<p>    也就是说，<strong><span style="color:#362e2b;">sigmoid函数的</span>功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，<span style="color:#362e2b;">g(z)</span>会趋近于1，而z是非常小的负数时，则g(z)会趋近于0</strong>。</p>

<p>    压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。</p>

<p>    举个例子，如下图（图引自<span style="color:#333333;">Stanford机器学习公开课</span>）</p>

<blockquote>
<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdnimg.cn/20190218213452278.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92,size_16,color_FFFFFF,t_70" /></p>
</blockquote>
</blockquote>

<p><span style="color:#362e2b;">    z = b</span><span style="color:#333333;"> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" />*<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" />，其中b为偏置项 假定取-30，<img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" />、<img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" />都取为20</span></p>

<ul><li><span style="color:#333333;">如果<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> = 0 <img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 0，则z = -30，g(z) = 1/( 1 + e^-z </span><span style="color:#333333;">)趋近于0。此外，从上图<span style="color:#362e2b;">sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0</span></span></li>
	<li><span style="color:#362e2b;">如果<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> = 0 <img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 1，或<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> =1 <img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 0，则z = <span style="color:#362e2b;">b</span><span style="color:#333333;"> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" /><span style="color:#333333;">*</span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /><span style="color:#333333;"> + </span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" /><span style="color:#333333;">*</span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = -30 + 20 = -10，同样，<span style="color:#362e2b;">g(z)的值趋近于0</span></span></span></li>
	<li><span style="color:#362e2b;">如果<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /> = 1 <img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = 1，则<span style="color:#362e2b;">z = </span><span style="color:#362e2b;">b</span><span style="color:#333333;"> + <img alt="" class="has" src="https://img-blog.csdn.net/20160720151620525" /><span style="color:#333333;">*</span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" /><span style="color:#333333;"> + </span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151633098" /><span style="color:#333333;">*</span><img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" /> = -30 + 20*1 + 20*1 = 10，此时，g(z</span></span><span style="color:#333333;">)趋近于1。</span></li>
</ul><p>    换言之，只有<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" />和<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" />都取1的时候，<span style="color:#362e2b;"><span style="color:#333333;">g(z</span></span><span style="color:#333333;">)<span style="color:#333333;">→1，判定为正样本；<img alt="" class="has" src="https://img-blog.csdn.net/20160720151554838" />或<img alt="" class="has" src="https://img-blog.csdn.net/20160720151607869" />取0的时候，g(z)<span style="color:#333333;">→0，判定为负样本</span></span>，</span>如此达到分类的目的。</p>

<h3>2.3 神经网络</h3>

<p>    将下图的这种单个神经元</p>

<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703140734967" /></p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>

<p>    组织在一起，便形成了神经网络。下图便是一个三层神经网络结构</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703140745657" /></p>
</blockquote>

<p>    上图中最左边的原始输入信息称之为输入层，最右边的神经元称之为输出层（上图中输出层只有一个神经元），中间的叫隐藏层。</p>

<p>    啥叫输入层、输出层、隐藏层呢？</p>

<ul><li>输入层（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。</li>
	<li>输出层（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。</li>
	<li>隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。</li>
</ul><p>    同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。比如下图中间隐藏层来说，隐藏层的3个神经元a1、a2、a3皆各自接受来自多个不同权重的输入（因为有x1、x2、x3这三个输入，所以a1 a2 a3都会接受x1 x2 x3各自分别赋予的权重，即几个输入则几个权重），接着，a1、a2、a3又在自身各自不同权重的影响下 成为的输出层的输入，最终由输出层输出最终结果。</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703110336151" /></p>

<p>    上图（图引自<span style="color:#333333;">Stanford机器学习公开课</span>）中</p>

<ul><li><img alt="" class="has" src="https://img-blog.csdn.net/20160703112204318" />表示第j层第i个单元的激活函数/神经元</li>
	<li><img alt="" class="has" src="https://img-blog.csdn.net/20160703112227254" />表示从第j层映射到第j+1层的控制函数的权重矩阵 </li>
</ul><p>    此外，<span style="color:#333333;">输入层和隐藏层都存在一个偏置（bias unit)，所以上图中也</span>增加了偏置项：x0、a0。针对上图，有如下公式</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703112107755" /></p>
</blockquote>

<p>    此外，上文中讲的都是一层隐藏层，但实际中也有多层隐藏层的，即输入层和输出层中间夹着数层隐藏层，层和层之间是全连接的结构，同一层的神经元之间没有连接。</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703113851013" /></p>

<p> </p>

<p> </p>

<h2>3 卷积神经网络之层级结构</h2>

<p>   <a href="http://cs231n.github.io/convolutional-networks/#overview" rel="nofollow">cs231n</a>课程里给出了卷积神经网络各个层级结构，如下图</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160702205047459" /></p>

<p>    上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车</p>

<p>    所以</p>

<ul><li>最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。</li>
</ul><p>    中间是</p>

<ul><li>CONV：卷积计算层，线性乘积 求和。</li>
	<li>RELU：激励层，上文2.2节中有提到：ReLU是激活函数的一种。</li>
	<li>POOL：池化层，简言之，即取区域平均或最大。</li>
</ul><p>    最右边是</p>

<ul><li>FC：全连接层</li>
</ul><p>    这几个部分中，卷积计算层是CNN的核心，下文将重点阐述。</p>

<h2><br />
4 CNN之卷积计算层</h2>

<p>4.1 CNN怎么进行识别<br />
   简言之，当我们给定一个"X"的图案，计算机怎么识别这个图案就是“X”呢？一个可能的办法就是计算机存储一张标准的“X”图案，然后把需要识别的未知图案跟标准"X"图案进行比对，如果二者一致，则判定未知图案即是一个"X"图案。<br /><br />
   而且即便未知图案可能有一些平移或稍稍变形，依然能辨别出它是一个X图案。如此，CNN是把未知图案和标准X图案一个局部一个局部的对比，如下图所示 [图来自参考文案25]<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy5vc3MtY24tYmVpamluZy5hbGl5dW5jcy5jb20vcXVlc2Jhc2U2NDE1MzE3MjY3NzYzODk1MjI4My45?x-oss-process=image/format,png" /><br /><br />
而未知图案的局部和标准X图案的局部一个一个比对时的计算过程，便是卷积操作。卷积计算结果为1表示匹配，否则不匹配。<br /><br />
具体而言，为了确定一幅图像是包含有"X"还是"O"，相当于我们需要判断它是否含有"X"或者"O"，并且假设必须两者选其一，不是"X"就是"O"。</p>

<p><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MjY2XzE0NS5wbmc?x-oss-process=image/format,png" /><br /><br />
理想的情况就像下面这个样子：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4Mjc1XzgyNC5wbmc?x-oss-process=image/format,png" /><br />
标准的"X"和"O"，字母位于图像的正中央，并且比例合适，无变形<br /><br />
对于计算机来说，只要图像稍稍有一点变化，不是标准的，那么要解决这个问题还是不是那么容易的：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MjgyXzM2Ny5wbmc?x-oss-process=image/format,png" /><br /><br />
计算机要解决上面这个问题，一个比较天真的做法就是先保存一张"X"和"O"的标准图像（就像前面给出的例子），然后将其他的新给出的图像来和这两张标准图像进行对比，看看到底和哪一张图更匹配，就判断为哪个字母。<br /><br />
但是这么做的话，其实是非常不可靠的，因为计算机还是比较死板的。在计算机的“视觉”中，一幅图看起来就像是一个二维的像素数组（可以想象成一个棋盘），每一个位置对应一个数字。在我们这个例子当中，像素值"1"代表白色，像素值"-1"代表黑色。<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MjkyXzUzOC5wbmc?x-oss-process=image/format,png" /><br /><br />
当比较两幅图的时候，如果有任何一个像素值不匹配，那么这两幅图就不匹配，至少对于计算机来说是这样的。<br /><br />
对于这个例子，计算机认为上述两幅图中的白色像素除了中间的3*3的小方格里面是相同的，其他四个角上都不同：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4Mjk4XzMxOC5wbmc?x-oss-process=image/format,png" /><br /><br />
因此，从表面上看，计算机判别右边那幅图不是"X"，两幅图不同，得出结论：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzA0XzUxNi5wbmc?x-oss-process=image/format,png" /><br /><br />
但是这么做，显得太不合理了。理想的情况下，我们希望，对于那些仅仅只是做了一些像平移，缩放，旋转，微变形等简单变换的图像，计算机仍然能够识别出图中的"X"和"O"。就像下面这些情况，我们希望计算机依然能够很快并且很准的识别出来：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzExXzczMy5wbmc?x-oss-process=image/format,png" /><br /><br />
这也就是CNN出现所要解决的问题。<br /><br />
Features<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzI0XzY4Mi5wbmc?x-oss-process=image/format,png" /><br /><br />
对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。<br /><br />
每一个feature就像是一个小图（就是一个比较小的有值的二维数组）。不同的Feature匹配图像中不同的特征。在字母"X"的例子中，那些由对角线和交叉线组成的features基本上能够识别出大多数"X"所具有的重要特征。<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzMzXzc4Ny5wbmc?x-oss-process=image/format,png" /><br /><br />
这些features很有可能就是匹配任何含有字母"X"的图中字母X的四个角和它的中心。那么具体到底是怎么匹配的呢？如下：<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzQ3Xzk5OC5wbmc?x-oss-process=image/format,png" /><br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzUzXzg4OC5wbmc?x-oss-process=image/format,png" /><br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzYwXzQ4Ny5wbmc?x-oss-process=image/format,png" /><br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4MzY3XzUxNy5wbmc?x-oss-process=image/format,png" /><br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4Mzc0XzM2My5wbmc?x-oss-process=image/format,png" /><br /><br />
看到这里是不是有了一点头目呢。但其实这只是第一步，你知道了这些Features是怎么在原图上面进行匹配的。但是你还不知道在这里面究竟进行的是怎样的数学计算，比如这个下面3*3的小块到底干了什么？<br /><img alt="" class="has" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9qdWx5ZWR1LWltZy1wdWJsaWMub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL1B1YmxpYy9JbWFnZS9RdWVzdGlvbi8xNTE2Nzc4Mzg0XzI0MC5wbmc?x-oss-process=image/format,png" /><br /><br />
这里面的数学操作，就是我们常说的“卷积”操作。接下来，我们来了解下什么是卷积操作。</p>

<h3>4.2 什么是卷积</h3>

<p>    对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做<strong>内积</strong>（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。</p>

<p>    非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。</p>

<blockquote>
<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160822134955264" /></p>
</blockquote>
</blockquote>

<p>    OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160702215705128" /></p>

<p>    分解下上图</p>

<blockquote>
<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160702220834570" /> 对应位置上是数字先相乘后相加 <img alt="" class="has" src="https://img-blog.csdn.net/20160702220819960" /> = <img alt="" class="has" src="https://img-blog.csdn.net/20160702220844314" /></p>
</blockquote>
</blockquote>

<p>    中间滤波器filter与数据窗口做内积，其具体计算过程则是：4*0 + 0*0 + 0*0 + 0*0 + 0*1 + 0*1 + 0*0 + 0*1 + -4*2 = -8</p>

<h3>4.3 图像上的卷积</h3>

<p>    在下图对应的计算过程中，输入是一定区域大小(width*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。</p>

<p>    具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。</p>

<p>    如下图所示</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160702214116669" />  </p>

<p> </p>
</blockquote>

<h3>4.4 GIF动态卷积图</h3>

<p>    在CNN中，<span style="color:#362e2b;">滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数： </span><br /><span style="color:#362e2b;">　　a. </span><span style="color:#362e2b;">深度depth</span><span style="color:#362e2b;">：神经元个数，决定输出的depth厚度。同时代表滤波器个数。</span><br /><span style="color:#362e2b;">　　b. </span><span style="color:#362e2b;">步长stride</span><span style="color:#362e2b;">：决定滑动多少步可以到边缘。</span></p>

<p><span style="color:#362e2b;">　　c. </span><span style="color:#362e2b;">填充值zero-padding</span><span style="color:#362e2b;">：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。 </span></p>

<blockquote>
<blockquote>
<blockquote>
<p><span style="color:#362e2b;">　　</span><img alt="这里写图片描述" class="has" src="https://img-blog.csdn.net/20160705162205761" /><span style="color:#362e2b;"> </span></p>
</blockquote>
</blockquote>
</blockquote>

<p>    cs231n课程中有一张卷积动图，貌似是用d3js 和一个util 画的，我<strong>根据cs231n的卷积动图依次截取了18张图，然后用一gif 制图工具制作了一gif 动态卷积图。如下gif 图所示</strong></p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707204048899" /></p>
</blockquote>

<p>    可以看到：</p>

<ul><li><span style="color:#362e2b;">两个神经元，即depth=2，意味着有两个滤波器。</span></li>
	<li><span style="color:#362e2b;">数据窗口每次移动两个步长取3*3的局部数据，即stride=2。</span></li>
	<li><span style="color:#362e2b;">zero-padding=1。</span></li>
</ul><p><span style="color:#362e2b;">    然后分别以两个滤波器filter为轴滑动数组进行卷积计算，得到两组不同的结果。</span></p>

<p>    如果初看上图，可能不一定能立马理解啥意思，但结合上文的内容后，理解这个动图已经不是很困难的事情：</p>

<ul><li>左边是输入（<strong>7*7*3</strong>中，7*7代表图像的像素/长宽，3代表R、G、B 三个颜色通道）</li>
	<li>中间部分是两个不同的滤波器Filter w0、Filter w1</li>
	<li>最右边则是两个不同的输出</li>
</ul><p>    随着左边数据窗口的平移滑动，滤波器Filter w0 / Filter w1对不同的局部数据进行卷积计算。</p>

<p>    值得一提的是：</p>

<ol><li>左边数据在变化，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的<strong>局部感知</strong>机制。</li>
</ol><ul><li>打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。</li>
</ul><p>与此同时，数据窗口滑动，导致输入在变化，但中间滤波器Filter w0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的<strong>参数（权重）共享</strong>机制。</p>

<ul><li>再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼 看同一个局部信息 所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器 就像不同的双眼，不同的人有着不同的反馈结果。</li>
</ul><p>    我第一次看到上面这个动态图的时候，只觉得很炫，另外就是据说计算过程是“相乘后相加”，但到底具体是个怎么相乘后相加的计算过程 则无法一眼看出，网上也没有一目了然的计算过程。本文来细究下。</p>

<p>    首先，我们来分解下上述动图，如下图</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707204919497" /></p>
</blockquote>

<p>    接着，我们细究下上图的具体计算过程。即上图中的输出结果1具体是怎么计算得到的呢？其实，类似wx + b，w对应滤波器Filter w0，x对应不同的数据窗口，b对应Bias b0，相当于滤波器Filter w0与一个个数据窗口相乘再求和后，最后加上Bias b0得到输出结果1，如下过程所示：</p>

<blockquote><img alt="" class="has" src="https://img-blog.csdn.net/20160707205622297" /><img alt="" class="has" src="https://img-blog.csdn.net/20160707205640719" /></blockquote>

<p><span style="color:#ff0000;">1</span>* <span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0 </span></p>

<p>+</p>

<p><span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span> + <u><strong><span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">1</span></strong></u></p>

<p>+</p>

<p><span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">1</span></p>

<p> </p>

<p>+</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707205653969" /><img alt="" class="has" src="https://img-blog.csdn.net/20160707205705937" /></p>

<p><span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span></p>

<p>+</p>

<p><span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">1</span> + <strong><u><span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">1</span></u></strong></p>

<p>+</p>

<p><span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">-1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">2</span></p>

<p> </p>

<p>+</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707205720140" /><img alt="" class="has" src="https://img-blog.csdn.net/20160707205732156" /></p>

<p><span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span></p>

<p>+</p>

<p><span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span> + <span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">2</span> + <span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span></p>

<p>+</p>

<p><span style="color:#ff0000;">0</span>*<span style="color:#3333ff;">0</span> + -<span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span> +<span style="color:#ff0000;"> </span><span style="color:#ff0000;">1</span>*<span style="color:#3333ff;">0</span></p>

<p> </p>

<p>+</p>

<p> </p>

<p><strong><span style="color:#ff0000;"><u>1</u></span></strong></p>

<p>=</p>

<p>1</p>

<p>    然后滤波器Filter w0固定不变，数据窗口向右移动2步，继续做内积计算，得到0的输出结果</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707230038086" /></p>
</blockquote>

<p>    最后，换做另外一个不同的滤波器Filter w1、不同的偏置Bias b1，再跟图中最左边的数据窗口做卷积，可得到另外一个不同的输出。</p>

<blockquote>
<p><img alt="" class="has" src="https://img-blog.csdn.net/20160707230115204" /></p>
</blockquote>

<p> </p>

<p> </p>

<h2>5 CNN之激励层与池化层</h2>

<h3>5.1 ReLU激励层</h3>

<p>    2.2节介绍了激活函数<span style="color:#362e2b;">sigmoid，但实际梯度下降中，<span style="color:#362e2b;">sigmoid</span>容易饱和、造成终止梯度传递，且没有0中心化。咋办呢，可以尝试另外一个激活函数：ReLU，其图形表示如下</span></p>

<blockquote>
<blockquote>
<blockquote>
<p><span style="color:#362e2b;"><img alt="" class="has" src="https://img-blog.csdn.net/20160703124215945" /></span></p>
</blockquote>
</blockquote>
</blockquote>

<p><span style="color:#362e2b;">    ReLU的优点是收敛快，求梯度简单。</span></p>

<h3>5.2 池化pool层</h3>

<p>    前头说了，池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）</p>

<p><img alt="" class="has" src="https://img-blog.csdn.net/20160703121026432" /></p>

<p>    上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。很简单不是？</p>

<p> </p>

<p> </p>

<h2>6 后记</h2>

<p>    本文基本上边看5月dl班寒讲的CNN视频边做笔记，之前断断续续看过不少CNN相关的资料（包括cs231n），但看过视频之后，才系统了解CNN到底是个什么东西，作为听众 寒讲的真心赞、清晰。然后在写CNN相关的东西时，发现一些前置知识（比如神经元、多层神经网络等也需要介绍下），包括CNN的其它层次机构（比如激励层），所以本文本只想简要介绍下卷积操作的，但考虑到知识之间的前后关联，所以越写越长，便成本文了。</p>

<p>    此外，在写作本文的过程中，请教了我们讲师团队里的寒、冯两位，感谢他两。同时，感谢爱可可老师的微博转发，感谢七月在线所有同事。</p>

<p style="text-indent:50px;"><img alt="" class="has" height="533" src="https://img-blog.csdn.net/20160815004026862" width="300" /><img alt="" class="has" height="533" src="https://img-blog.csdn.net/20160815004034924" width="300" /></p>

<p style="text-indent:0;">以下是修改日志：</p>

<ul><li>2016年7月5日，修正了一些笔误、错误，以让全文更通俗、更精准。有任何问题或槽点，欢迎随时指出。</li>
	<li>2016年7月7日，第二轮修改完毕。且根据cs231n的卷积动图依次截取了18张图，然后用制图工具制作了一gif 动态卷积图，放在文中4.3节。</li>
	<li>2016年7月16日，完成第三轮修改。本轮修改主要体现在sigmoid函数的说明上，通过举例和统一相关符号让其含义更一目了然、更清晰。</li>
	<li>2016年8月15日，完成第四轮修改，增补相关细节。比如补充4.3节GIF动态卷积图中输入部分的解释，即7*7*3的含义（其中7*7代表图像的像素/长宽，3代表R、G、B 三个颜色通道）。不断更易懂。</li>
	<li>2016年8月22日，完成第五轮修改。本轮修改主要加强滤波器的解释，及引入CNN中滤波器的通俗比喻。</li>
</ul><p>    July、最后修改于二零一六年八月二十二日中午于七月在线办公室。</p>

<p> </p>

<h2><br />
7 参考文献及推荐阅读</h2>

<ol><li><a href="http://www.wikiwand.com/zh-cn/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">人工神经网络wikipedia</a></li>
	<li><a href="http://52opencourse.com/139/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AB%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA-neural-networks-representation" rel="nofollow">斯坦福机器学习公开课</a></li>
	<li><a href="http://neuralnetworksanddeeplearning.com/" rel="nofollow">http://neuralnetworksanddeeplearning.com/</a></li>
	<li>雨石 卷积神经网络：<a href="http://blog.csdn.net/stdcoutzyx/article/details/41596663">http://blog.csdn.net/stdcoutzyx/article/details/41596663</a></li>
	<li>cs231n 神经网络结构与神经元激励函数：<a href="http://cs231n.github.io/neural-networks-1/" rel="nofollow">http://cs231n.github.io/neural-networks-1/</a>，<a href="https://zhuanlan.zhihu.com/p/21462488" rel="nofollow">中译版</a></li>
	<li>cs231n 卷积神经网络：<a href="http://cs231n.github.io/convolutional-networks/" rel="nofollow">http://cs231n.github.io/convolutional-networks/</a></li>
	<li>七月在线寒老师讲的<strong>5月dl班第4次课CNN与常用框架视频</strong>，已经剪切部分放在七月在线官网：<a href="https://www.julyedu.com/video/play/42/206" rel="nofollow">julyedu.com</a></li>
	<li>七月在线5月深度学习班第5课CNN训练注意事项部分视频：<a href="https://www.julyedu.com/video/play/42/207" rel="nofollow">https://www.julyedu.com/video/play/42/207</a></li>
	<li>七月在线5月深度学习班：<a href="https://www.julyedu.com/course/getDetail/37" rel="nofollow">https://www.julyedu.com/course/getDetail/37</a></li>
	<li>七月在线5月深度学习班课程笔记——No.4《CNN与常用框架》：<a href="http://blog.csdn.net/joycewyj/article/details/51792477">http://blog.csdn.net/joycewyj/article/details/51792477</a></li>
	<li>七月在线6月数据数据挖掘班第7课视频：数据分类与排序</li>
	<li>手把手入门神经网络系列(1)_从初等数学的角度初探神经网络：<a href="http://blog.csdn.net/han_xiaoyang/article/details/50100367">http://blog.csdn.net/han_xiaoyang/article/details/50100367</a></li>
	<li>深度学习与计算机视觉系列(6)_神经网络结构与神经元激励函数：<a href="http://blog.csdn.net/han_xiaoyang/article/details/50447834">http://blog.csdn.net/han_xiaoyang/article/details/50447834</a></li>
	<li>深度学习与计算机视觉系列(10)_细说卷积神经网络：<a href="http://blog.csdn.net/han_xiaoyang/article/details/50542880">http://blog.csdn.net/han_xiaoyang/article/details/50542880</a></li>
	<li>zxy 图像卷积与滤波的一些知识点：<a href="http://blog.csdn.net/zouxy09/article/details/49080029">http://blog.csdn.net/zouxy09/article/details/49080029</a></li>
	<li>zxy 深度学习CNN笔记：<a href="http://blog.csdn.net/zouxy09/article/details/8781543/">http://blog.csdn.net/zouxy09/article/details/8781543/</a></li>
	<li><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" rel="nofollow">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a>，<a href="http://www.csdn.net/article/2015-11-11/2826192">中译版</a></li>
	<li>《神经网络与深度学习》中文讲义：<a href="http://vdisk.weibo.com/s/A_pmE4iIPs9D" rel="nofollow">http://vdisk.weibo.com/s/A_pmE4iIPs9D</a></li>
	<li>ReLU与<span style="color:#362e2b;">sigmoid/tanh的区别：<a href="https://www.zhihu.com/question/29021768" rel="nofollow">https://www.zhihu.com/question/29021768</a></span></li>
	<li>CNN、RNN、DNN内部网络结构区别：<a href="https://www.zhihu.com/question/34681168" rel="nofollow">https://www.zhihu.com/question/34681168</a></li>
	<li>理解卷积：<a href="https://www.zhihu.com/question/22298352" rel="nofollow">https://www.zhihu.com/question/22298352</a></li>
	<li>神经网络与深度学习简史：<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=402032673&amp;idx=1&amp;sn=d7e636b6d033cbcf8a74dfaf710e9ccf&amp;scene=21#wechat_redirect" rel="nofollow">1 感知机和BP算法</a>、<a href="http://chuansong.me/n/2523251" rel="nofollow">4 深度学习的伟大复兴</a></li>
	<li>在线制作gif 动图：<a href="http://www.tuyitu.com/photoshop/gif.htm" rel="nofollow">http://www.tuyitu.com/photoshop/gif.htm</a></li>
	<li><a href="http://blog.csdn.net/v_july_v/article/details/7624837">支持向量机通俗导论（理解SVM的三层境界）</a></li>
	<li><a href="http://www.jianshu.com/p/fe428f0b32c1" rel="nofollow">CNN究竟是怎样一步一步工作的？</a> 本博客把卷积操作具体怎么个计算过程写清楚了，但这篇把为何要卷积操作也写清楚了，而且配偶图非常形象，甚赞。</li>
</ol></pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                                        <a href="javascript:;" class="readmore_btn"
                                           data-linkUrl="https://blog.csdn.net/v_JULY_v/article/details/51812459"
                                           data-so-type="blog">
                                            <span>展开全文</span>
                                            <img src='https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png' alt="">
                                        </a>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/yunpiao123456/article/details/52437794"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/yunpiao123456/article/details/52437794?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-3-52437794.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-3-52437794.pc_agg_rank_aggregation\&quot;}","index": "2","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/yunpiao123456/article/details/52437794?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-3-52437794.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-3-52437794.pc_agg_rank_aggregation\&quot;}","index": "2","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-3-52437794.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2><em>卷积神经网络</em>概念与原理</h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2016-09-05 10:00:27</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    一、<em>卷积神经网络</em>的基本概念   受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出<em>卷积神经网络</em>（CNN），Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来<em>卷积神经网络</em>在多个方向...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"><div id="article_content" style="margin-left:0px;">
<h1 style="margin-left:0px;">一、卷积神经网络的基本概念</h1>

<p> </p>

<p>       受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。</p>

<p>       卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。</p>

<p><strong>二、卷积神经网络的应用场景</strong></p>

<p> </p>

<p><strong>三、卷积神经网络的原理</strong></p>

<p><strong>3.1 神经网络</strong></p>

<p>       首先介绍神经网络，这一步的详细可以参考资源1。简要介绍下。神经网络的每个单元如下：</p>

<p><img alt="logistic" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8xLnBuZw?x-oss-process=image/format,png" /></p>

<p>       其对应的公式如下：</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8yLnBuZw?x-oss-process=image/format,png" /></p>

<p>       其中，该单元也可以被称作是Logistic回归模型。当将多个单元组合起来并具有分层结构时，就形成了神经网络模型。下图展示了一个具有一个隐含层的神经网络。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8zLnBuZw?x-oss-process=image/format,png" /></p>

<p>        其对应的公式如下：</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy80LnBuZw?x-oss-process=image/format,png" /></p>

<p>       比较类似的，可以拓展到有2,3,4,5，…个隐含层。</p>

<p>       神经网络的训练方法也同Logistic类似，不过由于其多层性，还需要利用链式求导法则对隐含层的节点进行求导，即梯度下降+链式求导法则，专业名称为反向传播。关于训练算法，本文暂不涉及。</p>

<p><strong>3.2 卷积神经网络</strong></p>

<p>       受Hubel和Wiesel对猫视觉皮层电生理研究启发，有人提出卷积神经网络（CNN），Yann Lecun 最早将CNN用于手写数字识别并一直保持了其在该问题的霸主地位。近年来卷积神经网络在多个方向持续发力，在语音识别、人脸识别、通用物体识别、运动分析、自然语言处理甚至脑电波分析方面均有突破。</p>

<p>       卷积神经网络与普通神经网络的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。卷积神经网络的基本结构如图所示：</p>

<p><img alt="" src="https://img-blog.csdn.net/20160923140132828?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p> </p>

<p>       卷积神经网络由三部分构成。第一部分是输入层。第二部分由n个卷积层和池化层的组合组成。第三部分由一个全连结的多层感知机分类器构成。</p>

<p><strong>3.2.1局部感受野</strong></p>

<p>       卷积神经网络有两种神器可以降低参数数目，第一种神器叫做局部感知野。一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。网络部分连通的思想，也是受启发于生物学里面的视觉系统结构。视觉皮层的神经元就是局部接受信息的（即这些神经元只响应某些特定区域的刺激）。如下图所示：左图为全连接，右图为局部连接。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy81LmpwZw?x-oss-process=image/format,png" /></p>

<p>        在上右图中，假如每个神经元只和10×10个像素值相连，那么权值数据为1000000×100个参数，减少为原来的万分之一。而那10×10个像素值对应的10×10个参数，其实就相当于卷积操作。</p>

<p><strong>3.2.3 权值共享</strong></p>

<p>       但其实这样的话参数仍然过多，那么就启动第二级神器，即权值共享。在上面的局部连接中，每个神经元都对应100个参数，一共1000000个神经元，如果这1000000个神经元的100个参数都是相等的，那么参数数目就变为100了。</p>

<p>       怎么理解权值共享呢？我们可以这100个参数（也就是卷积操作）看成是提取特征的方式，该方式与位置无关。这其中隐含的原理则是：图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上，所以对于这个图像上的所有位置，我们都能使用同样的学习特征。</p>

<p>       更直观一些，当从一个大尺寸图像中随机选取一小块，比如说 8x8 作为样本，并且从这个小块样本中学习到了一些特征，这时我们可以把从这个 8x8 样本中学习到的特征作为探测器，应用到这个图像的任意地方中去。特别是，我们可以用从 8x8 样本中所学习到的特征跟原本的大尺寸图像作卷积，从而对这个大尺寸图像上的任一位置获得一个不同特征的激活值。</p>

<p>       如下图所示，展示了一个3×<em>3的卷积核在5×</em>5的图像上做卷积的过程。每个卷积都是一种特征提取方式，就像一个筛子，将图像中符合条件（激活值越大越符合条件）的部分筛选出来。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy82LmdpZg" /></p>

<p><strong>3.2.4 多卷积核</strong></p>

<p>       上面所述只有100个参数时，表明只有1个10*10的卷积核，显然，特征提取是不充分的，我们可以添加多个卷积核，比如32个卷积核，可以学习32种特征。在有多个卷积核时，如下图所示：</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy83LmpwZw?x-oss-process=image/format,png" /></p>

<p>       上图右，不同颜色表明不同的卷积核。每个卷积核都会将图像生成为另一幅图像。比如两个卷积核就可以将生成两幅图像，这两幅图像可以看做是一张图像的不同的通道。如下图所示，下图有个小错误，即将w1改为w0，w2改为w1即可。下文中仍以w1和w2称呼它们。</p>

<p>      下图展示了在四个通道上的卷积操作，有两个卷积核，生成两个通道。其中需要注意的是，四个通道上每个通道对应一个卷积核，先将w2忽略，只看w1，那么在w1的某位置（i,j）处的值，是由四个通道上（i,j）处的卷积结果相加然后再取激活函数值得到的。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy84LnBuZw?x-oss-process=image/format,png" /></p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy85LnBuZw?x-oss-process=image/format,png" /></p>

<p>       所以，在上图由4个通道卷积得到2个通道的过程中，参数的数目为4×2×2×2个，其中4表示4个通道，第一个2表示生成2个通道，最后的2×2表示卷积核大小。</p>

<p><strong>3.2.5 Down-pooling</strong></p>

<p>       在通过卷积获得了特征 (features) 之后，下一步我们希望利用这些特征去做分类。理论上讲，人们可以用所有提取得到的特征去训练分类器，例如 softmax 分类器，但这样做面临计算量的挑战。例如：对于一个 96X96 像素的图像，假设我们已经学习得到了400个定义在8X8输入上的特征，每一个特征和图像卷积都会得到一个 (96 − 8 + 1) × (96 − 8 + 1) = 7921 维的卷积特征，由于有 400 个特征，所以每个样例 (example) 都会得到一个 7921 × 400 = 3,168,400 维的卷积特征向量。学习一个拥有超过 3 百万特征输入的分类器十分不便，并且容易出现过拟合 (over-fitting)。</p>

<p>       为了解决这个问题，首先回忆一下，我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计，例如，人们可以计算图像一个区域上的某个特定特征的平均值 (或最大值)。这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。这种聚合的操作就叫做池化 (pooling)，有时也称为平均池化或者最大池化 (取决于计算池化的方法)。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8xMC5naWY" /></p>

<p>        </p>

<p>       子采样有两种形式，一种是均值子采样（mean-pooling），一种是最大值子采样（max-pooling）。两种子采样看成特殊的卷积过程，如图下图所示：</p>

<p>       (1)均值子采样的卷积核中每个权重都是0.25，卷积核在原图inputX上的滑动的步长为2。均值子采样的效果相当于把原图模糊缩减至原来的1/4。</p>

<p>       (2)最大值子采样的卷积核中各权重值中只有一个为1，其余均为0，卷积核中为1的位置对应inputX被卷积核覆盖部分值最大的位置。卷积核在原图inputX上的滑动步长为2。最大值子采样的效果是把原图缩减至原来的1/4，并保留每个2*2区域的最强输入。</p>

<p><img alt="" src="https://img-blog.csdn.net/20160923135714235?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p>        至此，卷积神经网络的基本结构和原理已经阐述完毕。</p>

<p> </p>

<h2 style="margin-left:0px;">3.2.6 多卷积层</h2>

<p> </p>

<h2 style="margin-left:0px;"> </h2>

<p>       在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化。</p>

<p><strong>四、卷积神经网络的训练</strong></p>

<p><span style="color:#4b4b4b;">      本文的主要目的是介绍CNN参数在使用bp算法时该怎么训练，毕竟CNN中有卷积层和下采样层，虽然和MLP的bp算法本质上相同，但形式上还是有些区别的，很显然在完成CNN反向传播前了解bp算法是必须的。</span></p>

<p><span style="color:#4b4b4b;"><strong>4.1 Forward<span style="color:#333333;">前向传播</span></strong></span></p>

<p> </p>

<p>       前向过程的卷积为典型valid的卷积过程，即卷积核kernalW覆盖在输入图inputX上，对应位置求积再求和得到一个值并赋给输出图OutputY对应的位置。每次卷积核在inputX上移动一个位置，从上到下从左到右交叠覆盖一遍之后得到输出矩阵outputY(如图4.1与图4.3所示)。如果卷积核的输入图inputX为Mx*Nx大小，卷积核为Mw*Nw大小，那么输出图Y为（Mx-Mw+1）*（Nx-Nw+1）大小。</p>

<p> </p>

<p><img alt="" src="https://img-blog.csdn.net/20160923141957406?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p><strong><span style="color:#4b4b4b;">4.2 BackForward</span><span style="color:#333333;">反向传播</span></strong></p>

<p>       在错误信号反向传播过程中，先按照神经网络的错误反传方式得到尾部分类器中各神经元的错误信号，然后错误信号由分类器向前面的特征抽取器传播。错误信号从子采样层的特征图（subFeatureMap）往前面卷积层的特征图（featureMap）传播要通过一次full卷积过程来完成。这里的卷积和上一节卷积的略有区别。如果卷积核kernalW的长度为Mw*Mw的方阵，那么subFeatureMap的错误信号矩阵Q_err需要上下左右各拓展Mw-1行或列，与此同时卷积核自身旋转180度。subFeatureMap的错误信号矩阵P_err等于featureMap的误差矩阵Q_err卷积旋转180度的卷积核W_rot180。</p>

<p>       下图错误信号矩阵Q_err中的A，它的产生是P中左上2*2小方块导致的，该2*2的小方块的对A的责任正好可以用卷积核W表示，错误信号A通过卷积核将错误信号加权传递到与错误信号量为A的神经元所相连的神经元a、b、d、e中，所以在下图中的P_err左上角的2*2位置错误值包含A、2A、3A、4A。同理，我们可以论证错误信号B、C、D的反向传播过程。综上所述，错误信号反向传播过程可以用下图中的卷积过程表示。</p>

<p> </p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdn.net/20160923142211870?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p><strong>4.3 权值更新过程中的卷积</strong></p>

<p>       卷积神经网络中卷积层的权重更新过程本质是卷积核的更新过程。由神经网络的权重修改策略我们知道一条连接权重的更新量为该条连接的前层神经元的兴奋输出乘以后层神经元的输入错误信号，卷积核的更新也是按照这个规律来进行。</p>

<p> </p>

<p><img alt="" src="https://img-blog.csdn.net/20160923142327348?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p>       在前向卷积过程中，卷积核的每个元素（链接权重）被使用过四次，所以卷积核每个元素的产生四个更新量。把前向卷积过程当做切割小图进行多个神经网络训练过程，我们得到四个4*1的神经网络的前层兴奋输入和后层输入错误信号，如图所示。</p>

<p> </p>

<p><img alt="" src="https://img-blog.csdn.net/20160923142356755?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p>        根据神经网络的权重修改策略，我们可以算出如图所示卷积核的更新量W_delta。权重更新量W_delta可由P_out和Q_err卷积得到，如图下图所示。</p>

<p style="text-align:center;"><img alt="" src="https://img-blog.csdn.net/20160923142434756?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" /></p>

<p><strong>五、常见网络结构</strong></p>

<p> </p>

<h1 style="margin-left:0px;"><a name="t8"></a>5.1 ImageNet-2010网络结构</h1>

<p>ImageNet LSVRC是一个图片分类的比赛，其训练集包括127W+张图片，验证集有5W张图片，测试集有15W张图片。本文截取2010年Alex Krizhevsky的CNN结构进行说明，该结构在2010年取得冠军，top-5错误率为15.3%。值得一提的是，在今年的ImageNet LSVRC比赛中，取得冠军的GoogNet已经达到了top-5错误率6.67%。可见，深度学习的提升空间还很巨大。</p>

<p>下图即为Alex的CNN结构图。需要注意的是，该模型采用了2-GPU并行结构，即第1、2、4、5卷积层都是将模型参数分为2部分进行训练的。在这里，更进一步，并行结构分为数据并行与模型并行。数据并行是指在不同的GPU上，模型结构相同，但将训练数据进行切分，分别训练得到不同的模型，然后再将模型进行融合。而模型并行则是，将若干层的模型参数进行切分，不同的GPU上使用相同的数据进行训练，得到的结果直接连接作为下一层的输入。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8xMS5wbmc?x-oss-process=image/format,png" /></p>

<pre>
<code>上图模型的基本参数为：
</code></pre>

<ul><li>输入：224×224大小的图片，3通道</li>
	<li>第一层卷积：11×11大小的卷积核96个，每个GPU上48个。</li>
	<li>第一层max-pooling：2×2的核。</li>
	<li>第二层卷积：5×5卷积核256个，每个GPU上128个。</li>
	<li>第二层max-pooling：2×2的核。</li>
	<li>第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。</li>
	<li>第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。</li>
	<li>第五层卷积：3×3的卷积核256个，两个GPU上个128个。</li>
	<li>第五层max-pooling：2×2的核。</li>
	<li>第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。</li>
	<li>第二层全连接：4096维</li>
	<li>Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。</li>
</ul><h1 style="margin-left:0px;"><a name="t9"></a>5.2 DeepID网络结构</h1>

<p>        DeepID网络结构是香港中文大学的Sun Yi开发出来用来学习人脸特征的卷积神经网络。每张输入的人脸被表示为160维的向量，学习到的向量经过其他模型进行分类，在人脸验证试验上得到了97.45%的正确率，更进一步的，原作者改进了CNN，又得到了99.15%的正确率。</p>

<p>如下图所示，该结构与ImageNet的具体参数类似，所以只解释一下不同的部分吧。</p>

<p><img alt="equal" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL3N0ZGNvdXR6eXgvUGFwZXJfUmVhZC9tYXN0ZXIvYmxvZ3MvaW1ncy8xMi5wbmc?x-oss-process=image/format,png" /></p>

<p>       上图中的结构，在最后只有一层全连接层，然后就是softmax层了。论文中就是以该全连接层作为图像的表示。在全连接层，以第四层卷积和第三层max-pooling的输出作为全连接层的输入，这样可以学习到局部的和全局的特征。</p>

<p> </p>

<p> </p>

<p> </p>
 

<h1 style="margin-left:0px;"><a name="t10"></a>参考资源</h1>

<ul><li>[1] http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B 栀子花对Stanford深度学习研究团队的深度学习教程的翻译</li>
	<li>[2] http://blog.csdn.net/zouxy09/article/details/14222605 csdn博主zouxy09深度学习教程系列</li>
	<li>[3] http://deeplearning.net/tutorial/ theano实现deep learning</li>
	<li>[4] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</li>
	<li>[5] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.</li>
	<li>[6] http://blog.csdn.net/stdcoutzyx/article/details/41596663</li>
</ul></div>

<div> </div>

<div id="digg" style="margin-left:0px;"> </div>
</pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                                        <a href="javascript:;" class="readmore_btn"
                                           data-linkUrl="https://blog.csdn.net/yunpiao123456/article/details/52437794"
                                           data-so-type="blog">
                                            <span>展开全文</span>
                                            <img src='https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png' alt="">
                                        </a>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/jiaoyangwm/article/details/80011656"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/jiaoyangwm/article/details/80011656?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-4-80011656.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-4-80011656.pc_agg_rank_aggregation\&quot;}","index": "3","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/jiaoyangwm/article/details/80011656?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-4-80011656.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-4-80011656.pc_agg_rank_aggregation\&quot;}","index": "3","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-4-80011656.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2><em>卷积神经网络</em>超详细介绍</h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2018-09-19 10:16:59</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    1、<em>卷积神经网络</em>的概念  2、 发展过程  3、如何利用CNN实现图像识别的任务  4、CNN的特征  5、CNN的求解  6、<em>卷积神经网络</em>注意事项  7、CNN发展综合介绍  8、LeNet-5结构分析  9、AlexNet  10、ZFNet  10.1 意义  ...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"><p></p><div class="toc"><h3>文章目录</h3><ul><ul><ul><li><a href="#1_2" rel="nofollow">1、卷积神经网络的概念</a></li><li><a href="#2__28" rel="nofollow">2、 发展过程</a></li><li><a href="#3CNN_100" rel="nofollow">3、如何利用CNN实现图像识别的任务</a></li><li><a href="#4CNN_105" rel="nofollow">4、CNN的特征</a></li><li><a href="#5CNN_127" rel="nofollow">5、CNN的求解</a></li><li><a href="#6_172" rel="nofollow">6、卷积神经网络注意事项</a></li><li><a href="#7CNN_238" rel="nofollow">7、CNN发展综合介绍</a></li><li><a href="#8LeNet5_252" rel="nofollow">8、LeNet-5结构分析</a></li><li><a href="#9AlexNet_376" rel="nofollow">9、AlexNet</a></li><li><a href="#10ZFNet_556" rel="nofollow">10、ZFNet</a></li><ul><li><a href="#101__565" rel="nofollow">10.1 意义</a></li><li><a href="#102__573" rel="nofollow">10.2 实现方法</a></li><li><a href="#103__615" rel="nofollow">10.3 训练细节</a></li><li><a href="#104__676" rel="nofollow">10.4 卷积网络可视化</a></li><li><a href="#106__703" rel="nofollow">10.6 总结</a></li></ul><li><a href="#11VGGNet_728" rel="nofollow">11、VGGNet</a></li><ul><li><a href="#111__747" rel="nofollow">11.1 结构</a></li><li><a href="#112__823" rel="nofollow">11.2 网络特点：</a></li><li><a href="#113__836" rel="nofollow">11.3 分类框架：</a></li></ul><li><a href="#12GoogLeNet_873" rel="nofollow">12、GoogLeNet</a></li><ul><li><a href="#121_GoogLeNet_Inception_V122_880" rel="nofollow">12.1 GoogLeNet Inception V1——22层</a></li><li><a href="#122_GoogLeNet_1003" rel="nofollow">12.2 GoogLeNet</a></li><li><a href="#123_GoogleNet_Inception_V2_1027" rel="nofollow">12.3 GoogleNet Inception V2</a></li><li><a href="#124_GoogLeNet_Inception_V3_1345" rel="nofollow">12.4 GoogLeNet Inception V3</a></li><ul><li><a href="#1241__1349" rel="nofollow">12.4.1 简介</a></li><li><a href="#1242__1357" rel="nofollow">12.4.2 一般情况的设计准则</a></li><li><a href="#1243__1381" rel="nofollow">12.4.3 利用大尺度滤波器进行图像的卷积</a></li></ul></ul><li><a href="#13ResNet_1440" rel="nofollow">13、ResNet</a></li><ul><li><a href="#131_ResNet_1473" rel="nofollow">13.1 ResNet的提出</a></li><li><a href="#132_ResNet_1516" rel="nofollow">13.2 ResNet的意义</a></li><li><a href="#133_ResNet_1554" rel="nofollow">13.3 ResNet结构</a></li><li><a href="#134_ResNet50ResNet101_1641" rel="nofollow">13.4 ResNet50和ResNet101</a></li><li><a href="#135_ResNet101Faster_RCNN_1655" rel="nofollow">13.5 基于ResNet101的Faster RCNN</a></li></ul><li><a href="#14_CNNRCNN2013Fast_RCNN2015Faster_RCNN2015_1678" rel="nofollow">14、区域 CNN：R-CNN(2013年)、Fast R-CNN(2015年)、Faster R-CNN(2015年)</a></li><li><a href="#15_1700" rel="nofollow">15、生成式对抗网络</a></li><li><a href="#16_1717" rel="nofollow">16、深度学习在计算机视觉上的应用</a></li><li><a href="#17_1801" rel="nofollow">17、深度有监督学习在计算机视觉领域的进展</a></li><ul><li><a href="#171__1803" rel="nofollow">17.1 图像分类</a></li><li><a href="#172_Image_Dection_1808" rel="nofollow">17.2 图像检测（Image Dection）</a></li><li><a href="#174_Image_Captioning_1820" rel="nofollow">17.4 图像标注–看图说话（Image Captioning）</a></li></ul><li><a href="#18Reinforcement_Learning_1833" rel="nofollow">18、强化学习（Reinforcement Learning）</a></li></ul></ul></ul></div><p></p>
<h3><a id="1_2"></a>1、卷积神经网络的概念</h3>
<p><a href="http://mp.weixin.qq.com/s/eosTWBbLpwVroYPEb9Q0wA" rel="nofollow">计算机视觉和 CNN 发展十一座里程碑</a></p>
<p>上世纪60年代，Hubel等人通过对猫视觉皮层细胞的研究，提出了感受野这个概念，到80年代，Fukushima在感受野概念的基础之上提出了神经认知机的概念，可以看作是卷积神经网络的第一个实现网络，神经认知机将一个视觉模式分解成许多子模式（特征），然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。</p>
<p>卷积神经网络是多层感知机（MLP）的变种，由生物学家休博尔和维瑟尔在早期关于猫视觉皮层的研究发展而来，视觉皮层的细胞存在一个复杂的构造，这些细胞对视觉输入空间的子区域非常敏感，称之为感受野。</p>
<p><strong>CNN由纽约大学的Yann Lecun于1998年提出，其本质是一个多层感知机，成功的原因在于其所采用的局部连接和权值共享的方式：</strong></p>
<ul>
<li>
<p>一方面减少了权值的数量使得网络易于优化</p>
</li>
<li>
<p>另一方面降低了模型的复杂度，也就是减小了过拟合的风险</p>
</li>
</ul>
<p>该优点在网络的输入是图像时表现的更为明显，使得图像可以直接作为网络的输入，避免了传统识别算法中复杂的特征提取和数据重建的过程，在二维图像的处理过程中有很大的优势，如网络能够自行抽取图像的特征包括颜色、纹理、形状及图像的拓扑结构，在处理二维图像的问题上，特别是识别位移、缩放及其他形式扭曲不变性的应用上具有良好的鲁棒性和运算效率等。</p>

<table>
<thead>
<tr>
<th align="left">名称</th>
<th align="left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">LeNet5</td>
<td align="left">没啥特点-不过是第一个CNN应该要知道</td>
</tr>
<tr>
<td align="left">AlexNet</td>
<td align="left">引入了ReLU和dropout，引入数据增强、池化相互之间有覆盖，三个卷积一个最大池化+三个全连接层</td>
</tr>
<tr>
<td align="left">VGGNet</td>
<td align="left">采用1<em>1和3</em>3的卷积核以及2*2的最大池化使得层数变得更深。常用VGGNet-16和VGGNet19</td>
</tr>
<tr>
<td align="left">Google Inception Net</td>
<td align="left">这个在控制了计算量和参数量的同时，获得了比较好的分类性能，和上面相比有几个大的改进：1、去除了最后的全连接层，而是用一个全局的平均池化来取代它； 2、引入Inception Module，这是一个4个分支结合的结构。所有的分支都用到了1<em>1的卷积，这是因为1</em>1性价比很高，可以用很少的参数达到非线性和特征变换。3、Inception V2第二版将所有的5<em>5变成2个3</em>3，而且提出来著名的Batch Normalization；4、Inception V3第三版就更变态了，把较大的二维卷积拆成了两个较小的一维卷积，加速运算、减少过拟合，同时还更改了Inception Module的结构。</td>
</tr>
<tr>
<td align="left">微软ResNet残差神经网络(Residual Neural Network)</td>
<td align="left">1、引入高速公路结构，可以让神经网络变得非常深2、ResNet第二个版本将ReLU激活函数变成y=x的线性函数</td>
</tr>
</tbody>
</table><h3><a id="2__28"></a>2、 发展过程</h3>
<p>1986年Rumelhart等人提出了人工神经网络的反向传播算法，掀起了神经网络在机器学习中的热潮，神经网络中存在大量的参数，存在<strong>容易发生过拟合、训练时间长</strong>的缺点，但是对比Boosting、Logistic回归、SVM等基于统计学习理论的方法（也可以看做具有一层隐层节点或不含隐层节点的学习模型，被称为浅层模型）来说，具有较大的优越性。</p>
<p><strong>浅层模型为什么效果没有深层模型好？</strong></p>
<p>浅层学习模型通常要由人工的方法来获得好的样本特性，在此基础上进行识别和预测，因此方法的有效性在很大程度上受到特征提取的制约。</p>
<p><strong>深度学习的提出：</strong></p>
<p>2006年，Hinton提出了深度学习，两个主要的观点是：</p>
<ul>
<li>
<p>多隐层的人工神经网络具有优异的特征学习能力，学习到的数据更能反映数据的本质特征有利于可视化或分类</p>
</li>
<li>
<p>深度神经网络在训练上的难度，可以通过逐层无监督训练有效克服，</p>
</li>
</ul>
<p><strong>深度学习取得成功的原因：</strong></p>
<ul>
<li>
<p>大规模数据（例如ImageNet）：为深度学习提供了好的训练资源</p>
</li>
<li>
<p>计算机硬件的飞速发展：特别是GPU的出现，使得训练大规模上网络成为可能</p>
</li>
</ul>
<p><strong>深度学习的思想：</strong></p>
<p>深度神经网络的基本思想是通过构建多层网络，对目标进行多层表示，以期通过多层的高层次特征来表示数据的抽象语义信息，获得更好的特征鲁棒性。</p>
<p><strong>什么是卷积神经网络：</strong></p>
<p>卷积神经网络是一种带有卷积结构的深度神经网络，卷积结构可以减少深层网络占用的内存量，其三个关键的操作，<strong>其一是局部感受野，其二是权值共享，其三是pooling层</strong>，有效的减少了网络的参数个数，缓解了模型的过拟合问题。</p>
<p><strong>1）网络结构</strong></p>
<p>**卷积神经网络整体架构：**卷积神经网络是一种多层的监督学习神经网络，隐含层的卷积层和池采样层是实现卷积神经网络特征提取功能的核心模块。该网络模型通过采用梯度下降法最小化损失函数对网络中的权重参数逐层反向调节，通过频繁的迭代训练提高网络的精度。卷积神经网络的低隐层是由卷积层和最大池采样层交替组成，高层是全连接层对应传统多层感知器的隐含层和逻辑回归分类器。第一个全连接层的输入是由卷积层和子采样层进行特征提取得到的特征图像。最后一层输出层是一个分类器，可以采用逻辑回归，Softmax回归甚至是支持向量机对输入图像进行分类。</p>
<p><strong>卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。</strong></p>
<p>输入图像统计和滤波器进行卷积之后，提取该局部特征，一旦该局部特征被提取出来之后，它与其他特征的位置关系也随之确定下来了，每个神经元的输入和前一层的局部感受野相连，每个特征提取层都紧跟一个用来求局部平均与二次提取的计算层，也叫特征映射层，网络的每个计算层由多个特征映射平面组成，平面上所有的神经元的权重相等。</p>
<p>通常将输入层到隐藏层的映射称为一个特征映射，也就是通过卷积层得到特征提取层，经过pooling之后得到特征映射层。</p>
<p><strong>2）局部感受野与权值共享</strong></p>
<p>卷积神经网络的核心思想就是局部感受野、是权值共享和pooling层，以此来达到简化网络参数并使得网络具有一定程度的位移、尺度、缩放、非线性形变稳定性。</p>
<ul>
<li>
<p><strong>局部感受野</strong>：由于图像的空间联系是局部的，每个神经元不需要对全部的图像做感受，只需要感受局部特征即可，然后在更高层将这些感受得到的不同的局部神经元综合起来就可以得到全局的信息了，这样可以减少连接的数目。</p>
</li>
<li>
<p><strong>权值共享</strong>：不同神经元之间的参数共享可以减少需要求解的参数，使用多种滤波器去卷积图像就会得到多种特征映射。权值共享其实就是对图像用同样的卷积核进行卷积操作，也就意味着第一个隐藏层的所有神经元所能检测到处于图像不同位置的完全相同的特征。其主要的能力就能检测到不同位置的同一类型特征，也就是卷积网络能很好的适应图像的小范围的平移性，即有较好的平移不变性（比如将输入图像的猫的位置移动之后，同样能够检测到猫的图像）</p>
</li>
</ul>
<p><strong>3）卷积层、下采样层、全连接层</strong></p>
<p><strong>卷积层</strong>：因为通过卷积运算我们可以提取出图像的特征，通过卷积运算可以使得原始信号的某些特征增强，并且降低噪声。</p>
<ul>
<li>用一个可训练的滤波器fx去卷积一个输入的图像（第一阶段是输入的图像，后面的阶段就是卷积特征map了），然后加一个偏置bx，得到卷积层Cx。</li>
</ul>
<p><strong>下采样层</strong>：因为对图像进行下采样，可以减少数据处理量同时保留有用信息，采样可以混淆特征的具体位置，因为某个特征找出来之后，它的位置已经不重要了，我们只需要这个特征和其他特征的相对位置，可以应对形变和扭曲带来的同类物体的变化。</p>
<ul>
<li>每邻域四个像素求和变为一个像素，然后通过标量Wx+1加权，再增加偏置bx+1，然后通过一个sigmoid激活函数，产生一个大概缩小四倍的特征映射图Sx+1。 **</li>
</ul>
<p><strong>全连接层</strong>：采用softmax全连接，得到的激活值即卷积神经网络提取到的图片特征。</p>
<p><strong>卷积神经网络相比一般神经网络在图像理解中的优点：</strong></p>
<ul>
<li>网络结构能够较好的适应图像的结构</li>
<li>同时进行特征提取和分类，使得特征提取有助于特征分类</li>
<li>权值共享可以减少网络的训练参数，使得神经网络结构变得简单，适应性更强</li>
</ul>
<h3><a id="3CNN_100"></a>3、如何利用CNN实现图像识别的任务</h3>
<p><strong>输入层读入经过规则化（统一大小）的图像，每一层的每个神经元将前一层的一组小的局部近邻的单元作为输入，也就是局部感受野和权值共享，神经元抽取一些基本的视觉特征，比如边缘、角点等，这些特征之后会被更高层的神经元所使用。卷积神经网络通过卷积操作获得特征图，每个位置，来自不同特征图的单元得到各自不同类型的特征。一个卷积层中通常包含多个具有不同权值向量的特征图，使得能够保留图像更丰富的特征。卷积层后边会连接池化层进行降采样操作，一方面可以降低图像的分辨率，减少参数量，另一方面可以获得平移和形变的鲁棒性。卷积层和池化层的交替分布，使得特征图的数目逐步增多，而且分辨率逐渐降低，是一个双金字塔结构。</strong></p>
<h3><a id="4CNN_105"></a>4、CNN的特征</h3>
<p>1）具有一些传统技术所没有的优点：良好的容错能力、并行处理能力和自学习能力，可处理环境信息复杂，背景知识不清楚，推理规则不明确情况下的问题，允许样品有较大的缺损、畸变，运行速度快，自适应性能好，具有较高的分辨率。它是通过结构重组和减少权值将特征抽取功能融合进多层感知器，省略识别前复杂的图像特征抽取过程。</p>
<p>2）泛化能力要显著优于其它方法，卷积神经网络已被应用于模式分类，物体检测和物体识别等方面。利用卷积神经网络建立模式分类器，将卷积神经网络作为通用的模式分类器，直接用于灰度图像。</p>
<p>3）是一个前溃式神经网络，能从一个二维图像中提取其拓扑结构，采用反向传播算法来优化网络结构，求解网络中的未知参数。</p>
<p>4）一类特别设计用来处理二维数据的多层神经网络。CNN被认为是第一个真正成功的采用多层层次结构网络的具有鲁棒性的深度学习方法。CNN通过挖掘数据中的空间上的相关性，来减少网络中的可训练参数的数量，达到改进前向传播网络的反向传播算法效率，因为CNN需要非常少的数据预处理工作，所以也被认为是一种深度学习的方法。在CNN中，图像中的小块区域（也叫做“局部感知区域”）被当做层次结构中的底层的输入数据，信息通过前向传播经过网络中的各个层，在每一层中都由过滤器构成，以便能够获得观测数据的一些显著特征。因为局部感知区域能够获得一些基础的特征，比如图像中的边界和角落等，这种方法能够提供一定程度对位移、拉伸和旋转的相对不变性。</p>
<p>5）CNN中层次之间的紧密联系和空间信息使得其特别适用于图像的处理和理解，并且能够自动的从图像抽取出丰富的相关特性。</p>
<p>6）CNN通过结合局部感知区域、共享权重、空间或者时间上的降采样来充分利用数据本身包含的局部性等特征，优化网络结构，并且保证一定程度上的位移和变形的不变性。</p>
<p>7）CNN是一种深度的监督学习下的机器学习模型，具有极强的适应性，善于挖掘数据局部特征，提取全局训练特征和分类，它的权值共享结构网络使之更类似于生物神经网络，在模式识别各个领域都取得了很好的成果。</p>
<p>8） CNN可以用来识别位移、缩放及其它形式扭曲不变性的二维或三维图像。CNN的特征提取层参数是通过训练数据学习得到的，所以其避免了人工特征提取，而是从训练数据中进行学习；其次同一特征图的神经元共享权值，减少了网络参数，这也是卷积网络相对于全连接网络的一大优势。共享局部权值这一特殊结构更接近于真实的生物神经网络使CNN在图像处理、语音识别领域有着独特的优越性，另一方面权值共享同时降低了网络的复杂性，且多维输入信号（语音、图像）可以直接输入网络的特点避免了特征提取和分类过程中数据重排的过程。</p>
<p>9）CNN的分类模型与传统模型的不同点在于其可以直接将一幅二维图像输入模型中，接着在输出端即给出分类结果。其优势在于不需复杂的预处理，将特征抽取，模式分类完全放入一个黑匣子中，通过不断的优化来获得网络所需参数，在输出层给出所需分类，网络核心就是网络的结构设计与网络的求解。这种求解结构比以往多种算法性能更高。</p>
<p>10）隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。隐层的神经元个数,它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关。</p>
<h3><a id="5CNN_127"></a>5、CNN的求解</h3>
<p>CNN在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。</p>
<p>卷积网络执行的是监督训练，所以其样本集是由形如：**（输入向量，理想输出向量）**的向量对构成的。所有这些向量对，都应该是来源于网络即将模拟系统的实际“运行”结构，它们可以是从实际运行系统中采集来。</p>
<p><strong>1）参数初始化：</strong></p>
<p>在开始训练前，所有的权都应该用一些不同的随机数进行初始化。“小随机数”用来保证网络不会因权值过大而进入饱和状态，从而导致训练失败；“不同”用来保证网络可以正常地学习。实际上，如果用相同的数去初始化权矩阵，则网络无学习能力。</p>
<p><strong>2）训练过程包括四步</strong></p>
<p><strong>① 第一阶段：前向传播阶段</strong></p>
<ul>
<li>
<p>从样本集中取一个样本，输入网络</p>
</li>
<li>
<p>计算相应的实际输出；在此阶段信息从输入层经过逐级的变换，传送到输出层，这个过程也是网络在完成训练之后正常执行时执行的过程</p>
</li>
</ul>
<p><strong>② 第二阶段：后向传播阶段</strong></p>
<ul>
<li>
<p>计算实际输出与相应的理想输出的差</p>
</li>
<li>
<p>按照极小化误差的方法调整权值矩阵</p>
<p><strong>网络的训练过程如下：</strong></p>
</li>
</ul>
<ol>
<li>
<p>选定训练组，从样本集中分别随机地寻求N个样本作为训练组；</p>
</li>
<li>
<p>将各权值、阈值，置成小的接近于0的随机值，并初始化精度控制参数和学习率；</p>
</li>
<li>
<p>从训练组中取一个输入模式加到网络，并给出它的目标输出向量；</p>
</li>
<li>
<p>计算出中间层输出向量，计算出网络的实际输出向量；</p>
</li>
<li>
<p>将输出向量中的元素与目标向量中的元素进行比较，计算出输出误差；对于中间层的隐单元也需要计算出误差；</p>
</li>
<li>
<p>依次计算出各权值的调整量和阈值的调整量；</p>
</li>
<li>
<p>调整权值和调整阈值；</p>
</li>
<li>
<p>当经历M后，判断指标是否满足精度要求，如果不满足，则返回(3)，继续迭代；如果满足就进入下一步；</p>
</li>
<li>
<p>训练结束，将权值和阈值保存在文件中。这时可以认为各个权值已经达到稳定，分类器已经形成。再一次进行训练，直接从文件导出权值和阈值进行训练，不需要进行初始化。</p>
</li>
</ol>
<h3><a id="6_172"></a>6、卷积神经网络注意事项</h3>
<p><strong>1）数据集的大小和分块</strong></p>
<p>数据驱动的模型一般依赖于数据集的大小，CNN和其他经验模型一样，能够适用于任意大小的数据集，但用于训练的数据集应该足够大， 能够覆盖问题域中所有已知可能出现的问题，</p>
<p>设计CNN的时候，数据集应该包含三个子集：训练集、测试集、验证集</p>
<p><strong>训练集：包含问题域中的所有数据，并在训练阶段用来调整网络的权重</strong></p>
<p><strong>测试集：在训练的过程中用于测试网络对训练集中未出现的数据的分类性能，根据网络在测试集上的性能情况，网络的结构可能需要作出调整，或者增加训练循环次数。</strong></p>
<p><strong>验证集：验证集中的数据统一应该包含在测试集和训练集中没有出现过的数据，用于在网络确定之后能够更好的测试和衡量网络的性能</strong></p>
<p><strong>Looney等人建议，数据集中65%的用于训练，25%的用于测试，10%用于验证</strong></p>
<p><strong>2）数据预处理</strong></p>
<p>为了加速训练算法的收敛速度，一般都会采用一些数据预处理技术，其中包括：去除噪声、输入数据降维、删除无关数据等。</p>
<p>数据的平衡化在分类问题中异常重要，一般认为训练集中的数据应该相对于标签类别近似于平均分布，也就是每一个类别标签所对应的数据集在训练集中是基本相等的，以避免网络过于倾向于表现某些分类的特点。</p>
<p>为了平衡数据集，应该移除一些过度富余的分类中的数据，并相应补充一些相对样例稀少的分类中的数据。</p>
<p>还有一个方法就是复制一部分这些样例稀少分类中的数据，并在这些数据中加入随机噪声。</p>
<p><strong>3）数据规则化</strong></p>
<p>将数据规则化到统一的区间（如[0,1]）中具有很重要的优点：防止数据中存在较大数值的数据造成数值较小的数据对于训练效果减弱甚至无效化，一个常用的方法是将输入和输出数据按比例调整到一个和激活函数相对应的区间。</p>
<p><strong>4）网络权值初始化</strong></p>
<p>CNN的初始化主要是初始化卷积层和输出层的卷积核（权值）和偏置</p>
<p>网络权值初始化就是将网络中的所有连接权重赋予一个初始值，如果初始权重向量处在误差曲面的一个相对平缓的区域的时候，网络训练的收敛速度可能会很缓慢，一般情况下网络的连接权重和阈值被初始化在一个具有0均值的相对小的区间内均匀分布。</p>
<p><strong>5）BP算法的学习速率</strong></p>
<p>如果学习速率选取的较大，则会在训练过程中较大幅度的调整权值w，从而加快网络的训练速度，但是这和造成网络在误差曲面上搜索过程中频繁抖动，且有可能使得训练过程不能收敛。</p>
<p>如果学习速率选取的较小，能够稳定的使得网络逼近于全局最优点，但也可能陷入一些局部最优，并且参数更新速度较慢。</p>
<p>自适应学习率设定有较好的效果。</p>
<p><strong>6）收敛条件</strong></p>
<p>有几个条件可以作为停止训练的判定条件，训练误差、误差梯度、交叉验证等。一般来说，训练集的误差会随着网络训练的进行而逐步降低。</p>
<p><strong>7）训练方式</strong></p>
<p><strong>训练样例可以有两种基本的方式提供给网络训练使用，也可以是两者的结合：逐个样例训练(EET)、批量样例训练(BT)。</strong></p>
<p>在EET中，先将第一个样例提供给网络，然后开始应用BP算法训练网络，直到训练误差降低到一个可以接受的范围，或者进行了指定步骤的训练次数。然后再将第二个样例提供给网络训练。</p>
<p>EET的优点是相对于BT只需要很少的存储空间，并且有更好的随机搜索能力，防止训练过程陷入局部最小区域。</p>
<p>EET的缺点是如果网络接收到的第一个样例就是劣质（有可能是噪音数据或者特征不明显）的数据，可能使得网络训练过程朝着全局误差最小化的反方向进行搜索。</p>
<p>相对的，BT方法是在所有训练样例都经过网络传播后才更新一次权值，因此每一次学习周期就包含了所有的训练样例数据。</p>
<p>BT方法的缺点也很明显，需要大量的存储空间，而且相比EET更容易陷入局部最小区域。</p>
<p>而随机训练（ST）则是相对于EET和BT一种折衷的方法，ST和EET一样也是一次只接受一个训练样例，但只进行一次BP算法并更新权值，然后接受下一个样例重复同样的步骤计算并更新权值，并且在接受训练集最后一个样例后，重新回到第一个样例进行计算。</p>
<p>ST和EET相比，保留了随机搜索的能力，同时又避免了训练样例中最开始几个样例如果出现劣质数据对训练过程的过度不良影响。</p>
<h3><a id="7CNN_238"></a>7、CNN发展综合介绍</h3>
<p>CNN的开山之作是LeCun提出的LeNet-5，而其真正的爆发阶段是2012年AlexNet取得ImageNet比赛的分类任务的冠军，并且分类准确率远远超过利用传统方法实现的分类结果，该模型能够取得成功的原因主要有三个：</p>
<ul>
<li>海量的有标记的训练数据，也就是李飞飞团队提供的大规模有标记的数据集ImageNet</li>
<li>计算机硬件的支持，尤其是GPU的出现，为复杂的计算提供了强大的支持</li>
<li>算法的改进，包括网络结构加深、数据增强（数据扩充）、ReLU、Dropout等</li>
</ul>
<p>AlexNet之后，深度学习便一发不可收拾，分类准确率每年都被刷榜，下图展示了模型的变化情况，随着模型的变深，Top-5的错误率也越来越低，目前已经降低到了3.5%左右，同样的ImageNet数据集，人眼的辨识错误率大概为5.1%，也就是深度学习的识别能力已经超过了人类。</p>
<p><img src="https://img-blog.csdn.net/20180420154927247?" alt="这里写图片描述"></p>
<p><img src="https://img-blog.csdn.net/20180420160000389?" alt="这里写图片描述"></p>
<h3><a id="8LeNet5_252"></a>8、LeNet-5结构分析</h3>
<p><img src="https://img-blog.csdn.net/20180420092603357?" alt="这里写图片描述"></p>
<p>LeNet-5共包含8层</p>
<ul>
<li>
<p>C1层是一个卷积层，由6个特征图Feature Map构成。特征图中每个神经元与输入为5<em>5的邻域相连。特征图的大小为28</em>28，这样能防止输入的连接掉到边界之外（32-5+1=28）。C1有156个可训练参数（每个滤波器5<em>5=25个unit参数和一个bias参数，一共6个滤波器，共(5</em>5+1)<em>6=156个参数），共156</em>(28*28)=122,304个连接。</p>
</li>
<li>
<p>S2层是一个下采样层，有6个14<em>14的特征图。特征图中的每个单元与C1中相对应特征图的2</em>2邻域相连接。S2层每个单元的4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。每个单元的2<em>2感受野并不重叠，因此S2中每个特征图的大小是C1中特征图大小的1/4（行和列各1/2）。S2层有12（6</em>（1+1）=12）个可训练参数和5880（14<em>14</em>（2*2+1）*6=5880）个连接。</p>
</li>
<li>
<p>C3层也是一个卷积层，它同样通过5x5的卷积核去卷积层S2，然后得到的特征map就只有10x10个神经元，但是它有16种不同的卷积核，所以就存在16个特征map了。 C3中每个特征图由S2中所有6个或者几个特征map组合而成。为什么不把S2中的每个特征图连接到每个C3的特征图呢？原因有2点。第一，不完全的连接机制将连接的数量保持在合理的范围内。第二，也是最重要的，其破坏了网络的对称性。由于不同的特征图有不同的输入，所以迫使他们抽取不同的特征（希望是互补的）。</p>
</li>
</ul>
<p>例如，存在的一个方式是：C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。这样C3层有1516（6*（3<em>25+1）+6</em>（4<em>25+1）+3</em>（4<em>25+1）+（25</em>6+1）=1516）个可训练参数和151600（10<em>10</em>1516=151600）个连接。</p>
<ul>
<li>
<p>S4层是一个下采样层，由16个5<em>5大小的特征图构成。特征图中的每个单元与C3中相应特征图的2</em>2邻域相连接，跟C1和S2之间的连接一样。S4层有32个可训练参数（每个特征图1个因子和一个偏置16*（1+1）=32）和2000（16*（2*2+1）<em>5</em>5=2000）个连接。</p>
</li>
<li>
<p>C5层是一个卷积层，有120个特征图。每个单元与S4层的全部16个单元的5<em>5邻域相连。由于S4层特征图的大小也为5</em>5（同滤波器一样），故C5特征图的大小为1<em>1（5-5+1=1）：这构成了S4和C5之间的全连接。之所以仍将C5标示为卷积层而非全相联层，是因为如果LeNet-5的输入变大，而其他的保持不变，那么此时特征图的维数就会比1</em>1大。C5层有48120（120*（16<em>5</em>5+1）=48120由于与全部16个单元相连，故只加一个偏置）个可训练连接。</p>
</li>
<li>
<p>F6层有84个单元（之所以选这个数字的原因来自于输出层的设计），与C5层全相连。有10164（84*(120*(1*1)+1)=10164）个可训练参数。如同经典神经网络，F6层计算输入向量和权重向量之间的点积，再加上一个偏置。然后将其传递给sigmoid函数产生单元i的一个状态。</p>
</li>
<li>
<p>最后，输出层由欧式径向基函数（Euclidean Radial Basis Function）单元组成，每类一个单元，每个有84个输入。</p>
</li>
</ul>
<p><strong>1、输入层：N个32x32的训练样本</strong></p>
<p>输入图像大小为32x32，比MNIST数据库中的字母大，这样做的原因是希望潜在的明显特征，如笔画断点或角点能够出现在最高层特征监测子感受野的中心。</p>
<p><strong>2、C1层</strong></p>
<ul>
<li>输入图像大小：32x32</li>
<li>卷积核大小：5x5</li>
<li>卷积核个数：6</li>
<li>输出特征图数量：6</li>
<li>输出特征图大小：28x28（32-5+1）</li>
<li>神经元数量：4707（28x28x6）</li>
<li>连接数：122304（（28x28x5x5x6）+（28x28x6））</li>
<li>可训练参数：156（5x5x6+6，权值+偏置）</li>
</ul>
<p><strong>3、S2层</strong></p>
<ul>
<li>输入图像大小：（28x28x6）</li>
<li>卷积核大小：2x2</li>
<li>卷积核个数：6</li>
<li>输出特征图数量：6</li>
<li>输出特征图大小：14x14（28/2,28/2）</li>
<li>神经元数量：1176（14x14x6）</li>
<li>连接数：5880（（2x2x14x14x6）+（14x14x6））</li>
<li>可训练参数：12（1x6+6，权值+偏置）</li>
</ul>
<p>备注：S2层每个单元的4个输入相加，乘以一个可训练参数，再加上一个可训练偏置。结果通过sigmoid函数计算。可训练系数和偏置控制着sigmoid函数的非线性程度。</p>
<p>如果系数比较小，那么运算近似于线性运算，下采样相当于模糊图像。</p>
<p>如果系数比较大，根据偏置的大小下采样可以被看成是有噪声的“或”运算或者有噪声的“与”运算。</p>
<p>每个单元的2*2感受野并不重叠，因此S2中每个特征图的大小是C1中特征图大小的1/4（行和列各1/2）。</p>
<p><strong>4、C3层</strong></p>
<ul>
<li>输入图像大小：（14x14x6）</li>
<li>卷积核大小：5x5</li>
<li>卷积核个数：16</li>
<li>输出特征图数量：16</li>
<li>输出特征图大小：10x10（14-5+1）</li>
<li>神经元数量：1600（10x10x16）</li>
<li>连接数：151600（1516x10x10）</li>
<li>可训练参数：1516</li>
</ul>
<p>备注：C3层也是一个卷积层，通过5x5的卷积核去卷积S2层，然后得到的特征图map就有10x10个神经元，但是有16种不同的卷积核，就存在16个不同的特征map。</p>
<p>C3中每个特征图由S2中的所有6个或几个特征图组合而成，为什么不把S2中的所有特征图都连接到C3的特征图呢：</p>
<ul>
<li>第一，不完全的连接机制将连接的数量保持在合理的范围内</li>
<li>第二，也是最重要的，这样一来就可以破坏网络的对称性，由于不同的特征图有不同的输入，所以迫使他们抽取不同的特征。</li>
</ul>
<p><strong>5、S4层</strong></p>
<ul>
<li>输入图像大小：（10x10x16）</li>
<li>卷积核大小：2x2</li>
<li>卷积核个数：16</li>
<li>输出特征图数量：16</li>
<li>输出特征图大小：5x5x16</li>
<li>神经元数量：400（5x5x16）</li>
<li>连接数：2000（（2x2x5x5x16）+(5x5x16））</li>
<li>可训练参数：32（（1+1）x16）</li>
</ul>
<p>备注：S4是一个下采样层，由16个5x5大小的特征图构成，特征图的每个单元与C3中相应的特征图的2x2邻域相连，S4层有32个可训练参数（每个特征图1个因子和一个偏置）和2000个连接。</p>
<p><strong>6、C5层</strong></p>
<ul>
<li>输入图像大小：5x5x16</li>
<li>卷积核大小：5x5</li>
<li>卷积核个数：120</li>
<li>输出特征图数量：120</li>
<li>输出特征图大小：1X1（5-5+1）</li>
<li>神经元数量：120（1x120）</li>
<li>连接数：48120（5x5x16x120x1+120x1）</li>
<li>可训练参数：48120（5x5x16x120+120）</li>
</ul>
<p>备注：C5层是一个卷积层，有120个特征图，每个单元与S4层的全部16个单元的5x5邻域相连，构成了S4和C5的全连接，之所以仍将C5标识为卷积层而非全连接层是因为如果LeNet-5的输入变大，而其他的保持不变，那么此时特征图的维数就会比1x1大。</p>
<p><strong>7、F6层</strong></p>
<ul>
<li>输入图像大小：（1x1x120）</li>
<li>卷积核大小：1x1</li>
<li>卷积核个数：84</li>
<li>输出特征图数量：1</li>
<li>输出特征图大小：84</li>
<li>神经元数量：84</li>
<li>连接数：10164（120x84+84）</li>
<li>可训练参数：10164（120x84+84）</li>
</ul>
<p>备注：F6有84个单元（之所以选择84是源于输出层的设计），与C5层相连，有10164个可训练参数，类似经典的全连接神经网络，F6层计算输入向量和权重向量之间的点积，再加上一个偏置，之后将其传递给sigmoid函数产生一个单元i的状态。</p>
<p><strong>8、output层</strong></p>
<ul>
<li>输入图像大小：1x84</li>
<li>输出特征图数量：1x10</li>
</ul>
<h3><a id="9AlexNet_376"></a>9、AlexNet</h3>
<p><a href="https://blog.csdn.net/zyqdragon/article/details/72353420">超详细介绍AlexNet</a></p>
<p>这篇论文，题目叫做“ImageNet Classification with Deep Convolutional Networks”，迄今被引用6184次，被业内普遍视为行业最重要的论文之一。Alex Krizhevsky、Ilya Sutskever和 Geoffrey Hinton创造了一个“大型的深度卷积神经网络”，赢得了2012 ILSVRC(2012年ImageNet 大规模视觉识别挑战赛)。稍微介绍一下，这个比赛被誉为计算机视觉的年度奥林匹克竞赛，全世界的团队相聚一堂，看看是哪家的视觉模型表现最为出色。2012年是CNN首次实现Top 5误差率15.4%的一年(Top 5误差率是指给定一张图像，其标签不在模型认为最有可能的5个结果中的几率)，当时的次优项误差率为26.2%。这个表现不用说震惊了整个计算机视觉界。可以说，是自那时起，CNN才成了家喻户晓的名字。</p>
<p>ImageNet 2012比赛分类任务的冠军，将分类错误率降低到了15.315%，使用传统计算机视觉的第二名小组的分类错误率为26.172%。</p>
<p><img src="https://img-blog.csdn.net/20180420112344646?" alt="这里写图片描述"></p>
<p>上图所示是caffe中alexnet的网络结构，上图采用是两台GPU服务器，所有会看到两个流程图。下边把AlexNet的网络结构示意一下：<br>
<img src="https://img-blog.csdn.net/20180711223753365?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>简化的结构：</p>
<p><img src="https://img-blog.csdn.net/2018071122382587?" alt="这里写图片描述"></p>
<p><strong>架构：</strong></p>
<p>因为使用了两台GPU训练，因而有两股“流”。使用两台GPU训练的原因是计算量太大，只能拆开来。</p>
<p><strong>要点：</strong></p>
<p>数据集：ImageNet数据集，含1500多万个带标记的图像，超过2.2万个类别<br>
激活函数：ReLU（训练速度快，一定程度上减小了梯度消失的问题）<br>
数据增强：平移、镜像、缩放等<br>
过拟合：dropout<br>
如何训练：批处理梯度下降训练模型，注明了动量衰减值和权值衰减值<br>
训练时间：使用两台GTX 580 GPU，训练了5到6天</p>
<p><img src="https://img-blog.csdn.net/20180420112045433?" alt="这里写图片描述"></p>
<p><img src="https://img-blog.csdn.net/20180420160053465?" alt="这里写图片描述"></p>
<p><img src="https://img-blog.csdn.net/20180423191324840?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>Alex Krizhevsky等人于2012年的ImageNet比赛中提出了新型卷积神经网络AlexNet，并获得了图像分类问题的最好成绩（Top-5错误率为15.3%）。</p>
<p><strong>网络结构：</strong></p>
<p>其实AlexNet的结构很简单，只是LeNet的放大版，输入是一个224x224的图像，经过5个卷积层，3个全连接层（包含一个分类层），达到了最后的标签空间。</p>
<p><strong>AlexNet学习出来的特征是什么样子的？</strong></p>
<ul>
<li>第一层：都是一些填充的块状物和边界等特征</li>
<li>中间层：学习一些纹理特征</li>
<li>更高层：接近于分类器的层级，可以明显的看到物体的形状特征</li>
<li>最后一层：分类层，完全是物体的不同的姿态，根据不同的物体展现出不同姿态的特征了。</li>
</ul>
<p>即无论对什么物体，学习过程都是：边缘<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.36687em; vertical-align: 0em;"></span><span class="mrel">→</span></span></span></span></span>部分<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\to</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.36687em; vertical-align: 0em;"></span><span class="mrel">→</span></span></span></span></span>整体</p>
<p>该方法训练了一个端到端的卷积神经网络实现对图像特征提取和分类，网络结构共7层，包含5层卷积层和2层全连接层。</p>
<p>AlexNet包含了6亿三千万个连接，6000万个参数和65万个神经元，拥有5个卷积层，其中3个卷积层后面连接了最大池化层，最后还有3个全连接层。</p>
<p>AlexNet可以说是神经网络在低谷期后的第一次发声，确立了深度学习（深度卷积神经网络）在计算机界的统治地位，同时也推动了深度学习在语音识别、自然语言处理、强化学习等方面的拓展。</p>
<p>**训练技巧：dropout防止过拟合，提高泛化能力 **</p>
<p>训练阶段使用了Dropout技巧随机忽略一部分神经元，缓解了神经网络的过拟合现象，和防止对网络参数优化时陷入局部最优的问题，Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。</p>
<p>该网络是利用Dropout在训练过程中将输入层和中间层的一些神经元随机置零，使得训练过程收敛的更慢，但得到的网络模型更加具有鲁棒性。</p>
<p><strong>数据扩充 / 数据增强：防止过拟合</strong></p>
<p>通过图像平移、水平翻转、调整图像灰度等方法扩充样本训练集，扩充样本训练集，使得训练得到的网络对局部平移、旋转、光照变化具有一定的不变性，数据经过扩充以后可以达到减轻过拟合并提升泛化能力。进行预测时，则是取图像的四个角加上中间共5个位置，并进行左右翻转，一共获得10张图像，对它们进行预测并对10次结果求均值。</p>
<ul>
<li>
<p>水平翻转：<br>
<img src="https://img-blog.csdn.net/20180420160240255?" alt="这里写图片描述"></p>
</li>
<li>
<p>随机裁剪、平移旋转：<br>
<img src="https://img-blog.csdn.net/20180420160307634?" alt="这里写图片描述"></p>
</li>
<li>
<p>颜色变换：<br>
<img src="https://img-blog.csdn.net/20180420160340891?" alt="这里写图片描述"></p>
</li>
</ul>
<p><strong>池化方式：</strong></p>
<p>AlexNet全部使用最大池化的方式，避免了平均池化所带来的模糊化的效果，并且步长&lt;池化核的大小，这样一来池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。</p>
<p>此前的CNN一直使用平均池化的操作。</p>
<p><strong>激活函数：ReLU</strong></p>
<p>Relu函数：f(x)=max(0,x)</p>
<p>采用非饱和线性单元——ReLU代替传统的经常使用的tanh和sigmoid函数，加速了网络训练的速度，降低了计算的复杂度，对各种干扰更加具有鲁棒性，并且在一定程度上避免了梯度消失问题。<br>
<img src="https://img-blog.csdn.net/20180711224018105?" alt="这里写图片描述"></p>
<p><strong>优势：</strong></p>
<ol>
<li>ReLU本质上是分段线性模型，前向计算非常简单，无需指数之类操作；</li>
<li>ReLU的偏导也很简单，反向传播梯度，无需指数或者除法之类操作；</li>
<li>ReLU不容易发生梯度发散问题，Tanh和Logistic激活函数在两端的时候导数容易趋近于零，多级连乘后梯度更加约等于0；</li>
<li>ReLU关闭了右边，从而会使得很多的隐层输出为0，即网络变得稀疏，起到了类似L1的正则化作用，可以在一定程度上缓解过拟合。</li>
</ol>
<p><strong>缺点：</strong></p>
<p>当然，ReLU也是有缺点的，比如左边全部关了很容易导致某些隐藏节点永无翻身之日，所以后来又出现pReLU、random ReLU等改进，而且ReLU会很容易改变数据的分布，因此ReLU后加Batch Normalization也是常用的改进的方法。</p>
<p><strong>提出了LRN层（Local Response Normalization）：</strong></p>
<p>LRN即Local Response Normalization，局部响应归一化处理，实际就是利用临近的数据做归一化，该策略贡献了1.2%的准确率，该技术是深度学习训练时的一种提高准确度的技术方法，LRN一般是在激活、池化后进行的一种处理方法。</p>
<p>LRN是对局部神经元的活动创建竞争机制，使得其中响应较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<p><strong>为什么输入数据需要归一化（Normalized Data）？</strong></p>
<p>归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。</p>
<p>对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。</p>
<p><strong>分布式计算：</strong></p>
<p>AlexNet使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算，AlexNet使用两个GTX580的GPU进行训练，单个GTX580只有3GB的显存，限制了可训练网络的最大规模，因此将其分布在两个GPU上，在每个GPU的显存中储存一般的神经元参数。</p>
<p><strong>有多少层需要训练</strong></p>
<p>整个AlexNet有8个需要训练参数的层，不包括池化层和LRN层，前5层为卷积层，后3层为全连接层，AlexNet的最后一层是由1000类输出的Softmax层用作分类，LRN层出现在第一个和第二个卷积层之后，最大池化层出现在两个LRN之后和最后一个卷积层之后。</p>
<p><strong>每层的超参数、参数量、计算量：</strong></p>
<p><img src="https://img-blog.csdn.net/2018042015212820?" alt="这里写图片描述"></p>
<p>虽然前几个卷积层的计算量很大，但是参数量都很小，在1M左右甚至更小。只占AlexNet总参数量的很小一部分，这就是卷积层的作用，可以通过较小的参数量有效的提取特征。</p>
<p><strong>为什么使用多层全连接：</strong></p>
<ul>
<li>
<p>全连接层在CNN中起到分类器的作用，前面的卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间，全连接层是将学到的特征映射映射到样本标记空间，就是矩阵乘法，再加上激活函数的非线性映射，多层全连接层理论上可以模拟任何非线性变换。但缺点也很明显: 无法保持空间结构。</p>
</li>
<li>
<p>由于全连接网络的冗余（占整个我拿过来参数的80%），近期一些好的网络模型使用全局平均池化（GAP）取代FC来融合学到的深度特征，最后使用softmax等损失函数作为网络目标函数来指导学习过程，用GAP替代FC的网络通常有较好的预测性能。</p>
</li>
<li>
<p>全连接的一个作用是维度变换，尤其是可以把高维变到低维，同时把有用的信息保留下来。全连接另一个作用是隐含语义的表达(embedding)，把原始特征映射到各个隐语义节点(hidden node)。对于最后一层全连接而言，就是分类的显示表达。不同channel同一位置上的全连接等价与1x1的卷积。N个节点的全连接可近似为N个模板卷积后的均值池化(GAP)。</p>
</li>
</ul>
<p><strong>GAP：假如最后一层的数据是10个6<em>6的特征图，global average pooling是将每个特征图计算所有像素点的均值，输出一个数据值，10个特征图就会输出10个值，组成一个1</em>10的特征向量。</strong></p>
<ul>
<li>
<p>用特征图直接表示属于某类的置信率，比如有10个输出，就在最后输出10个特征图，每个特征图的值加起来求均值，然后将均值作为其属于某类的置信值，再输入softmax中，效果较好。</p>
</li>
<li>
<p>因为FC的参数众多，这么做就减少了参数的数量（在最近比较火的模型压缩中，这个优势可以很好的压缩模型的大小）。</p>
</li>
<li>
<p>因为减少了参数的数量，可以很好的减轻过拟合的发生。</p>
</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180908231728181?" alt="这里写图片描述"></p>
<p><strong>为什么过了20年才卷土重来：</strong></p>
<p><strong>1. 大规模有标记数据集的出现，防止以前不可避免的过拟合现象</strong></p>
<p>**2. 计算机硬件的突飞猛进，卷积神经网络对计算机的运算要求比较高，需要大量重复可并行化的计算，在当时CPU只有单核且运算能力比较低的情况下，不可能进行个很深的卷积神经网络的训练。随着GPU计算能力的增长，卷积神经网络结合大数据的训练才成为可能。 **</p>
<p><strong>3. 卷积神经网络有一批一直在坚持的科学家（如Lecun）才没有被沉默，才没有被海量的浅层方法淹没。然后最后终于看到卷积神经网络占领主流的曙光。</strong></p>
<h3><a id="10ZFNet_556"></a>10、ZFNet</h3>
<p>和AlexNet很像，只是把参数优化了。</p>
<p>2012年AlexNet出尽了风头，ILSVRC 2013就有一大批CNN模型冒了出来。2013年的冠军是纽约大学Matthew Zeiler 和 Rob Fergus设计的网络 ZF Net，错误率 11.2%。ZF Net模型更像是AlexNet架构的微调优化版，但还是提出了有关优化性能的一些关键想法。还有一个原因，这篇论文写得非常好，论文作者花了大量时间阐释有关卷积神经网络的直观概念，展示了将滤波器和权重可视化的正确方法。</p>
<p>在这篇题为“Visualizing and Understanding Convolutional Neural Networks”的论文中，Zeiler和Fergus从大数据和GPU计算力让人们重拾对CNN的兴趣讲起，讨论了研究人员对模型内在机制知之甚少，一针见血地指出“发展更好的模型实际上是不断试错的过程”。虽然我们现在要比3年前知道得多一些了，但论文所提出的问题至今仍然存在!这篇论文的主要贡献在于提出了一个比AlexNet稍微好一些的模型并给出了细节，还提供了一些制作可视化特征图值得借鉴的方法。</p>
<h4><a id="101__565"></a>10.1 意义</h4>
<p>该论文是在AlexNet基础上进行了一些细节的改动，网络结构上并没有太大的突破。该论文最大的贡献在于通过使用可视化技术揭示了神经网络各层到底在干什么，起到了什么作用。</p>
<p>从科学的观点出发，如果不知道神经网络为什么取得了如此好的效果，那么只能靠不停的实验来寻找更好的模型。</p>
<p>使用一个多层的反卷积网络来可视化训练过程中特征的演化及发现潜在的问题；同时根据遮挡图像局部对分类结果的影响来探讨对分类任务而言到底那部分输入信息更重要。</p>
<h4><a id="102__573"></a>10.2 实现方法</h4>
<p><strong>训练过程：</strong></p>
<p>对前一层的输入进行卷积 -&gt; relu -&gt; max pooling(可选) -&gt;  局部对比操作(可选) -&gt; 全连接层 -&gt; softmax分类器。</p>
<p>输入是(x,y)，计算y与y的估计值之间的交叉熵损失，反向传播损失值的梯度，使用随机梯度下降算法来更新参数（w和b）以完成模型的训练。</p>
<p><strong>反卷积可视化：</strong></p>
<p>一个卷积层加一个对应的反卷积层；</p>
<p>输入是feature map，输出是图像像素；</p>
<p>过程包括反池化操作、relu和反卷积过程。</p>
<p><strong>反池化：</strong></p>
<p>严格意义上的反池化是无法实现的。作者采用近似的实现，在训练过程中记录每一个池化操作的一个z*z的区域内输入的最大值的位置，这样在反池化的时候，就将最大值返回到其应该在的位置，其他位置的值补0。</p>
<p><strong>relu:</strong></p>
<p>卷积神经网络使用relu非线性函数来保证输出的feature map总是为正数。在反卷积的时候，也需要保证每一层的feature map都是正值，所以这里还是使用relu作为非线性激活函数。</p>
<p><strong>滤波：</strong></p>
<p>使用原卷积核的转秩和feature map进行卷积。反卷积其实是一个误导，这里真正的名字就是转秩卷积操作。</p>
<p><img src="https://img-blog.csdn.net/20180426214214964?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>上图左边是一个解卷积层，右边为一个卷积层，解卷积层将会重建一个来自下一层的卷积特征近似版本，图中使用switch来记录在卷积网中进行最大池化操作时每个池化区域的局部最大值的位置，经过非池化操作后，原来的非最大值的位置全都置为0。</p>
<p><strong>预处理：</strong></p>
<p>网络对输入图片进行预处理，裁剪图片中间的256x256区域，并减去整个图像每个像素的均值，然后用10个不同的对256x256图像进行224x224的裁剪（中间区域加上四个角落，以及他们的水平翻转图像），对以128个图片分的块进行随机梯度下降法来更新参数。起始学习率为$10 ^{−2} $ ，动量为0.9，当验证集误差停滞时，手动调整学习率。在全连接网络中使用概率为0.5的dropout，并且所有权值都初始化为$10 ^{−2} $ ，偏置设为0。</p>
<p>在训练时第一层的可视化揭露了一些占主导的因素，为了了解这些，我们采用重新归一化每个卷积层的滤波器，这些滤波器的均方根值超过了一个固定半径的$10 ^{−1} $  。这是非常关键的，尤其是在模型中的第一层，因为输出图片大约在[-128,128]的范围内。</p>
<p><strong>特征可视化：</strong></p>
<p>每个特征单独投影到像素空间揭露了不同的结构能刺激不同的一个给定的特征图，因此展示了它对于变形的输入内在的不变性。下图即在一个已经训练好的网络中可视化后的图。</p>
<h4><a id="103__615"></a>10.3 训练细节</h4>
<p>网络结构类似于AlexNet，有两点不同，一是将3，4，5层的变成了全连接，二是卷积核的大小减小。</p>
<p>图像预处理和训练过程中的参数设置也和AlexNet很像。</p>
<ul>
<li>
<p>AlexNet用了1500万张图像，ZFNet用了130万张图像。</p>
</li>
<li>
<p>AlexNet在第一层中使用了大小为11×11的滤波器，而ZF使用的滤波器大小为7x7，整体处理速度也有所减慢。做此修改的原因是，对于输入数据来说，第一层卷积层有助于保留大量的原始象素信息。11×11的滤波器漏掉了大量相关信息，特别是因为这是第一层卷积层。</p>
</li>
<li>
<p>随着网络增大，使用的滤波器数量增多。</p>
</li>
<li>
<p>利用ReLU的激活函数，将交叉熵代价函数作为误差函数，使用批处理随机梯度下降进行训练。</p>
</li>
<li>
<p>使用一台GTX 580 GPU训练了12天。</p>
</li>
<li>
<p>开发可视化技术“解卷积网络”(Deconvolutional Network)，有助于检查不同的特征激活和其对输入空间关系。名字之所以称为“deconvnet”，是因为它将特征映射到像素(与卷积层恰好相反)。</p>
</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180426214251501?" alt="这里写图片描述"></p>
<p><strong>解卷积层DeConvNet：</strong></p>
<p>DeConvNet工作的基本原理是，每层训练过的CNN后面都连一层“deconvet”，它会提供一条返回图像像素的路径。输入图像进入CNN之后，每一层都计算激活。然而向前传递。现在，假设我们想知道第4层卷积层某个特征的激活值，我们将保存这个特征图的激活值，并将这一层的其他激活值设为0，再将这张特征图作为输入送入deconvnet。Deconvnet与原来的CNN拥有同样的滤波器。输入经过一系列unpool(maxpooling倒过来)，修正，对前一层进行过滤操作，直到输入空间满。</p>
<p>这一过程背后的逻辑在于，我们想要知道是激活某个特征图的是什么结构。下面来看第一层和第二层的可视化。<br>
<img src="https://img-blog.csdn.net/20180426215517209?" alt="这里写图片描述"></p>
<p>ConvNet的第一层永远是低层特征检测器，在这里就是对简单的边缘、颜色进行检测。第二层就有比较圆滑的特征了。再来看第三、第四和第五层。</p>
<p><img src="https://img-blog.csdn.net/2018042621552755?" alt="这里写图片描述"></p>
<p>第二层应对角落和其他边缘或者颜色的结合；第三层有更加复杂的不变性，捕捉到了相似的纹理；第四层显示了特定类间显著的差异性；第五层显示了有显著构成变化的整个物体。</p>
<p>这些层展示出了更多的高级特征，比如狗的脸和鲜花。值得一提的是，在第一层卷积层后面，我们通常会跟一个池化层将图像缩小(比如将 32x32x32 变为16x16x3)。这样做的效果是加宽了第二层看原始图像的视野。更详细的内容可以阅读论文。</p>
<p><strong>训练时的特征演变过程：</strong></p>
<p>外表突然的变化导致图像中的一个变换即产生了最强烈的激活。模型的底层在少数几个epoches就能收敛聚集，然而上层在一个相当多的epoches(40-50)之后才能有所变化，这显示了让模型完全训练到完全收敛的必要性。可以由下图看到颜色对比度都逐步增强。<br>
<img src="https://img-blog.csdn.net/20180427104509734?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>
<strong>特征不变性：</strong></p>
<p>一般来说，小的变化对于模型的第一层都有非常大的影响，但对于最高层的影响却几乎没有。对于图像的平移、尺度、旋转的变化来说，网络的输出对于平移和尺度变化都是稳定的，但却不具有旋转不变性，除非目标图像时旋转对称的。下图为分别对平移，尺度，旋转做的分析图。</p>
<p><img src="https://img-blog.csdn.net/20180427104518188?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>上图按行顺序分别为对5类图像进行不同程度的垂直方向上的平移、尺度变换、旋转对输出结果影响的分析图。按列顺序分别为原始变换图像，第一层中原始图片和变换后的图片的欧氏距离，第7层中原始图片和变换后的图片的欧氏距离，变换后图片被正确分类的概率图。</p>
<p>可视化不仅能够看到一个训练完的模型的内部操作，而且还能帮助选择好的网络结构。</p>
<p><strong>ZF Net为什么重要?</strong></p>
<p>ZF Net不仅是2013年比赛的冠军，还对CNN的运作机制提供了极好的直观信息，展示了更多提升性能的方法。论文所描述的可视化方法不仅有助于弄清CNN的内在机理，也为优化网络架构提供了有用的信息。Deconv可视化方法和 occlusion 实验也让这篇论文成了我个人的最爱。</p>
<h4><a id="104__676"></a>10.4 卷积网络可视化</h4>
<p><strong>特征可视化：</strong></p>
<p>通过对各层卷积核学习到的特征进行可视化发现神经网络学习到的特征存在层级结构。第二层是学习到边缘和角点检测器，第三层学习到了一些纹理特征，第四层学习到了对于指定类别图像的一些不变性的特征，例如狗脸、鸟腿，第五层得到了目标更显著的特征并且获取了位置变化信息。</p>
<p><strong>训练过程中的特征演化：</strong></p>
<p>低层特征经过较少epoch的训练过程之后就学习的比较稳定了，层数越高越需要更多的epoch进行训练。因此需要足够多的epoch过程来保证顺利的模型收敛。</p>
<p><strong>特征不变性：</strong></p>
<p>卷积神经网络具有平移和缩放不变性，并且层数越高不变性越强。但是不具有旋转不变性。</p>
<p><strong>特征结构选择：</strong></p>
<p>作者通过可视化AlexNet第一层和第二层的特征，发现比较大的stride和卷积核提取的特征不理想，所以作者将第一层的卷积核从11<em>11减小到7</em>7，将stride从4减小到2，实验说明，这样有助于分类性能的提升。</p>
<p><strong>遮挡实验：</strong></p>
<p>遮挡实验说明图像的关键区域被遮挡之后对分类性能有很大的影响，说明分类过程中模型明确定位出了场景中的物体。</p>
<p><strong>一致性分析：</strong></p>
<p>不同图像的指定目标局部块之间是否存在一致性的关联，作者认为深度模型可能默认学习到了这种关联关系。作者通过对五张不同的狗的图像进行局部遮挡，然后分析原图和遮挡后的图像的特征之间的汉明距离的和值，值越小说明一致性越大。实验表明，对不同的狗的图像遮挡左眼、右眼和鼻子之后的汉明距离小于随机遮挡，证明存在一定的关联性。</p>
<h4><a id="106__703"></a>10.6 总结</h4>
<ul>
<li>
<p>提出了一种可视化方法；</p>
</li>
<li>
<p>发现学习到的特征远不是无法解释的，而是特征间存在层次性，层数越深，特征不变性越强，类别的判别能力越强；</p>
</li>
<li>
<p>通过可视化模型中间层，在alexnet基础上进一步提升了分类效果；</p>
</li>
<li>
<p>遮挡实验表明分类时模型和局部块的特征高度相关；</p>
</li>
<li>
<p>模型的深度很关键；</p>
</li>
<li>
<p>预训练模型可以在其他数据集上fine-tuning得到很好的结果。</p>
</li>
</ul>
<h3><a id="11VGGNet_728"></a>11、VGGNet</h3>
<p>很适合做迁移学习，提到了一系列，VGG-16、VGG-19，不同层，参数也不同，最后选择了D的参数，结果最好。</p>
<p>传统的网络训练19层的网络很不容易，很厉害。</p>
<p><img src="https://img-blog.csdn.net/20180718174141928?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>VGGNet是牛津大学计算机视觉组（Visual?Geometry?Group）和Google DeepMind公司的研究员一起研发的的深度卷积神经网络。</p>
<p>VGGNet探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠3<em>3的小型卷积核和2</em>2的最大池化层，VGGNet成功地构筑了16~19层深的卷积神经网络。VGGNet相比之前state-of-the-art的网络结构，错误率大幅下降，并取得了ILSVRC 2014比赛分类项目的第2名和定位项目的第1名。</p>
<p>VGGNet论文中全部使用了3<em>3的卷积核和2</em>2的池化核，通过不断加深网络结构来提升性能。下图所示为VGGNet各级别的网络结构图，和每一级别的参数量，从11层的网络一直到19层的网络都有详尽的性能测试。</p>
<p><img src="https://img-blog.csdn.net/20180420161410550?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>A网络（11层）有8个卷积层和3个全连接层，E网络（19层）有16个卷积层和3个全连接层，卷积层宽度（通道数）从64到512，每经过一次池化操作，扩大一倍。</strong></p>
<h4><a id="111__747"></a>11.1 结构</h4>
<ul>
<li>
<p>输入：训练时输入大小为224x224大小的RGB图像；</p>
</li>
<li>
<p>预处理：在训练集中的每个像素减去RGB的均值</p>
</li>
<li>
<p>卷积核：3x3大小的卷积核，有的地方使用1x1的卷积，这种1x1的卷积可以被看做是对输入通道的线性变换。</p>
</li>
<li>
<p>步长：步长stride为1</p>
</li>
<li>
<p>填充：填充1个像素</p>
</li>
<li>
<p>池化：max-pooling，共有5层在一部分卷积层之后，连接的max-pooling的窗口是2x2，步长为2</p>
</li>
<li>
<p>全连接层：前两个全连接层均有4096个通道，第三个全连接层由1000个通道，用来分类。所有网络的全连接层配置相同。</p>
</li>
<li>
<p>激活函数：ReLU</p>
</li>
<li>
<p>不使用LRN，这种标准化并不能带来很大的提升，反而会导致更多的内存消耗和计算时间</p>
</li>
</ul>
<p><img src="https://img-blog.csdn.net/20180420163042232?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>相比AlexNet的变化：</strong></p>
<ul>
<li>LRN层作用不大，还耗时，抛弃</li>
<li>网络越深，效果越好</li>
<li>卷积核使用更小的卷积核，比如3x3</li>
</ul>
<p>VGG虽然比AlexNet模型层数多，且每轮训练时间会比AlexNet更长，但是因为更深的网络和更小的卷积核带来的隐式正则化结果，需要的收敛的迭代次数减小了许多。</p>
<p><strong>要点：</strong></p>
<p>这里使用3x3的滤波器和AlexNet在第一层使用11x11的滤波器和ZF Net 7x7的滤波器作用完全不同。作者认为两个3x3的卷积层组合可以实现5x5的有效感受野。这就在保持滤波器尺寸较小的同时模拟了大型滤波器，减少了参数。此外，有两个卷积层就能够使用两层ReLU。</p>
<ul>
<li>
<p>3卷积层具有7x7的有效感受野。</p>
</li>
<li>
<p>每个maxpool层后滤波器的数量增加一倍。进一步加强了缩小空间尺寸，但保持深度增长的想法。</p>
</li>
<li>
<p>图像分类和定位任务都运作良好。</p>
</li>
<li>
<p>使用Caffe工具包建模。</p>
</li>
<li>
<p>训练中使用scale jittering的数据增强技术。</p>
</li>
<li>
<p>每层卷积层后使用ReLU层和批处理梯度下降训练。</p>
</li>
<li>
<p>使用4台英伟达Titan Black GPU训练了两到三周。</p>
</li>
</ul>
<p><strong>为什么重要?</strong></p>
<p>VGG Net是最重要的模型之一，因为它再次强调CNN必须够深，视觉数据的层次化表示才有用。深的同时结构简单。</p>
<h4><a id="112__823"></a>11.2 网络特点：</h4>
<ol>
<li>VGGNet使用3x3的卷积</li>
</ol>
<p>AlexNet和ZFNet在第一个卷积层的卷积分别是11x11 、步长为4，7x7、步长为2）</p>
<ol start="2">
<li>使用三个3x3的卷积，而不是一个7x7的卷积</li>
</ol>
<p>两个连续的3x3的卷积相当于5x5的感受野，三个相当于7x7的感受野，优势在于：① 包含三个ReLU层而不是一个，使得决策函数更有判别性；② 减少了参数，比如输入输出都是c个通道，使用3x3的3个卷积层需要3（3x3xCxC）=27xCxC，使用7x7的1个卷积层需要7x7xCxC=49xCxC，这可以看做是为7x7x的卷积施加一种正则化，使它分解为3个3x3的卷积。</p>
<ol start="3">
<li>使用1x1的卷积层，该层主要是为了增加决策函数的非线性，而不影响卷积层的感受野，虽然1x1的卷积操作是线性的，但是ReLU增加了非线性</li>
</ol>
<h4><a id="113__836"></a>11.3 分类框架：</h4>
<p><strong>a. 训练过程：</strong></p>
<p>除了从多尺度的训练图像上采样输入图像外，VGGNet和AlexNet类似</p>
<p>**优化方法：**是含有动量的随机梯度下降SGD+momentum（0.9）</p>
<p><strong>批尺度：</strong> batch size = 256</p>
<p>**正则化：**采用L2正则化，weight decay 是5e-4，dropout在前两个全连接层之后，p=0.5</p>
<p><strong>为什么能在相比AlexNet网络更深，参数更多的情况下，VGGNet能在更少的周期内收敛：</strong></p>
<p>① 更大的深度和更小的卷积核带来隐式正则化；② 一些层的预训练</p>
<p><strong>参数初始化：</strong></p>
<p>对于较浅的A网络，参数进行随机初始化，权值w从N（0，0.01）中采样，偏差bias初始化为0；对于较深的网络，先用A网络的参数初始化前四个卷积层和三个全连接层。</p>
<p><strong>数据预处理：</strong></p>
<p>为了获得224x224的输入图像，要在每个SGD迭代中对每张重新缩放的图像进行随机裁剪，为了增强数据集，裁剪的图像还要随机水平翻转和RGB色彩偏移。</p>
<p><strong>b. 测试过程</strong></p>
<ol>
<li>对输入图像重新缩放到一个预定义的最小图像边的尺寸Q</li>
<li>网络密集地应用在重缩放后的图像上，也就是说全连接层转化为卷积层（第一个全连接层转化为7x7的卷积层，后两个全连接层转化为1x1的卷积层），然后将转化后的全连接层应用在整张图中）</li>
<li>为了获得固定尺寸的类别分数向量，对分数图进行空间平均化处理。</li>
</ol>
<p><strong>c. 实现</strong></p>
<p>基于C++ Caffe，进行一些重要修改，在单系统多GPU上训练。<br>
在装有4个NVIDIA Titan Black GPUs的电脑上，训练一个网络需要2-3周。</p>
<h3><a id="12GoogLeNet_873"></a>12、GoogLeNet</h3>
<p>ImageNet 2014比赛分类任务的冠军，将错误率降低到了6.656%，突出的特点是大大增加了卷积神经网络的深度。</p>
<p>将最后的全连接层都换成了1x1的卷积层，大大加速了训练速率。</p>
<h4><a id="121_GoogLeNet_Inception_V122_880"></a>12.1 GoogLeNet Inception V1——22层</h4>
<p>GoogLeNet Incepetion V1<a href="http://arxiv.org/abs/1409.4842" rel="nofollow">《Going deeper with convolutions》</a>。之所以名为“GoogLeNet”而非“GoogleNet”,文章说是为了向早期的LeNet致敬。</p>
<p><strong>1）动机：</strong></p>
<p>深度学习以及神经网络快速发展，人们不再只关注更给力的硬件、更大的数据集、更大的模型，而是更在意新的idea、新的算法以及模型的改进。</p>
<p>一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，这也就意味着巨量的参数。但是，巨量参数容易产生过拟合也会大大增加计算量。</p>
<p>文章认为解决上述两个缺点的根本方法是将全连接甚至一般的卷积都转化为稀疏连接。一方面现实生物神经系统的连接也是稀疏的，另一方面有文献1表明：对于大规模稀疏的神经网络，可以通过分析激活值的统计特性和对高度相关的输出进行聚类来逐层构建出一个最优网络。这点表明臃肿的稀疏网络可能被不失性能地简化。 虽然数学证明有着严格的条件限制，但Hebbian准则有力地支持了这一点：fire together,wire together。</p>
<p>早些的时候，为了打破网络对称性和提高学习能力，传统的网络都使用了随机稀疏连接。但是，计算机软硬件对非均匀稀疏数据的计算效率很差，所以在AlexNet中又重新启用了全连接层，目的是为了更好地优化并行运算。</p>
<p>所以，现在的问题是有没有一种方法，既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。大量的文献表明可以将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能，据此论文提出了名为Inception 的结构来实现此目的。</p>
<p><img src="https://img-blog.csdn.net/20180712111315378?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>
2012年AlexNet做出历史突破以来，直到GoogLeNet出来之前，主流的网络结构突破大致是网络更深（层数），网络更宽（神经元数）。所以大家调侃深度学习为“深度调参”，但是纯粹的增大网络的缺点：</p>
<ol>
<li>参数太多，容易过拟合，若训练数据集有限；</li>
<li>网络越大计算复杂度越大，难以应用；</li>
<li>网络越深，梯度越往后穿越容易消失（梯度弥散），难以优化模型</li>
</ol>
<p>那么解决上述问题的方法当然就是增加网络深度和宽度的同时减少参数，Inception就是在这样的情况下应运而生。</p>
<p><strong>2）网络结构</strong></p>
<p>Inception 结构的主要思路是怎样用密集成分来近似最优的局部稀疏结构，基本结构如下：</p>
<p><img src="https://img-blog.csdn.net/20180421230012284?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>上图说明：</p>
<ol>
<li>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合</li>
<li>之所以卷积核采用1x1，3x3和5x5，主要是为了方便对齐，设定卷积步长stride=1，只要分别设定padding=0，1，2，那么卷积之后便可以得到相同维度的特征，然后将这些特征就可以直接拼接在一起了。</li>
<li>文章中说pooling被证明很有效，所以网络结构中也加入了</li>
<li>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也变大了，因此随着层数的增加，3x3和5x5的比例也要增加。</li>
</ol>
<p>但是使用5x5的卷积核仍然会带来巨大的计算量，为此文章借鉴NIN，采用1x1的卷积核来进行降维。</p>
<p>例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。</p>
<p><strong>卷积层参数数量计算：</strong></p>
<pre><code>输入通道数为K，输出通道数为L，那么卷积核个数为K*L。因为高维卷积计算是多个通道与多个卷积核分别进行二维计算，
所以K个通道会需要K个卷积核，计算之后，合并也就是相加得到一个通道，又因为输出通道为L，所以需要K*L个卷积核。

然后就是如何求解参数数量？

其实很简单，就是卷积核个数乘以卷积核尺寸，para=I*J*K*L。
</code></pre>
<p>输入是3个32<em>32, 共3</em>1024=3072。每条边padding为2，则内存里实际为3个36*36.</p>
<p>卷积核个数是3维的5<em>5分别与3个输入进行卷积运算,得到3维的32</em>32的输出,这里将3维的32<em>32对应位相加得到一张32</em>32的feature Map</p>
<p>如果有64个3维的5*5卷积核就有64张feature Map</p>
<p>具体过程图示为：<br>
<img src="https://img-blog.csdn.net/20180712113734571?" alt="这里写图片描述"></p>
<p>卷积的权值存取方式为:</p>
<p>第1个5*5作用于第一张输入全图，</p>
<p>第2个5*5作用于第二张输入全图，</p>
<p>第3个5*5作用于第三张输入全图，</p>
<p>再把这三个对应位置相加,在加上biases,得到第一张feature map</p>
<p>最后64个5<em>5</em>3重复上面的过程，得到64个featuremap</p>
<p>这里weights有3<em>5</em>5*64个,biases有64个.</p>
<p>这里输入是3 输出是64，卷积核是5<em>5权值个数是64</em> 5<em>5</em>3</p>
<p><strong>具体改进后的Inception Module如下：</strong></p>
<p><img src="https://img-blog.csdn.net/20180421230617539?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>GoogLeNet Incepetion V1比AlexNet的8层或者VGGNet的19层还要更深。但其计算量只有15亿次浮点运算，同时只有500万的参数量，仅为AlexNet参数量（6000万）的1/12，却可以达到远胜于AlexNet的准确率，可以说是非常优秀并且非常实用的模型。</p>
<p><strong>Inception V1降低参数量的目的有两点：</strong></p>
<ul>
<li>
<p>第一，参数越多模型越庞大，需要供模型学习的数据量就越大，而目前高质量的数据非常昂贵；</p>
</li>
<li>
<p>第二，参数越多，耗费的计算资源也会更大。</p>
</li>
</ul>
<p><strong>Inception V1参数少但效果好的原因除了模型层数更深、表达能力更强外，还有两点：</strong></p>
<ul>
<li>
<p>其一，去除了最后的全连接层，用全局平均池化层（即将图片尺寸变为1*1）来取代它。全连接层几乎占据了AlexNet或VGGNet中90%的参数量，而且会引起过拟合，去除全连接层后模型训练更快并且减轻了过拟合。</p>
</li>
<li>
<p>其二，Inception V1中精心设计的Inception Module提高了参数的利用效率，其结构如上图所示。这一部分也借鉴了Network In Network的思想，形象的解释就是Inception  Module本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。</p>
</li>
</ul>
<p><strong>Inception  Module的基本结构：</strong></p>
<p>其中有4个分支：</p>
<ul>
<li>
<p>第一个分支对输入进行1<em>1的卷积，这其实也是NIN中提出的一个重要结构。1</em>1的卷积是一个非常优秀的结构，它可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维。<br>
Inception Module的4个分支都用到了1x1的卷积，来进行低成本（计算量比3x3小很多）的跨通道的特征变换</p>
</li>
<li>
<p>第二个分支，先使用了1x1卷积，然后连接3x3卷积，相当于进行了两次特征变换</p>
</li>
<li>
<p>第三个分支，先使用1x1卷积，然后连接5x5卷积</p>
</li>
<li>
<p>第四个分支，3x3最大池化后直接使用1x1卷积</p>
</li>
</ul>
<p>四个分支在最后通过一个聚合操作合并，在输出通道这个维度上聚合。</p>
<p>我们立刻注意到，并不是所有的事情都是按照顺序进行的，这与此前看到的架构不一样。我们有一些网络，能同时并行发生反应，这个盒子被称为 Inception 模型。</p>
<h4><a id="122_GoogLeNet_1003"></a>12.2 GoogLeNet</h4>
<p><img src="https://img-blog.csdn.net/20180421230708516?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>先小心翼翼的训练得到一组权重参数（第一个出来的分支），再利用这些参数作为初始化参数，训练网络，之后再进行一次初始化，训练得到22层的网络。</p>
<p>上图说明：</p>
<ol>
<li>GoogLeNet 采用了模块化的几个，方便增添和修改</li>
<li>网络最后采用了平均池化来代替全连接层，想法来自NIN，事实证明可以将TOP accuracy 提高0.6%，但是，实际在最后一层还是加了一个全连接层，主要为了方便之后的微调</li>
<li>虽然移除了全连接，但是网络中依然使用了Dropout</li>
<li>为了避免梯度消失，网络额外增加了2个辅助的softmax用于前向传导梯度，文章中说着两个辅助分类器的loss应该加一个衰减系数，但是caffe中的模型没有加任何衰减，此外，实际测试的时候，这两个额外的softmax会被去掉。</li>
</ol>
<p><strong>比较清晰的结构图：</strong></p>
<p><img src="https://img-blog.csdn.net/20180422093401246?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>结论：</strong></p>
<p>GoogLeNet是谷歌团队为了参加ILSVRC 2014比赛而精心准备的，为了达到最佳的性能，除了使用上述的网络结构外，还做了大量的辅助工作：包括训练多个model求平均、裁剪不同尺度的图像做多次验证等等。详细的这些可以参看文章的实验部分。</p>
<p>本文的主要想法其实是想通过构建密集的块结构来近似最优的稀疏结构，从而达到提高性能而又不大量增加计算量的目的。GoogleNet的caffemodel大小约50M，但性能却很优异。</p>
<p><img src="https://img-blog.csdn.net/20180718174544311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4><a id="123_GoogleNet_Inception_V2_1027"></a>12.3 GoogleNet Inception V2</h4>
<p><strong>V2和V1的最大的不同就是，V2增加了Batch Normalization。<a href="%E3%80%8ABatch%20Normalization:%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift%E3%80%8B" rel="nofollow">《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》</a></strong></p>
<p>Inception V2学习了VGGNet，用两个3x3的卷积代替5x5的卷积，用以降低参数量并减轻过拟合，还提出了著名的Batch Normalization方法，该方法是一个很有效的正则化的方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅度的提高。</p>
<p>BN在用于神经网络某层时，会对每一个mini-batch数据的内部进行标准化（normalization）处理，使输出规范化到N(0,1)的正态分布，减少了Internal Covariate Shift（内部神经元分布的改变）。</p>
<p>BN的论文指出，传统的深度神经网络在训练时，每一层的输入的分布都在变化，导致训练变得困难，我们只能使用一个很小的学习速率解决这个问题。而对每一层使用BN之后，我们就可以有效地解决这个问题，学习速率可以增大很多倍，达到之前的准确率所需要的迭代次数只有1/14，训练时间大大缩短。</p>
<p>而达到之前的准确率后，可以继续训练，并最终取得远超于Inception V1模型的性能——top-5错误率4.8%，已经优于人眼水平。因为BN某种意义上还起到了正则化的作用，所以可以减少或者取消Dropout，简化网络结构。</p>
<p><strong>12.3.1 动机：</strong></p>
<p>文章作者认为，网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使得每一层适应输入的分布，因此我们不得不降低学习率、小心的初始化，分布发生变化称为internal covariate shift。</p>
<p><strong>一般在训练网络时会对数据进行预处理，包括去掉均值、白化操作等，目的是为了加快训练：</strong></p>
<p><strong>去均值化：</strong></p>
<p>首先，图像数据是高度相关的，假设其分布如下图a所示（简化为2维），由于初始化的时候，我们的参数一般都是0均值的，因此开始的拟合y=Wx+b基本过原点附近，如图b红色虚线，因此，网络需要经过多次学习才能逐步达到紫色实线的拟合，即收敛的比较慢。</p>
<p>如果对数据先去均值化，如图c，显然可以加快学习，更进一步的，我们对数据在进行去相关操作，使得数据更加容易区分，这样又会加快训练，如图d。</p>
<p>去均值是一种常用的数据处理方式，它是将各个特征值减去其均值，几何上的展现是可以将数据的中心移到坐标原点，python代码为<code>X=X-np.mean(X,axis=0)</code>，对于图像来说，就是对每个像素的值都要减去平均值。</p>
<p><img src="https://img-blog.csdn.net/20180422100112132?" alt="这里写图片描述"></p>
<p><strong>PCA：</strong></p>
<p>由于计算需要，需要实现进行前面所说的均值0化。<br>
PCA要做的是将数据的主成分找出。流程如下：</p>
<ol>
<li>计算协方差矩阵</li>
<li>求特征值和特征向量</li>
<li>坐标转换</li>
<li>选择主成分</li>
</ol>
<p>首先我们需要求出数据各个特征之间的协方差矩阵，以得到他们之间的关联程度，Python代码如下：</p>
<pre><code># Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix，公式含义可按照协方差矩阵的定义得到
</code></pre>
<p>其中得到的矩阵中的第（i,j）个元素代表第i列和第j列的协方差，对角线代表方差。协方差矩阵是对称半正定矩阵可以进行SVD分解：</p>
<pre><code>U,S,V = np.linalg.svd(cov)1
</code></pre>
<p>U 的列向量是特征向量， S 是对角阵其值为奇异值也是特征值的平方.奇异值分解的直观展示：</p>
<p><img src="https://img-blog.csdn.net/20180422135253289?" alt="这里写图片描述"></p>
<p><a href="https://blog.csdn.net/bea_tree/article/details/51288570">详细介绍</a></p>
<p>我们可以用特征向量（正交且长度为1可以看做新坐标系的基）右乘X（相当于旋转坐标系）就可以得到新坐标下的无联系（正交）的特征成分：</p>
<pre><code>Xrot = np.dot(X, U) # decorrelate the data1
</code></pre>
<p>注意上面使用的np.linalg.svd(）已经将特征值按照大小排序了，这里仅需要取前几列就是取前几个主要成分了（实际使用中我们一般按照百分比取），代码：</p>
<pre><code>Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
</code></pre>
<p><strong>白化：</strong></p>
<p>白化的方式有很多种，常用的有PCA白化，就是对数据进行PCA操作之后，再进行方差归一化，这样数据基本满足0均值、单位方差、弱相关性。</p>
<p>作者首先尝试了对每一层数据都是用白化操作，但分析认为这是不可取的，因为白化需要计算协方差矩阵、求逆等操作，计算量很大，此外，反向传播时，白化操作不一定可导，于是使用了Normalization的操作。</p>
<pre><code># whiten the data:
# divide by the eigenvalues (which are square roots of the singular values)
Xwhite = Xrot / np.sqrt(S + 1e-5)
</code></pre>
<p>但是白化因为将数据都处理到同一个范围内了，所以如果原始数据有原本影响不大的噪声，它原本小幅的噪声也会被放大到与全局相同的范围内了。</p>
<p>另外我们防止出现除以0的情况，在分母处多加了0.00001，如果增大它会使得噪声减小。</p>
<p>白化之后，得到的是一个多元高斯分布。</p>
<p>上面两种处理的结果如下：</p>
<p><img src="https://img-blog.csdn.net/20180422140608706?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>可以看出，经过PCA的去相关操作，将原始数据的坐标轴旋转，并且可以看出x方向的信息量比较大，如果只选择一个特征，那么久选横轴方向的特征，经过白化之后数据进入了相同的范围。</p>
<p>下面以处理之前提到过的CIFAR-10为例，看PCA和Whitening的作用：</p>
<p><img src="https://img-blog.csdn.net/20180422140808731?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>左边是原始图片，每张图片都是一个3072维的一行向量，经过PCA之后选取144维最重要的特征（左2），将特征转化到原来的坐标系<code>U.transpose()[:144,:]</code>得到了降维之后的图形（左3），图形变模糊了，说明我们的主要信息都是低频信息，关于高低频的含义在下一段展示一下，图片模糊了但是主要成分都还在，最后一个图是白化之后再转换坐标系之后的结果。</p>
<ul>
<li>
<p><strong>1. CNN不用进行PCA和白化，只需要进行零均值化就好</strong></p>
</li>
<li>
<p><strong>2. 注意进行所有的预处理时训练集、验证集、测试集都要使用相同的处理方法，比如减去相同的均值。</strong></p>
</li>
</ul>
<p><strong>12.3.2 对小批量进行统计特性的标准化</strong></p>
<p><strong>标准化 Normalization：</strong></p>
<p>标准化是将矩阵X中的Dimensions都保持在相似的范围内，有两种实现方式：</p>
<ol>
<li>
<p>先使得均值为0，然后除以标准差，<code>X=X / np.std(X, axis=0)</code></p>
</li>
<li>
<p>在数据不在同一范围，而且各个维度在同一范围内对算法比较重要时，可以将其最大最小值分别缩放为1和-1，对于图像处理而言，因为一般数据都在0~255之间，所以不用再进行这一步了。</p>
</li>
</ol>
<p><img src="https://img-blog.csdn.net/2018042213483958?" alt="这里写图片描述"></p>
<p>数据归一化的方法很简单，就是让数据具有0均值和单位方差：</p>
<p><img src="https://img-blog.csdn.net/20180422101546676?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>但是作者又说如果简单的这么干，会降低层的表达能力。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。</p>
<p><img src="https://img-blog.csdn.net/20180422101616189?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>因此，作者又为BN增加了2个参数，用来保持模型的表达能力，于是最后的输出为：</p>
<p><img src="https://img-blog.csdn.net/2018042210204019?" alt="这里写图片描述"></p>
<p>上述公式中用到了均值E和方差Var，需要注意的是理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。</p>
<p><strong>BN算法实现如下：</strong></p>
<p><img src="https://img-blog.csdn.net/2018042210211985?" alt="这里写图片描述"></p>
<p>输入：输入数据<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">x_1...x_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>（这些数据是准备进入激活函数的数据）</p>
<p>计算过程：</p>
<ol>
<li>求数据均值</li>
<li>求数据方差</li>
<li>数据进行标准化</li>
<li>训练参数<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma , \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span></li>
<li>输出y通过<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span></span></span></span></span> 与<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>的线性变换得到新的值</li>
</ol>
<p>正向传播的过程：通过可学习的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span></span></span></span></span>与<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>参数求出新的分布值</p>
<p>反向传播的过程：通过链式求导方式，求出<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>以及相关权值<br>
<img src="https://img-blog.csdn.net/20180422102606185?" alt="这里写图片描述"></p>
<p><strong>12.3.3 BN的意义：</strong></p>
<p><strong>解决的问题是梯度消失和梯度爆炸的问题</strong></p>
<p><strong>a. 关于梯度消失：</strong></p>
<p>以sigmoid函数为例，sigmoid函数使得输出在[0,1]之间：</p>
<p><img src="https://img-blog.csdn.net/20180422102801776?" alt="这里写图片描述"></p>
<p>事实上x到了一定大小，经过sigmoid函数的输出范围就很小了，如下图：</p>
<p><img src="https://img-blog.csdn.net/20180422103834834?" alt="这里写图片描述"></p>
<p>如果输入很大，其对应的斜率就很小，我们知道其斜率在反向传播中是权值学习速率，所以就会出现以下问题：</p>
<p><img src="https://img-blog.csdn.net/2018042210392131?" alt="这里写图片描述"></p>
<p>在深度网络中，如果网络的激活输出很大，那么其梯度就很小，学习速率就很慢，假设每层学习梯度都小于最大值0.25，网络有n层，因为链式求导的原因，第一层的梯度小于0.25的n次方，所以学习速率就慢，对于最后一层只需对自身求导1次，梯度就打，学习速率就快。</p>
<p><strong>影响：在一个很大的深度网络中，浅层基本不学习，权值变化小，后面几层一直在学习，结果就是后面几层基本可以表示整个网络，失去了深度的意义。</strong></p>
<p><strong>b. 关于梯度爆炸：</strong></p>
<p>根据链式求导法则：</p>
<p>第一层偏移量的梯度 = 激活层斜率1 x 权值1 x 激活层斜率2 x …激活层斜率(n-1) x 权值(n-1) x 激活层斜率n</p>
<p>假如激活层斜率均为最大值0.25，所有层的权值为100，这样梯度就会指数增加。</p>
<p><strong>12.3.4 BN在CNN中的用法</strong><br>
<img src="https://img-blog.csdn.net/20180422120844751?" alt="这里写图片描述"><br>
首先解释对图像的卷积是如何使用BN层的，上图是CNN中的5x5的图像通过valid卷积得到的3x3的特征图，<strong>特征图里边的值作为BN的输入</strong>，也就是这9个数值通过BN计算并保存<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，通过<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>使得输出与输入不变。</p>
<p>假设输入的batch_size=m，那就有m x 9个数值，计算这m x 9个数据的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>并保存，这就是正向传播的过程，反向传播就是根据求得的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>来计算梯度。</p>
<p><strong>重要说明：</strong></p>
<ol>
<li>
<p>网络训练中以batch_size为最小单位不断迭代，很显然新的batch_size进入网络，就会有新的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，因此在BN层中，共有<strong>总图像数 / batch_size</strong>组<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>被保存下来。</p>
</li>
<li>
<p>图像卷积过程中，通常是使用多个卷积核，得到多张特征图，对于多个的卷积核需要保存多个<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>。</p>
</li>
</ol>
<p><img src="https://img-blog.csdn.net/20180422121329647?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>输入：待进入激活函数的变量</p>
<p>输出：</p>
<ol>
<li>
<p>对于k维的输入，假设每一维包含m个变量，所以需要k个循环，每个循环中按照上面所介绍的方法计算<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，这里的k维，在卷积网络中可以看做卷积核个数，如网络中的第n层有64个卷积核，就需要计算64次。（注意：在正向传播时，会使用<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>使得BN层输入与输出一样。）</p>
</li>
<li>
<p>在反向传播时利用<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>求得梯度，从而改变训练权值（变量）</p>
</li>
<li>
<p>通过不断迭代直到结束，求得关于不同层的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，如果有n个BN层，每层根据batch_size决定有多少个变量，设定为m，这里的mini_batcher指的是<strong>特征图大小 x batch_size</strong>，即<strong>m=特征图大小 x batch_size</strong>，因此对于batch_size为1，这里的m就是每层特征图的大小。</p>
</li>
<li>
<p>不断遍历训练集中的图像，取出每个batch_size的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，最后统计每层BN的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>各自的和除以图像数量，得到平均值，并对其做无偏估计值作为每一层的E[x]与Var[x]。</p>
</li>
<li>
<p>在预测的正向传播时，对测试数据求取<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.05278em;">β</span></span></span></span></span>，并使用该层的E[x]与Var[x]，通过图中所表示的公式计算BN层的输出。</p>
</li>
</ol>
<p><strong>12.3.5 BN应该放在激活层之前还是之后</strong></p>
<p>作者在文章中说应该把BN放在激活函数之前，这是因为Wx+b具有更加一致和非稀疏的分布。但是也有人做实验表明放在激活函数后面效果更好。这是实验链接，里面有很多有意思的对比实验：<a href="https://github.com/ducha-aiki/caffenet-benchmark">https://github.com/ducha-aiki/caffenet-benchmark</a></p>
<p>**12.3.6 实验 **</p>
<p>作者在文章中也做了很多实验对比，我这里就简单说明2个。<br>
下图a说明，BN可以加速训练。图b和c则分别展示了训练过程中输入数据分布的变化情况。<br>
<img src="https://img-blog.csdn.net/20180422125054497?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>
下表是一个实验结果的对比，需要注意的是在使用BN的过程中，作者发现Sigmoid激活函数比Relu效果要好。</p>
<p><img src="https://img-blog.csdn.net/20180422125103937?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4><a id="124_GoogLeNet_Inception_V3_1345"></a>12.4 GoogLeNet Inception V3</h4>
<p>GoogLeNet凭借其优秀的表现，得到了很多研究人员的学习和使用，因此Google团队又对其进行了进一步发掘改进，产生了升级版本的GoogLeNet。这一节介绍的版本记为V3，文章为：<a href="http://arxiv.org/abs/1512.00567" rel="nofollow">《Rethinking the Inception Architecture for Computer Vision》。</a></p>
<h5><a id="1241__1349"></a>12.4.1 简介</h5>
<p>2014年以来，构建更深网络的同时逐渐成为主流，但是模型的变大也使得计算效率越来越低，这里文章试图找到不同方法来扩大网络的同时又尽可能的发挥计算性能。</p>
<p>首先，GoogLeNet V1出现的同期，性能与之接近的大概只有VGGNet了，并且二者在图像分类之外的很多领域都得到了成功的应用。但是相比之下，GoogLeNet的计算效率明显高于VGGNet，大约只有500万参数，只相当于Alexnet的1/12(GoogLeNet的caffemodel大约50M，VGGNet的caffemodel则要超过600M)。</p>
<p>GoogLeNet的表现很好，但是，如果想要通过简单地放大Inception结构来构建更大的网络，则会立即提高计算消耗。此外，在V1版本中，文章也没给出有关构建Inception结构注意事项的清晰描述。因此，在文章中作者首先给出了一些已经被证明有效的用于放大网络的通用准则和优化方法。这些准则和方法适用但不局限于Inception结构。</p>
<h5><a id="1242__1357"></a>12.4.2 一般情况的设计准则</h5>
<p>下面的准则来源于大量的实验，因此包含一定的推测，但实际证明基本都是有效的。</p>
<p><strong>1）避免表达瓶颈，特别是在网络靠前的地方</strong></p>
<p>信息流前向传播过程中显然不能经过高度压缩的层，即表达瓶颈。从input到output，feature map的宽和高基本都会逐渐变小，但是不能一下子就变得很小。比如你上来就来个kernel = 7, stride = 5 ,这样显然不合适。</p>
<p>另外输出的维度channel，一般来说会逐渐增多(每层的num_output)，否则网络会很难训练。（特征维度并不代表信息的多少，只是作为一种估计的手段）</p>
<p>这种情况一般发生在pooling层，字面意思是，pooling后特征图变小了，但有用信息不能丢，不能因为网络的漏斗形结构而产生表达瓶颈，解决办法是作者提出了一种特征图缩小方法，更复杂的池化。</p>
<p><strong>2）高维特征更容易处理</strong></p>
<p>高维特征更加容易区分，会加快训练</p>
<p><strong>3）可以在低维嵌入上进行空间汇聚而无需担心丢失很多信息</strong></p>
<p>比如在进行3x3卷积之前，可以对输入先进行降维而不会产生严重的后果，假设信息可以被简单的压缩，那么训练就会加快。</p>
<p><strong>4）平衡网络的深度和宽度</strong></p>
<p>上述的这些并不能直接用来提高网络质量，而仅用来在大环境下作指导。</p>
<h5><a id="1243__1381"></a>12.4.3 利用大尺度滤波器进行图像的卷积</h5>
<p>大尺寸的卷积核可以带来更大的感受野，但也意味着更多的参数，比如5x5卷积核参数是3x3卷积核的25/9=2.78倍。为此，作者提出可以用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，(保持感受野范围的同时又减少了参数量)如下图：</p>
<p><img src="https://img-blog.csdn.net/20180422182200400?" alt="这里写图片描述"></p>
<p><strong>问题：</strong></p>
<ol>
<li>这种代替会造成表达能力的下降吗？</li>
</ol>
<p>实验证明该操作不会造成表达缺失</p>
<ol start="2">
<li>3x3卷积之后还要再激活吗？</li>
</ol>
<p>添加非线性激活之后会提高性能</p>
<p>从上面来看，大卷积核完全可以由一系列的3x3卷积核来替代，那能不能分解的更小一点呢。文章考虑了 nx1 卷积核。</p>
<p>如下图所示的取代3x3卷积：</p>
<p><img src="https://img-blog.csdn.net/2018042218242225?" alt="这里写图片描述"></p>
<p>于是，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。实际上，作者发现在网络的前期使用这种分解效果并不好，还有在中度大小的feature map上使用效果才会更好。（对于mxm大小的feature map,建议m在12到20之间）。</p>
<p>总结如下图：</p>
<p><img src="https://img-blog.csdn.net/20180422182453287?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>(1) 图4是GoogLeNet V1中使用的Inception结构；</p>
<p>(2) 图5是用3x3卷积序列来代替大卷积核；</p>
<p>(3) 图6是用nx1卷积来代替大卷积核，这里设定n=7来应对17x17大小的feature map。该结构被正式用在GoogLeNet V2中。即非对称个卷积核，其实类似于卷积运算中，二维分解为1维计算，提高了计算速度。</p>
<h3><a id="13ResNet_1440"></a>13、ResNet</h3>
<p>ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名，因为它“简单与实用”并存，之后很多方法都建立在ResNet50或者ResNet101的基础上完成的，检测，分割，识别等领域都纷纷使用ResNet，Alpha zero也使用了ResNet，所以可见ResNet确实很好用。</p>
<p>训练神经网络的反向传播，导致很容易出现梯度消失。</p>
<p>对常规的网络（plain network，也称平原网络）直接堆叠很多层次，经对图像识别结果进行检验，训练集、测试集的误差结果如下图：</p>
<p><img src="https://img-blog.csdn.net/20180718183806737?" alt="这里写图片描述"></p>
<p>从上面两个图可以看出，在网络很深的时候（56层相比20层），模型效果却越来越差了（误差率越高），并不是网络越深越好。</p>
<p>通过实验可以发现：随着网络层级的不断增加，模型精度不断得到提升，而当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这说明当网络变得很深以后，深度网络就变得更加难以训练了。</p>
<p>下图是一个简单神经网络图，由输入层、隐含层、输出层构成：</p>
<p><img src="https://img-blog.csdn.net/2018071818391671?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>回想一下神经网络反向传播的原理，先通过正向传播计算出结果output，然后与样本比较得出误差值Etotal</p>
<p><img src="https://img-blog.csdn.net/2018071818394361?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>根据误差结果，利用著名的“链式法则”求偏导，使结果误差反向传播从而得出权重w调整的梯度。下图是输出结果到隐含层的反向传播过程（隐含层到输入层的反向传播过程也是类似）：<br>
<img src="https://img-blog.csdn.net/20180718183959304?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>通过不断迭代，对参数矩阵进行不断调整后，使得输出结果的误差值更小，使输出结果与事实更加接近。</p>
<p>从上面的过程可以看出，神经网络在反向传播过程中要不断地传播梯度，而当网络层数加深时，梯度在传播过程中会逐渐消失（假如采用Sigmoid函数，对于幅度为1的信号，每向后传递一层，梯度就衰减为原来的0.25，层数越多，衰减越厉害），导致无法对前面网络层的权重进行有效的调整。</p>
<p>那么，如何又能加深网络层数、又能解决梯度消失问题、又能提升模型精度呢？</p>
<h4><a id="131_ResNet_1473"></a>13.1 ResNet的提出</h4>
<p>前面描述了一个实验结果现象，在不断加神经网络的深度时，模型准确率会先上升然后达到饱和，再持续增加深度时则会导致准确率下降，示意图如下：</p>
<p><img src="https://img-blog.csdn.net/20180718184129622?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>那么我们作这样一个假设：假设现有一个比较浅的网络（Shallow Net）已达到了饱和的准确率，这时在它后面再加上几个恒等映射层（Identity mapping，也即y=x，输出等于输入），这样就增加了网络的深度，并且起码误差不会增加，也即更深的网络不应该带来训练集上误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想，便是著名深度残差网络ResNet的灵感来源。</p>
<p>ResNet引入了残差网络结构（residual network），通过这种残差网络结构，可以把网络层弄的很深（据说目前可以达到1000多层），并且最终的分类效果也非常好，残差网络的基本结构如下图所示，很明显，该图是带有跳跃结构的：<br>
<img src="https://img-blog.csdn.net/20180718184452540?" alt="这里写图片描述"></p>
<p>残差网络借鉴了高速网络的跨层连接的思想，但是对其进行了改进，残差项原本是带权值的，但ResNet用恒等映射代替了它。</p>
<p><strong>假设：</strong></p>
<p><strong>神经网络输入：x</strong><br>
<strong>期望输出：H(x)，即H(x)是期望的复杂映射，如果要学习这样的模型，训练的难度会比较大。</strong></p>
<p>此时，如果已经学习到较为饱和的准确率，或者发现下层的误差变大时，接下来的目标就转化为恒等映射的学习，也就是使得输入x近似于输出H(x)，以保持在后面的层次中不会造成精度下降。</p>
<p>上图的残差网络中，通过捷径连接的方式直接将输入x传到输出作为初始结果，输出结果为H(x)=F(x)+x，当F(x)=0时，H(x)=x，也就是恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H(X)和x的差值，也就是所谓的残差F(x) := H(x)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。</p>
<p><strong>学习的目标：目标值H(x)和输入x的差值，即F(x):=H(x)-x，将残差逼近于0，使得随着网络加深，准确率不下降。</strong></p>
<h4><a id="132_ResNet_1516"></a>13.2 ResNet的意义</h4>
<p><img src="https://img-blog.csdn.net/20180718174719715?" alt="这里写图片描述"></p>
<p>图像是层次非常深的数据，所以要层次深的网络来进行特征提取，网络深度是很有意义的。</p>
<p>一般的卷积神经网络，输入图像x，输出卷积后的特征F(x)，一次性抽出所有的信息，梯度消失会出现，Res网络就说只学习残差即可。</p>
<p><strong>第一条直接向下传递的网络：试图从x中直接学习残差F(x)</strong><br>
<strong>第二条捷径网络：输入x</strong><br>
<strong>整合：将残差和x相加，即H(x)=F(x)+x，也就是所要求的映射H(x)</strong></p>
<p><strong>好处：只有一条通路的反向传播，会做连乘导致梯度消失，但现在有两条路，会变成求和的形式，避免梯度消失。后面的层可以看见输入，不至于因为信息损失而失去学习能力。</strong></p>
<p><strong>如果连乘的方式会造成梯度消失的话，那么连加。传统的网络每次学习会学习x-&gt;f(x)的完整映射，那么ResNet只学习残差的映射，</strong></p>
<p>随着网络的加深，出现了训练集准确率下降的现象，但是我们又可以确定这不是由过拟合造成的，因为过拟合的情况下，训练集应该准确率很高，所以作者针对这个问题提出了一种全新的网络，称为深度残差网络，它能够允许网络尽可能的假设，其中引入了一种全新的结构：</p>
<p><img src="https://img-blog.csdn.net/20180422190543725?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>残差指什么：</strong></p>
<p>ResNet提出了两种mapping：</p>
<ul>
<li>
<p>其一：identity mapping，指的“本身的线”，也就是公式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span>，就是图中弯曲的曲线；</p>
</li>
<li>
<p>其二：residual mapping，指的是“差”，也就是<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>−</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y-x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.77777em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span>，所以残差指的是<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi mathvariant="normal">部</mi><mi mathvariant="normal">分</mi></mrow><annotation encoding="application/x-tex">F(x)部分</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord cjk_fallback">部</span><span class="mord cjk_fallback">分</span></span></span></span></span>，也就是除过identity mapping之外的其余的线；</p>
</li>
</ul>
<p>所以最后的输出为，<span class="katex--inline"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>F</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y=F(x)+x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathit" style="margin-right: 0.13889em;">F</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit">x</span></span></span></span></span></p>
<p><strong>为什么ResNet可以解决随着网络加深，准确率不下降的问题？</strong></p>
<p>理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。</p>
<h4><a id="133_ResNet_1554"></a>13.3 ResNet结构</h4>
<p>传统的神经网络都是以层叠卷积层的方式提高网络深度，从而提高识别精度，但层叠过多的卷积层会出现一个问题，就是梯度消失，使得反向传播的过程中无法有效的将梯度更新到前面的网络层，导致前面的层的参数无法更新。</p>
<p>而BN和ResNet的skip connection就是为了解决这个问题，BN通过规范化输入数据，改变数据的分布，在前向传播的过程中，防止梯度消失的问题，而skip connection则能在后传过程中更好地把梯度传到更浅的层次中。</p>
<p><strong>问题：为什么加了一个捷径就可以把梯度传到浅层网络？</strong></p>
<p>这和神经网络参数更新的过程密切相关，cs231n 2016视频有很好的讲解。</p>
<p>前向传播：</p>
<p>首先x与w1相乘，得到1；1与w2相乘，得到0.1，以此类推，如下面的gif图绿色数字表示<br>
<img src="https://img-blog.csdn.net/20180422194144561?" alt="这里写图片描述"></p>
<p>反向传播：</p>
<p>假设从下一层网络传回来的梯度为1（最右边的数字），后向传播的梯度数值如下面gif图红色数字表示：<br>
<img src="https://img-blog.csdn.net/20180422194156879?" alt="这里写图片描述"></p>
<p>那么这里可以看到，本来从上一层传过来的梯度为1，经过这个block之后，得到的梯度已经变成了0.0001和0.01，也就是说，梯度流过一个blcok之后，就已经下降了几个量级，传到前一层的梯度将会变得很小！</p>
<p>当权重很小的时候，前向传播之后到输出层的参数值会非常小，反向传播时依然要和小的权重值相乘，参数值只会越来越小，数量级下降的非常快。</p>
<p>这就是梯度弥散。假如模型的层数越深，这种梯度弥散的情况就更加严重，导致浅层部分的网络权重参数得不到很好的训练，这就是为什么在Resnet出现之前，CNN网络都不超过二十几层的原因。</p>
<p><strong>防止梯度消失的方法：</strong></p>
<p><img src="https://img-blog.csdn.net/2018042219435360?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>假如，我们在这个block的旁边加了一条“捷径”（如图5橙色箭头），也就是常说的“skip connection”。假设左边的上一层输入为x，虚线框的输出为f(x)，上下两条路线输出的激活值相加为h(x)，即h(x) = F(x) + x，得出的h(x)再输入到下一层。</p>
<p>当进行后向传播时，右边来自深层网络传回来的梯度为1，经过一个加法门，橙色方向的梯度为dh(x)/dF(x)=1，蓝色方向的梯度也为1。这样，经过梯度传播后，现在传到前一层的梯度就变成了[1, 0.0001, 0.01]，多了一个“1”！正是由于多了这条捷径，来自深层的梯度能直接畅通无阻地通过，去到上一层，使得浅层的网络层参数等到有效的训练！</p>
<p>这个想法是何等的简约而伟大，不得不佩服作者的强大的思维能力！</p>
<p><strong>ResNet网络：</strong></p>
<p><img src="https://img-blog.csdn.net/20180718190519650?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>
直观理解：</p>
<p><img src="https://img-blog.csdn.net/20180422194525302?" alt="这里写图片描述"></p>
<p>如图，左边来了一辆装满了“梯度”商品的货车，来领商品的客人一般都要排队一个个拿才可以，如果排队的人太多，后面的人就没有了。于是这时候派了一个人走了“快捷通道”，到货车上领了一部分“梯度”，直接送给后面的人，这样后面排队的客人就能拿到更多的“梯度”。</p>
<p>它使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思，看下图我们就能大致理解：</p>
<p><img src="https://img-blog.csdn.net/20180422191135582?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>图1 Shortcut Connection</p>
<p>这是文章里面的图，我们可以看到一个“弯弯的弧线“这个就是所谓的”shortcut connection“，也是文中提到identity mapping，这张图也诠释了ResNet的真谛，当然真正在使用的ResNet模块并不是这么单一，文章中就提出了两种方式：</p>
<p><img src="https://img-blog.csdn.net/20180422191222156?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>图2 两种ResNet设计</p>
<p>这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个“building block”，其中右图为“bottleneck design”，目的就是为了降低参数数目，第一个1x1的卷积把256维的channel降到64维，然后在最后通过1x1卷积恢复，整体上用到参数数目+ 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。</p>
<p>对于常规ResNet，可以用于34层或者更少的网络中，对于Bottleneck Design的ResNet通常用于更深的如101这样的网络中，目的是减少计算和参数量（实用目的）。</p>
<p><strong>问题：图1中的F（x）和x的channel个数不同怎么办，因为F（x）和x是按照channel维度相加的，channel不同怎么相加呢？</strong></p>
<p><strong>解释：</strong></p>
<p>针对channel个数是否相同，要分成两种情况考虑，如下图：</p>
<p><img src="https://img-blog.csdn.net/20180422191819634?" alt="这里写图片描述"></p>
<p>图3 两种Shortcut Connection方式</p>
<p>如图3所示，我们可以清楚的”实线“和”虚线“两种连接方式：</p>
<ul>
<li>
<p>实线的的Connection部分(”第一个粉色矩形和第三个粉色矩形“)都是3x3x64的特征图，他们的channel个数一致，所以采用计算方式： $y=F(x)+x $</p>
</li>
<li>
<p>虚线的Connection部分(”第一个绿色矩形和第三个绿色矩形“)分别是3x3x64和3x3x128的特征图，他们的channel个数不同(64和128)，所以采用计算方式： $y=F(x)+Wx $</p>
</li>
</ul>
<p>其中W是卷积操作，用来调整x的channel维度的；</p>
<p><strong>两个实例：</strong></p>
<p><img src="https://img-blog.csdn.net/20180422191948368?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>图4 两种Shortcut Connection方式实例（左图channel一致，右图channel不一样）</p>
<h4><a id="134_ResNet50ResNet101_1641"></a>13.4 ResNet50和ResNet101</h4>
<p>这里把ResNet50和ResNet101特别提出，主要因为它们的出镜率很高，所以需要做特别的说明。给出了它们具体的结构：</p>
<p><img src="https://img-blog.csdn.net/2018042219204969?" alt="这里写图片描述"></p>
<p>表2，Resnet不同的结构</p>
<p>首先我们看一下表2，上面一共提出了5中深度的ResNet，分别是18，34，50，101和152，首先看表2最左侧，我们发现所有的网络都分成5部分，分别是：conv1，conv2_x，conv3_x，conv4_x，conv5_x，之后的其他论文也会专门用这个称呼指代ResNet50或者101的每部分。<br>
拿101-layer那列，我们先看看101-layer是不是真的是101层网络，首先有个输入7x7x64的卷积，然后经过3 + 4 + 23 + 3 = 33个building block，每个block为3层，所以有33 x 3 = 99层，最后有个fc层(用于分类)，所以1 + 99 + 1 = 101层，确实有101层网络；<br>
注：101层网络仅仅指卷积或者全连接层，而激活层或者Pooling层并没有计算在内；<br>
这里我们关注50-layer和101-layer这两列，可以发现，它们唯一的不同在于conv4_x，ResNet50有6个block，而ResNet101有23个block，查了17个block，也就是17 x 3 = 51层。</p>
<h4><a id="135_ResNet101Faster_RCNN_1655"></a>13.5 基于ResNet101的Faster RCNN</h4>
<p>文章中把ResNet101应用在Faster RCNN上取得了更好的结果，结果如下：</p>
<p><img src="https://img-blog.csdn.net/20180422192144655?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><img src="https://img-blog.csdn.net/20180422192150264?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>表3，Resnet101 Faster RCNN在Pascal VOC07/12 以及COCO上的结果</p>
<p>**问题：Faster RCNN中RPN和Fast RCNN的共享特征图用的是conv5_x的输出么？ **</p>
<p>针对这个问题我们看看实际的基于ResNet101的Faster RCNN的结构图：</p>
<p><img src="https://img-blog.csdn.net/2018042219222888?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>图5 基于ResNet101的Faster RCNN</p>
<p>图5展示了整个Faster RCNN的架构，其中蓝色的部分为ResNet101，可以发现conv4_x的最后的输出为RPN和RoI Pooling共享的部分，而conv5_x(共9层网络)都作用于RoI Pooling之后的一堆特征图(14 x 14 x 1024)，特征图的大小维度也刚好符合原本的ResNet101中conv5_x的输入；</p>
<p>最后一定要记得最后要接一个average pooling，得到2048维特征，分别用于分类和框回归。</p>
<h3><a id="14_CNNRCNN2013Fast_RCNN2015Faster_RCNN2015_1678"></a>14、区域 CNN：R-CNN(2013年)、Fast R-CNN(2015年)、Faster R-CNN(2015年)</h3>
<p>一些人可能会认为，R-CNN的出现比此前任何关于新的网络架构的论文都有影响力。第一篇关于R-CNN的论文被引用了超过1600次。Ross Girshick 和他在UC Berkeley 的团队在机器视觉上取得了最有影响力的进步。正如他们的文章所写， Fast R-CNN 和 Faster R-CNN能够让模型变得更快，更好地适应现代的物体识别任务。</p>
<p>R-CNN的目标是解决物体识别的难题。在获得特定的一张图像后， 我们希望能够绘制图像中所有物体的边缘。这一过程可以分为两个组成部分，一个是区域建议，另一个是分类。</p>
<p>论文的作者强调，任何分类不可知区域的建议方法都应该适用。Selective Search专用于RCNN。Selective Search 的作用是聚合2000个不同的区域，这些区域有最高的可能性会包含一个物体。在我们设计出一系列的区域建议之后，这些建议被汇合到一个图像大小的区域，能被填入到经过训练的CNN(论文中的例子是AlexNet)，能为每一个区域提取出一个对应的特征。这个向量随后被用于作为一个线性SVM的输入，SVM经过了每一种类型和输出分类训练。向量还可以被填入到一个有边界的回归区域，获得最精准的一致性。非极值压抑后被用于压制边界区域，这些区域相互之间有很大的重复。<br>
<img src="https://img-blog.csdn.net/20180427110256476?" alt="这里写图片描述"></p>
<p><strong>Fast R-CNN：</strong><br>
原始模型得到了改进，主要有三个原因：训练需要多个步骤，这在计算上成本过高，而且速度很慢。Fast R-CNN通过从根本上在不同的建议中分析卷积层的计算，同时打乱生成区域建议的顺利以及运行CNN，能够快速地解决问题。<br>
<img src="https://img-blog.csdn.net/20180427110403861?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>Faster R-CNN</strong></p>
<p>Faster R-CNN的工作是克服R-CNN和 Fast R-CNN所展示出来的，在训练管道上的复杂性。作者 在最后一个卷积层上引入了一个区域建议网络(RPN)。这一网络能够只看最后一层的特征就产出区域建议。从这一层面上来说，相同的R-CNN管道可用。<br>
<img src="https://img-blog.csdn.net/20180427110434843?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>重要性：</p>
<p>能够识别出一张图像中的某一个物体是一方面，但是，能够识别物体的精确位置对于计算机知识来说是一个巨大的飞跃。更快的R-CNN已经成为今天标准的物体识别程序。</p>
<h3><a id="15_1700"></a>15、生成式对抗网络</h3>
<p>按照Yann LeCun的说法，生成对抗网络可能就是深度学习下一个大突破。假设有两个模型，一个生成模型，一个判别模型。判别模型的任务是决定某幅图像是真实的(来自数据库)，还是机器生成的，而生成模型的任务则是生成能够骗过判别模型的图像。这两个模型彼此就形成了“对抗”，发展下去最终会达到一个平衡，生成器生成的图像与真实的图像没有区别，判别器无法区分两者。</p>
<p><img src="https://img-blog.csdn.net/20180427110743742?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>左边一栏是数据库里的图像，也即真实的图像，右边一栏是机器生成的图像，虽然肉眼看上去基本一样，但在CNN看起来却十分不同。</p>
<p><strong>为什么重要?</strong></p>
<p>听上去很简单，然而这是只有在理解了“数据内在表征”之后才能建立的模型，你能够训练网络理解真实图像和机器生成的图像之间的区别。因此，这个模型也可以被用于CNN中做特征提取。此外，你还能用生成对抗模型制作以假乱真的图片。</p>
<h3><a id="16_1717"></a>16、深度学习在计算机视觉上的应用</h3>
<p>计算机视觉中比较成功的深度学习的应用，包括人脸识别，图像问答，物体检测，物体跟踪。</p>
<p><strong>人脸识别：</strong></p>
<p>这里说人脸识别中的人脸比对，即得到一张人脸，与数据库里的人脸进行比对；或同时给两张人脸，判断是不是同一个人。</p>
<p>这方面比较超前的是汤晓鸥教授，他们提出的DeepID算法在LWF上做得比较好。他们也是用卷积神经网络，但在做比对时，两张人脸分别提取了不同位置特征，然后再进行互相比对，得到最后的比对结果。最新的DeepID-3算法，在LWF达到了99.53%准确度，与肉眼识别结果相差无几。</p>
<p><strong>图片问答问题：</strong></p>
<p>这是2014年左右兴起的课题，即给张图片同时问个问题，然后让计算机回答。比如有一个办公室靠海的图片，然后问“桌子后面有什么”，神经网络输出应该是“椅子和窗户”。<br>
<img src="https://img-blog.csdn.net/20180423193447416?" alt="这里写图片描述"></p>
<p>这一应用引入了LSTM网络，这是一个专门设计出来具有一定记忆能力的神经单元。特点是，会把某一个时刻的输出当作下一个时刻的输入。可以认为它比较适合语言等，有时间序列关系的场景。因为我们在读一篇文章和句子的时候，对句子后面的理解是基于前面对词语的记忆。</p>
<p>图像问答问题是基于卷积神经网络和LSTM单元的结合，来实现图像问答。LSTM输出就应该是想要的答案，而输入的就是上一个时刻的输入，以及图像的特征，及问句的每个词语。</p>
<p><strong>物体检测问题：</strong></p>
<p><strong>① Region CNN</strong></p>
<p>深度学习在物体检测方面也取得了非常好的成果。2014年的Region CNN算法，基本思想是首先用一个非深度的方法，在图像中提取可能是物体的图形块，然后深度学习算法根据这些图像块，判断属性和一个具体物体的位置。</p>
<p><img src="https://img-blog.csdn.net/20180423193531778?" alt="这里写图片描述"></p>
<p>为什么要用非深度的方法先提取可能的图像块？因为在做物体检测的时候，如果你用扫描窗的方法进行物体监测，要考虑到扫描窗大小的不一样，长宽比和位置不一样，如果每一个图像块都要过一遍深度网络的话，这种时间是你无法接受的。</p>
<p>所以用了一个折中的方法，叫Selective Search。先把完全不可能是物体的图像块去除，只剩2000左右的图像块放到深度网络里面判断。那么取得的成绩是AP是58.5，比以往几乎翻了一倍。有一点不尽如人意的是，region CNN的速度非常慢，需要10到45秒处理一张图片。</p>
<p><strong>② Faster R-CNN方法</strong></p>
<p>而且我在去年NIPS上，我们看到的有Faster R-CNN方法，一个超级加速版R-CNN方法。它的速度达到了每秒七帧，即一秒钟可以处理七张图片。技巧在于，不是用图像块来判断是物体还是背景，而把整张图像一起扔进深度网络里，让深度网络自行判断哪里有物体，物体的方块在哪里，种类是什么？</p>
<p>经过深度网络运算的次数从原来的2000次降到一次，速度大大提高了。</p>
<p>Faster R-CNN提出了让深度学习自己生成可能的物体块，再用同样深度网络来判断物体块是否是背景？同时进行分类，还要把边界和给估计出来。</p>
<p>Faster R-CNN可以做到又快又好，在VOC2007上检测AP达到73.2，速度也提高了两三百倍。</p>
<p><strong>③ YOLO</strong></p>
<p>去年FACEBOOK提出来的YOLO网络，也是进行物体检测，最快达到每秒钟155帧，达到了完全实时。它让一整张图像进入到神经网络，让神经网络自己判断这物体可能在哪里，可能是什么。但它缩减了可能图像块的个数，从原来Faster R-CNN的2000多个缩减缩减到了98个。</p>
<p><img src="https://img-blog.csdn.net/20180423193618291?" alt="这里写图片描述"></p>
<p>同时取消了Faster R-CNN里面的RPN结构，代替Selective Search结构。YOLO里面没有RPN这一步，而是直接预测物体的种类和位置。</p>
<p>YOLO的代价就是精度下降，在155帧的速度下精度只有52.7，45帧每秒时的精度是63.4。</p>
<p><strong>④ SSD</strong></p>
<p>在arXiv上出现的最新算法叫Single Shot MultiBox Detector，即SSD。<br>
<img src="https://img-blog.csdn.net/20180423193646402?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>它是YOLO的超级改进版，吸取了YOLO的精度下降的教训，同时保留速度快的特点。它能达到58帧每秒，精度有72.1。速度超过Faster R-CNN 有8倍，但达到类似的精度。</p>
<p><strong>物体跟踪</strong></p>
<p>所谓跟踪，就是在视频里面第一帧时锁定感兴趣的物体，让计算机跟着走，不管怎么旋转晃动，甚至躲在树丛后面也要跟踪。</p>
<p><img src="https://img-blog.csdn.net/20180423193703514?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>深度学习对跟踪问题有很显著的效果。是第一在线用深度学习进行跟踪的文章，当时超过了其它所有的浅层算法。</p>
<p>今年有越来越多深度学习跟踪算法提出。去年十二月ICCV 2015上面，马超提出的Hierarchical Convolutional Feature算法，在数据上达到最新的记录。它不是在线更新一个深度学习网络，而是用一个大网络进行预训练，然后让大网络知道什么是物体什么不是物体。</p>
<p>将大网络放在跟踪视频上面，然后再分析网络在视频上产生的不同特征，用比较成熟的浅层跟踪算法来进行跟踪，这样利用了深度学习特征学习比较好的好处，同时又利用了浅层方法速度较快的优点。效果是每秒钟10帧，同时精度破了记录。</p>
<p>最新的跟踪成果是基于Hierarchical Convolutional Feature，由一个韩国的科研组提出的MDnet。它集合了前面两种深度算法的集大成，首先离线的时候有学习，学习的不是一般的物体检测，也不是ImageNet，学习的是跟踪视频，然后在学习视频结束后，在真正在使用网络的时候更新网络的一部分。这样既在离线的时候得到了大量的训练，在线的时候又能够很灵活改变自己的网络。</p>
<p><strong>基于嵌入式系统的深度学习</strong></p>
<p>回到ADAS问题（慧眼科技的主业），它完全可以用深度学习算法，但对硬件平台有比较高的要求。在汽车上不太可能把一台电脑放上去，因为功率是个问题，很难被市场所接受。</p>
<p>现在的深度学习计算主要是在云端进行，前端拍摄照片，传给后端的云平台处理。但对于ADAS而言，无法接受长时间的数据传输的，或许发生事故后，云端的数据还没传回来。</p>
<p>那是否可以考虑NVIDIA推出的嵌入式平台？NVIDIA推出的嵌入式平台，其运算能力远远强过了所有主流的嵌入式平台，运算能力接近主流的顶级CPU，如台式机的i7。那么慧眼科技在做工作就是要使得深度学习算法，在嵌入式平台有限的资源情况下能够达到实时效果，而且精度几乎没有减少。</p>
<p>具体做法是，首先对网络进行缩减，可能是对网络的结构缩减，由于识别场景不同，也要进行相应的功能性缩减；另外要用最快的深度检测算法，结合最快的深度跟踪算法，同时自己研发出一些场景分析算法。三者结合在一起，目的是减少运算量，减少检测空间的大小。在这种情况下，在有限资源上实现了使用深度学习算法，但精度减少的非常少。</p>
<h3><a id="17_1801"></a>17、深度有监督学习在计算机视觉领域的进展</h3>
<h4><a id="171__1803"></a>17.1 图像分类</h4>
<p>自从Alex和他的导师Hinton（深度学习鼻祖）在2012年的ImageNet大规模图像识别竞赛（ILSVRC2012）中以超过第二名10个百分点的成绩(83.6%的Top5精度)碾压第二名（74.2%，使用传统的计算机视觉方法）后，深度学习真正开始火热，卷积神经网络（CNN）开始成为家喻户晓的名字，从12年的AlexNet（83.6%），到2013年ImageNet 大规模图像识别竞赛冠军的88.8%，再到2014年VGG的92.7%和同年的GoogLeNet的93.3%，终于，到了2015年，在1000类的图像识别中，微软提出的残差网（ResNet）以96.43%的Top5正确率，达到了超过人类的水平（人类的正确率也只有94.9%）.<br>
Top5精度是指在给出一张图片，模型给出5个最有可能的标签，只要在预测的5个结果中包含正确标签，即为正确<br>
<img src="https://img-blog.csdn.net/20180427113154161?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4><a id="172_Image_Dection_1808"></a>17.2 图像检测（Image Dection）</h4>
<p>伴随着图像分类任务，还有另外一个更加有挑战的任务–图像检测，图像检测是指在分类图像的同时把物体用矩形框给圈起来。从14年到16年，先后涌现出R-CNN,Fast R-CNN, Faster R-CNN, YOLO, SSD等知名框架，其检测平均精度（mAP），在计算机视觉一个知名数据集上PASCAL VOC上的检测平均精度（mAP），也从R-CNN的53.3%，到Fast RCNN的68.4%，再到Faster R-CNN的75.9%，最新实验显示，Faster RCNN结合残差网（Resnet-101），其检测精度可以达到83.8%。深度学习检测速度也越来越快，从最初的RCNN模型，处理一张图片要用2秒多，到Faster RCNN的198毫秒/张，再到YOLO的155帧/秒（其缺陷是精度较低，只有52.7%），最后出来了精度和速度都较高的SSD，精度75.1%，速度23帧/秒。<br>
<img src="https://img-blog.csdn.net/20180427113237991?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>####17.3 图像分割（Semantic Segmentation）</p>
<p>图像分割也是一项有意思的研究领域，它的目的是把图像中各种不同物体给用不同颜色分割出来，如下图所示，其平均精度（mIoU，即预测区域和实际区域交集除以预测区域和实际区域的并集），也从最开始的FCN模型（图像语义分割全连接网络，该论文获得计算机视觉顶会CVPR2015的最佳论文的）的62.2%，到DeepLab框架的72.7%，再到牛津大学的CRF as RNN的74.7%。该领域是一个仍在进展的领域，仍旧有很大的进步空间。</p>
<p><img src="https://img-blog.csdn.net/20180427113251631?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h4><a id="174_Image_Captioning_1820"></a>17.4 图像标注–看图说话（Image Captioning）</h4>
<p>图像标注是一项引人注目的研究领域，它的研究目的是给出一张图片，你给我用一段文字描述它，如图中所示，图片中第一个图，程序自动给出的描述是“一个人在尘土飞扬的土路上骑摩托车”，第二个图片是“两只狗在草地上玩耍”。由于该研究巨大的商业价值（例如图片搜索），近几年，工业界的百度，谷歌和微软 以及学术界的加大伯克利，深度学习研究重地多伦多大学都在做相应的研究。</p>
<p><img src="https://img-blog.csdn.net/20180427113321862?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>####17.5 图像生成–文字转图像（Image Generator）</p>
<p>图片标注任务本来是一个半圆，既然我们可以从图片产生描述文字，那么我们也能从文字来生成图片。如图6所示，第一列“一架大客机在蓝天飞翔”，模型自动根据文字生成了16张图片，第三列比较有意思，“一群大象在干燥草地行走”（这个有点违背常识，因为大象一般在雨林，不会在干燥草地上行走），模型也相应的生成了对应图片，虽然生成的质量还不算太好，但也已经中规中矩。</p>
<p><img src="https://img-blog.csdn.net/20180427113340361?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<h3><a id="18Reinforcement_Learning_1833"></a>18、强化学习（Reinforcement Learning）</h3>
<p>在监督学习任务中，我们都是给定样本一个固定标签，然后去训练模型，可是，在真实环境中，我们很难给出所有样本的标签，这时候，强化学习就派上了用场。简单来说，我们给定一些奖励或惩罚，强化学习就是让模型自己去试错，模型自己去优化怎么才能得到更多的分数。2016年大火的AlphaGo就是利用了强化学习去训练，它在不断的自我试错和博弈中掌握了最优的策略。利用强化学习去玩flyppy bird，已经能够玩到几万分了。</p>
<p><img src="https://img-blog.csdn.net/20180427113416762?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW95YW5nd20=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>强化学习在机器人领域和自动驾驶领域有极大的应用价值，当前arxiv上基本上每隔几天就会有相应的论文出现。机器人去学习试错来学习最优的表现，这或许是人工智能进化的最优途径，估计也是通向强人工智能的必经之路。</p>

</pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                                        <a href="javascript:;" class="readmore_btn"
                                           data-linkUrl="https://blog.csdn.net/jiaoyangwm/article/details/80011656"
                                           data-so-type="blog">
                                            <span>展开全文</span>
                                            <img src='https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png' alt="">
                                        </a>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/chaipp0607/article/details/72847422"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/chaipp0607/article/details/72847422?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-5-72847422.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-5-72847422.pc_agg_rank_aggregation\&quot;}","index": "4","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/chaipp0607/article/details/72847422?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-5-72847422.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-5-72847422.pc_agg_rank_aggregation\&quot;}","index": "4","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-5-72847422.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2>[Intensive Reading]从AlexNet理解<em>卷积神经网络</em>的一般结构</h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2017-06-04 17:58:37</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    2012年AlexNet在ImageNet大赛上一举夺魁，开启了深度学习的时代，虽然后来大量比AlexNet更快速更准确的<em>卷积神经网络</em>结构相继出现，但是AlexNet作为开创者依旧有着很多值得学习参考的地方，它为后续的CNN甚至是R-CNN...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"></pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/happyorg/article/details/78274066"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/happyorg/article/details/78274066?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-6-78274066.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-6-78274066.pc_agg_rank_aggregation\&quot;}","index": "5","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/happyorg/article/details/78274066?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-6-78274066.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-6-78274066.pc_agg_rank_aggregation\&quot;}","index": "5","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-6-78274066.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2>深度学习 CNN<em>卷积神经网络</em> LeNet-5详解</h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2017-10-18 16:04:35</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    <em>卷积神经网络</em>（ Convolutional Neural Network, CNN）：   是一种常见的深度学习架构，受生物自然视觉认知机制(动物视觉皮层细胞负责检测光学信号)启发而来，是一种特殊的多层前馈神经网络。它的人工神经元可以响应...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"></pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                            </div>
                                <div class="tags-view">
                                    <img src='https://csdnimg.cn/release/aggregate/img/tags.png' alt="">
                                        <a href="../tags/MtTaggxsNzMxNC1ibG9n.html">深度学习   </a>
                                        <a href="../tags/MtjaMg3sNzkyMy1ibG9n.html">图像处理   </a>
                                        <a href="../tags/MtzaIgwsNDIwOTMtYmxvZwO0O0OO0O0O.html">网络   </a>
                                </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/weixin_42451919/article/details/81381294"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/weixin_42451919/article/details/81381294?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-7-81381294.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-7-81381294.pc_agg_rank_aggregation\&quot;}","index": "6","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/weixin_42451919/article/details/81381294?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-7-81381294.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-7-81381294.pc_agg_rank_aggregation\&quot;}","index": "6","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-7-81381294.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2>一文让你彻底了解<em>卷积神经网络</em></h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2018-08-03 09:27:11</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    <em>卷积神经网络</em>（Convolutional Neural Network，CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。 它包括卷积层(convolutional layer)和池化层(pooling ...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"></pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                            </div>

                        </li>
                        <li data-page="1">
                            <div class="blog-title">
                                <a href="https://blog.csdn.net/hoho1151191150/article/details/79714691"
                                   data-report-click='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/hoho1151191150/article/details/79714691?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-8-79714691.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-8-79714691.pc_agg_rank_aggregation\&quot;}","index": "7","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-view='{"mod": "popu_876","spm": "3001.4430","dest": "https://blog.csdn.net/hoho1151191150/article/details/79714691?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D&amp;request_id=162408338516780271588145&amp;utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-8-79714691.pc_agg_rank_aggregation","extra": "{\&quot;utm_medium\&quot;:\&quot;distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-8-79714691.pc_agg_rank_aggregation\&quot;}","index": "7","strategy": "2~aggregatepage~first_rank_v2~rank_aggregation","biz_id": "0","ops_request_misc": "%257B%2522request%255Fid%2522%253A%2522162408338516780271588145%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Faggregatepage.%2522%257D","request_id": "162408338516780271588145"}'
                                   data-report-query="utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_v2~rank_aggregation-8-79714691.pc_agg_rank_aggregation&utm_term=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=3001.4430"
                                   target="_blank" class="blog-title">
                                    <h2>神经网络学习（十三）<em>卷积神经网络</em>的MATLAB实现</h2>
                                </a>
                                    <span class="tags">
                                            <i class="c1">万次阅读</i>
                                            <i class="c2">多人点赞</i>
                                    </span>
                                <span class="update-time">2018-03-27 16:08:28</span>
                            </div>
                            <div class="description">
                                <div class="desc-detail digest">
                                    上一节，我们简单探讨了<em>卷积神经网络</em>的反向传播算法，本节我们着手实现了一个简单的卷积神经网，在此之前先以最基本的批量随机梯度下降法+L2正则化对对<em>卷积神经网络</em>的反向传播算法做一个很简单回顾。    需要确定...
                                </div>
                                <div class="desc-detail content">
                                    <div class='article_content'>
                                        <pre class="preclass"></pre>
                                    </div>
                                    <div class="hide-content"><a href="javascript:;">收起 <img
                                                    src="https://csdnimg.cn/release/aggregate/img/arrow-down@2x.png" alt=""></a></div>
                                </div>
                            </div>

                        </li>
                </ul>
            </div>

            <div class="tab-bd wrap-download">
                <div class="qa-list"></div>
                <div class="qa-no-data hide">
                    <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/monkeyWhite.png" alt="">
                    <p style="color: #6b6b6b;font-size: 16px;font-weight: bold;letter-spacing: 2px;">空空如也</p>
                </div>
            </div>

            <div class="tab-bd wrap-qa">
                <div class="qa-list"></div>
                <div class="qa-no-data hide">
                    <img src="https://csdnimg.cn/release/blogv2/dist/pc/img/monkeyWhite.png" alt="">
                    <p style="color: #6b6b6b;font-size: 16px;font-weight: bold;letter-spacing: 2px;">空空如也</p>
                </div>
            </div>

            <div class="wrap-pagination">
                <div class="ftl_page">

 <script>  
    console.log("pageNum", `1`)
    console.log("pages", `20`)
    console.log("toalNum", `400`)
</script>

 <div class="search-result-pagination"> 
         <div class="csdn-pagination hide-set">
            <span class="page-nav">
                    <a class="disable iconfont icon-fanhui btn btn-xs btn-default btn-prev checklogin" href="javascript:;"></a> 


                    
                        <a href="?platform=pc&page=1&pageSize=20"  page_num="1" class="btn btn-xs btn-default active checklogin">1</a> 
                    
                    
                        <a href="?platform=pc&page=2&pageSize=20"  page_num="2" class="btn btn-xs btn-default checklogin">2</a> 
                    
                    
                        <a href="?platform=pc&page=3&pageSize=20"  page_num="3" class="btn btn-xs btn-default checklogin">3</a> 
                    
                    
                        <a href="?platform=pc&page=4&pageSize=20"  page_num="4" class="btn btn-xs btn-default checklogin">4</a> 
                    
                    
                        <a href="?platform=pc&page=5&pageSize=20"  page_num="5" class="btn btn-xs btn-default checklogin">5</a> 
                    
                        <span class="ellipsis">...</span> 
                    
                        <a href="?platform=pc&page=20&pageSize=20"  page_num="20" class="btn btn-xs btn-default checklogin">20</a> 
                    

                    <a href="?platform=pc&page=2&pageSize=20" page_num="2" class="iconfont icon-fanhui btn btn-xs btn-default btn-next checklogin"></a>
            </span>
        </div>
   
</div>                </div>
                <div class="ant_page hide" style="padding-top: 10px;background-color: #fff;"></div>
            </div>

        </div>
    </div>
    <div class="con-r-tag">
        <div class="collect-and-boutique">
            <div>
                <span>收藏数</span>
                    <span>47,038</span>
            </div>
            <div>
                <span>精华内容</span>
                    <span>18,815</span>
            </div>
        </div>
        <div class="hot-tags">
            <div>热门标签</div>
            <ul class="clearfloat">
                    <li>
                        <a href=""
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":""}'
                           data-report-query="spm=3001.4431">
                            cnn
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/MtTaEg0sMTI3NDctYmxvZwO0O0OO0O0O.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/MtTaEg0sMTI3NDctYmxvZwO0O0OO0O0O.html"}'
                           data-report-query="spm=3001.4431">
                            神经网络
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaIgzsNjcxNy1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaIgzsNjcxNy1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            cnn卷积神经网络
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/MtTaEg0sMzY2MDMtYmxvZwO0O0OO0O0O.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/MtTaEg0sMzY2MDMtYmxvZwO0O0OO0O0O.html"}'
                           data-report-query="spm=3001.4431">
                            卷积
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaIg1sNTE3My1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaIg1sNTE3My1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            卷积神经网络原理
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaIg2sNjcxMS1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaIg2sNjcxMS1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            卷积神经网络图像识别
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/MtTaggxsNzMxNC1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/MtTaggxsNzMxNC1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            深度学习
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaMgxsNTAyNi1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaMgxsNTAyNi1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            一维卷积神经网络
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaMgxsMzUwMC1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaMgxsMzUwMC1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            图卷积神经网络
                        </a>
                    </li>
                    <li>
                        <a href="https://www.csdn.net/tags/NtTaIg2sMTIxNS1ibG9n.html"
                           target="_blank"
                           data-report-click='{"spm":"3001.4431","dest":"https://www.csdn.net/tags/NtTaIg2sMTIxNS1ibG9n.html"}'
                           data-report-query="spm=3001.4431">
                            卷积神经网络结构
                        </a>
                    </li>
            </ul>
        </div>

        <div class="hot-brand hide" style="margin-top: 8px;background-color: #fff;"></div>

        <div class="hot-tags keywords">
            关键字：<h1>卷积神经网络 </h1>
        </div>



        <div class="bottom-part">
            <div class="persion_article" style="width:300px;margin-top: 8px;"></div>
        </div>
        <script src="https://g.csdnimg.cn/common/csdn-footer/csdn-footer.js" data-isfootertrack="false"></script>
    </div>
</div>

<script id="tpl_qa" type="text/html">
    <div class="qa-list">
        {{each items}}
        <div class="qa-item">
            <div class="qa-item-hd">
                <a href="{{$value.ext.url_location}}"
                   data-report-click='{"mod":"{{@$value.report_data.data.mod}}","spm":"{{@$value.spm}}","dest":"{{$value.ext.url_location}}","index":"{{@$value.report_data.data.index}}","strategy":"{{@$value.report_data.data.strategy}}","biz_id":"{{@$value.report_data.data.biz_id}}","ops_request_misc":"{{@$value.report_data.data.ops_request_misc}}","request_id":"{{@$value.report_data.data.request_id}}"}'
                   data-report-view='{"mod":"{{@$value.report_data.data.mod}}","spm":"{{@$value.spm}}","dest":"{{$value.ext.url_location}}","index":"{{@$value.report_data.data.index}}","strategy":"{{@$value.report_data.data.strategy}}","biz_id":"{{@$value.report_data.data.biz_id}}","ops_request_misc":"{{@$value.report_data.data.ops_request_misc}}","request_id":"{{@$value.report_data.data.request_id}}"}'
                   class="list-title" target="_blank">{{@$value.ext.title}}</a>
            </div>
            <div class="qa-item-bd">
                <div class="detail">{{$value.ext.description}}</div>
                {{if $value.ext.answer && $value.ext.answer != ''}}
                <div class="answer">
                    <pre><code>{{$value.ext.answer}}</code></pre>
                </div>
                {{/if}}
            </div>
            <div class="qa-item-ft">
                {{if $value.ext.create_time != ''}}
                <span class="qa-time">{{$value.ext.create_time|parseTime}}</span>
                {{/if}}
                {{if $value.ext.nickname != ''}}
                <a href="https://me.csdn.net/{{$value.ext.author}}" target="_blank" class="qa-nickname">{{$value.ext.nickname}}</a>
                {{/if}}
                {{if $value.ext.view != 0}}
                <span class="qa-view"><i class="icon icon-view"></i>{{$value.ext.view|parseN}}</span>
                {{/if}}
                {{if $value.ext.comment != 0}}
                <span class="qa-comment"><i class="icon icon-comment"></i>{{$value.ext.comment|parseN}}</span>
                {{/if}}
                {{if $value.ext.digg != 0}}
                <span class="qa-dig"><i class="icon icon-dig"></i>{{$value.ext.digg|parseN}}</span>
                {{/if}}

                {{each $value.ext.tags tag}}
                <span class="qa-tags">
                    {{if tag != ''}}
                    <i class="qa-tag">{{tag}}</i>
                    {{/if}}
                </span>
                {{/each}}
            </div>
        </div>
        {{/each}}
    </div>
</script>

<script id="tpl_page" type="text/html">
    {{set pageNum = data.pageNum}}
    {{set pages = data.pages}}
    {{set pageSize = data.pageSize}}
    {{set toalNum = data.totalNum}}
    {{set pagesList = data.pageList}}

    <div class="search-result-pagination">
        {{if toalNum > 0}}
        <div class="csdn-pagination hide-set">
            <span class="page-nav">
                {{if pageNum == 1}}
                    <a class="disable iconfont icon-fanhui btn btn-xs btn-default btn-prev checklogin"
                       href="javascript:;"></a>
                {{/if}}
                {{if pageNum > 1}}
                    <a href="javascript:;" page_num="{{pageNum - 1}}"
                       class="iconfont icon-fanhui btn btn-xs btn-default btn-prev checklogin"></a>
                 {{/if}}


                {{each pagesList pages pages_index}}
                    {{if pages_index == 0 && pages > 1}}
                        <a href="javascript:;" page_num="1"
                           class="btn btn_normal btn-xs btn-default checklogin">1</a>
                        {{if pageNum != 4}}
                            <span class="ellipsis">...</span>
                        {{/if}}
                    {{/if}}


                    {{if pages == pageNum}}
                        <a href="javascript:;" page_num="{{pages}}"
                           class="btn btn_normal btn-xs btn-default active checklogin">{{pages}}</a>
                    {{else}}
                        <a href="javascript:;" page_num="{{pages}}"
                           class="btn btn_normal btn-xs btn-default checklogin">{{pages}}</a>
                    {{/if}}

                    {{set curPageNext=pages_index + 1}}
                    {{set pl = pagesList[curPageNext]}}
                    {{set ps = pages + 1}}
                    {{if pl && ps < pl }}
                        <span class="ellipsis">...</span>
                    {{/if}}

                {{/each}}

                 {{if pageNum < 20}}
                    <a href="javascript:;" page_num="{{pageNum + 1}}"
                       class="iconfont icon-fanhui btn btn-xs btn-default btn-next checklogin"></a>
                {{else}}
                    <a href="javascript:;"
                       class="disable iconfont icon-fanhui btn btn-xs btn-default btn-next checklogin"></a>
                {{/if}}
            </span>
        </div>
        {{/if}}

    </div>

</script>


<script src='https://csdnimg.cn/release/aggregate/js/common-773ba0471c.min.js' type="text/javascript"></script>
<script src='https://csdnimg.cn/release/aggregate/js/pc-df555339c0.min.js' type="text/javascript"></script>
<script src='https://csdnimg.cn/release/aggregate/js/libs/template-web.js' type="text/javascript"></script>

<script type="text/javascript">
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

<script>
    (function () {
            var el = document.createElement("script");
            el.src = "https://s3a.pstatp.com/toutiao/push.js?310416c564ee613927023910da70cee87ab5225d389206dcae94ee9057000d80c50ae680cb4ab776e94401444c404d285b8b644eda8af801d0911a0d00026e7f";
            el.id = "ttzz";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(el, s);
        }
    )(window)
</script>

<script type="text/javascript">
    (function ($) {

        function z(s) {
            return s < 10 ? '0' + s : s;
        }

        function dwd(data, spm) {
            if (!data || !data.items.length) return;
            for (let i = 0; i < data.items.length; i++) {
                let item = data.items[i];
                item.ext.tags = item.ext.tags.split(",");
                item.spm = spm;
            }
            return data;
        }

        template.defaults.imports.parseN = function (n) {
            return (n > 9999 ? (Math.floor(n / 10000) + '万+') : n);
        };

        template.defaults.imports.parseTime = function (n) {
            let d = new Date(parseInt(n));
            let y = d.getFullYear();
            let m = d.getMonth() + 1;
            let dd = d.getDate();
            return y + '-' + z(m) + '-' + z(dd);
        };
        var __agg_pc__ = {
            isGetDownloadList: false,
            isGetQaList: false,
            isAjax: true,
            initIframeBlinkHeight: function () {
                let winHeight = window.innerHeight;
                let isHasBaike = $(".baike-view").size();
                let other = isHasBaike ? 280 : 140;
                let res = Math.floor(winHeight - other);
                $("#blink").css("height", res + 'px');
            },
            init: function () {
                //this.initIframeBlinkHeight();
                this.events();
                //this.getQAList();
                //加载右侧广告
                this.getBrandDataById(528, function (res) {
                    $(".hot-brand").removeClass("hide").html(res.data.con);
                });
                //加载Top广告
                this.getBrandDataById(529, function (res) {
                    $(".top-banner").removeClass("hide").html(res.data.con);
                });
            },
            events: function () {
                var self = this;
                let tab_hd = $(".ag-tab-bar").find(".tab-hd-item");
                let tab_bd = $(".content-list").find(".tab-bd");
                tab_hd.on('click', function () {
                    let that = $(this);
                    let type = that.attr('type');

                    that.addClass("active").siblings().removeClass("active");

                    if (type === 'hot') {
                        $(".ag-tb-rt").show();
                        $(".bottom-part").show();
                        $(".ftl_page").show().siblings("div").hide();
                    } else if (type === 'blink') {
                        $("#blink").attr('src', 'https://blink.csdn.net/frame');
                        __agg_pc__.initIframeBlinkHeight();
                        $(".ag-tb-rt").hide();
                        $(".bottom-part").hide();
                        $(".ftl_page").hide();
                        $(".ant_page").hide();
                    } else if (type === 'download') {
                        if (!self.isGetDownloadList) {
                            self.isGetDownloadList = true;
                            $(".wrap-download").html('');
                            self.getDownloadList(1);
                        }
                        $(".ag-tb-rt").show();
                        $(".bottom-part").show();
                        $(".ant_page").show().siblings("div").hide();
                    } else if (type === 'qa') {
                        if (!self.isGetQaList) {
                            self.isGetQaList = true;
                            $(".wrap-qa").html('');
                            self.getQAList(1);
                        }
                        $(".ag-tb-rt").show();
                        $(".bottom-part").show();
                        $(".ant_page").show().siblings("div").hide();
                    }
                    tab_bd.eq(that.index()).addClass("active").siblings("div.tab-bd").removeClass("active");
                })

                let btn_link = $(".ag-tab-bar").find(".icon-link");
                btn_link.on('click', function () {
                    window.open('https://so.csdn.net/search?mode=1&q=' + mod_keywords + '&spm=1000.2123.3001.5142', '_self');
                });

                let btn_ask = $(".btn-ask");
                btn_ask.on('click', function () {
                    let UserName = self.getCookieByKey("UserName");
                    if (UserName != null) {
                        window.open('https://ask.csdn.net/?word=' + mod_keywords, '_blank');
                    } else {
                        window.csdn.loginBox.show();
                    }
                })


                $(".right-special-column .rsc-subscible").on('click', function () {
                    let userName = getCookie('UserName')
                    if (!userName) {
                        window.csdn.loginBox.show();
                        return
                    }

                    let title = $(this).prev().text();
                    // 收藏
                    var params = {
                        'url': window.location.href,         // 访问来源地址 必传
                        'title': title,      // 访问收藏标题 必传ß
                        'description': '', //访问收藏描述
                        'author': userName,
                        'source_id': 1,           // 访问来源项目标识id 必传
                        'source': 'GATHER'
                    };
                    window.csdn.collectionBox.show(params);
                })

                $(".right-special-column").on('click', 'img', function () {
                    var that = $(this);
                    var url = that.attr('data-url');
                    window.open(url, '_blank');
                });

                $(".right-special-column").on('click', 'span.rsc-name', function () {
                    var that = $(this);
                    var url = that.attr('data-url');
                    window.open(url, '_blank');
                });

            },
            getCookieByKey: function (key) {
                var cookie = '';
                var cookieStr = document.cookie || cookie;
                var cookieObj = {};
                cookieStr.split(';').forEach(function (value) {
                    var arr = value.split('=');
                    cookieObj[$.trim(arr[0])] = $.trim(arr[1]);
                })
                return cookieObj[key] || null;
            },
            getBrandDataById: function (id, cb) {
                //528右侧广告位， 529Top广告位
                $.get('/get_ad_info?ad_id=' + id, function (res) {
                    if (res.code == 200) {
                        if (res.data && res.data.con && res.data.status) {
                            cb(res);
                        }
                    } else {
                        console.log(res.msg);
                    }
                })
            },
            bindTagEvent: function () {
                $(".qa-list").on('click', 'i.qa-tag', function () {
                    var that = $(this);
                    var val = that.text();
                    if (val) {
                        window.open('https://so.csdn.net/so/search/s.do?q=' + val, '_blank');
                    }
                });

                setTimeout(function () {
                    window.csdn.report.viewCheck({'curl': location.href});
                }, 500)
            },
            getQAList: function (pageIndex) {
                let self = this;
                //http://local.java.csdn.net:9090
                $.ajax({
                    url: '/gather/search_ask?platform=pc&p=' + pageIndex + '&query=' + mod_keywords,
                    type: 'GET',
                    success: function (res) {
                        if (res.code == 200 && res.data.items.length) {
                            self.isAjax = true;
                            self.isGetQaList = false;
                            var new_data = dwd(res.data, '3001.5561');
                            var html = template("tpl_qa", new_data);
                            $(".wrap-qa").html(html);
                            self.bindTagEvent();
                            self.createPagination(res);
                        } else {
                            $(".wrap-qa .qa-no-data").removeClass("hide");
                        }
                    },
                    error: function (err) {
                        $(".wrap-qa .qa-no-data").removeClass("hide");
                    }
                });
            },
            getDownloadList: function (pageIndex) {
                let self = this;
                //http://local.java.csdn.net:9090
                $.ajax({
                    url: '/gather/search_download?platform=pc&p=' + pageIndex + '&query=' + mod_keywords,
                    type: 'GET',
                    success: function (res) {
                        if (res.code == 200 && res.data.items.length) {
                            self.isAjax = true;
                            self.isGetDownloadList = false;
                            var new_data = dwd(res.data, '3001.5560');
                            var html = template("tpl_qa", new_data);
                            $(".wrap-download").html(html);
                            self.bindTagEvent();
                            self.createPagination(res);
                        } else {
                            $(".wrap-download .qa-no-data").removeClass("hide");
                        }
                    },
                    error: function (err) {
                        $(".wrap-download .qa-no-data").removeClass("hide");
                    }
                });
            },
            dealWithPageData: function (res) {
                if (res && res.code == 200) {
                    return {
                        data: {
                            pageNum: res.pageNum,
                            pages: res.total_page,
                            pageSize: res.page_size,
                            totalNum: res.totalNum,
                            pageList: res.page_list
                        }
                    };
                }
            },
            createPagination: function (res) {
                let self = this;
                let tpl_data = this.dealWithPageData(res);
                let tpl_page = template("tpl_page", tpl_data);
                let ant_page = $(".wrap-pagination .ant_page");
                let type = $(".tab-hd-item.active").attr('type');
                ant_page.removeClass("hide").html(tpl_page);


                //页码
                $(".ant_page .btn_normal").on('click', function () {
                    let that = $(this);
                    let curr = that.attr('page_num');
                    that.addClass("active").siblings("a").removeClass("active");
                    if (curr && self.isAjax) {
                        self.isAjax = false;
                        if (type === 'download') {
                            self.getDownloadList(curr);
                        } else {
                            self.getQAList(curr);
                        }

                    }
                });

                //上一页
                $(".ant_page .btn-prev").on('click', function () {
                    let curr = $(this).attr('page_num');
                    if (curr && self.isAjax) {
                        self.isAjax = false;
                        if (type === 'download') {
                            self.getDownloadList(curr);
                        } else {
                            self.getQAList(curr);
                        }
                    }
                });

                //下一页
                $(".ant_page .btn-next").on('click', function () {
                    let curr = $(this).attr('page_num');
                    if (curr && self.isAjax) {
                        self.isAjax = false;
                        if (type === 'download') {
                            self.getDownloadList(curr);
                        } else {
                            self.getQAList(curr);
                        }
                    }
                });
            }
        }
        __agg_pc__.init();

    })($)
</script>

<script type="text/javascript">
    $(function () {
        function insertChromePluginLink() {
            let count = 0;
            let timer = null;
            let link = $('<li class="plugin-link"><a href="https://plugin.csdn.net/?from=ju" target="_blank">Chrome插件</a></li>')
            timer = setInterval(() => {
                count++;
                let menus = $("ul.toolbar-menus");
                if (menus.length || count > 10) {
                    clearInterval(timer);
                }
                if (!menus.has("li.plugin-link").length) {
                    menus.append(link)
                }
            }, 100)
        }

        insertChromePluginLink();
    })
</script>

<script type="text/javascript">
    $(function () {
        window.csdn.cse = null;
        var _types = {
            "codes_snippet": '',
            "blog": "blog",
            "discuss": "bbs",
            "doc": "download",
            "news": "geek",
            "course": "edu",
            "gitchat": "gitchat",
            "ask": "ask"
        }
        var _csdnDataObj = {};
        var _fn = {
            _CFG: {
                API_URL: '/api/v1/search',
                js_insert_first: insert_baidu_first,
                js_insert_count: insert_baidu_count,
            },
            uniqueBaidu: function (data) {
                var csdnObjKeys = [];
                var hasOwnProperty = Object.prototype.hasOwnProperty
                for (var prop in _csdnDataObj) {
                    if (hasOwnProperty.call(_csdnDataObj, prop)) csdnObjKeys.push(prop);
                }
                var csdnObjKeysLink = csdnObjKeys.map(function (v) {
                    return v.split('?')[0]
                });
                return data.filter(function (v) {
                    try {
                        return csdnObjKeysLink.indexOf(v.linkUrl.split('?')[0]) < 0;
                    } catch (e) {
                        return false
                    }
                })
            },
            distinct: function (data) {
                return data.filter(function (obj) {
                    return !_csdnDataObj[obj['linkUrl']];
                })
            },
            getIdByUrl: function (url) {
                var id = ""
                if (url == null || url == undefined || url == "") {
                    return "";
                }
                if (url.indexOf("?") > -1) {
                    url = url.substring(0, url.indexOf("?"));
                }
                if (url.endsWith("/")) {
                    url = url.substring(0, url.length - 1);
                }
                id = url.substring(url.lastIndexOf("/") + 1);
                if (id.length > 0) {
                    var regEx = "[0-9]+";
                    id.replace(regEx, function () {
                        id = arguments[0];
                    })
                }
                return id;
            },
            /**
             * 数据按 每组sublength个分组
             * @param {Array} array 源数据
             * @param {Number} subLength 每组多少个
             * @return {Boolean} isDom 如果是DOM,则每组为包裹Div
             */
            group: function (array, subLength, isDom) {
                if (Object.prototype.toString.call(array) !== "[object Array]" || typeof subLength !== 'number') {
                    return [];
                }
                var index = 0;
                var newArray = [];

                while (index < array.length) {
                    newArray.push(array.slice(index, index += subLength));
                }
                if (isDom) {
                    newArray = newArray.map(function (childrenNodes) {
                        var $frag = $('<div></div>');
                        childrenNodes.forEach(function (curNode) {
                            $frag.append(curNode)
                        })
                        return $frag
                    })
                }
                return newArray
            },
            baiduDOMHandle: function (pageNo, data, num, type, keyword) {
                var $data = _fn.group(data, num);
                var domArr = [];
                var utm_term = encodeURI(keyword);//当前搜索关键字
                var requestId = "baidu_js";
                for (var i in data) {
                    var item = data[i];
                    if (item.dispUrl) {
                        if (item.dispUrl.indexOf('gitchat') > -1) {
                            item.requestId = requestId
                            item.bizId = 101
                            item.type = "gitchat"
                        } else if (item.dispUrl.indexOf('blog') > -1) {
                            item.flagIcon = 'flag_icon1'
                            item.tip = '博客'
                            item.requestId = requestId
                            item.bizId = 102
                            item.type = "blog"
                        } else if (item.dispUrl.indexOf('download') > -1) {
                            item.flagIcon = 'flag_icon6'
                            item.tip = '下载'
                            item.requestId = requestId
                            item.bizId = 103
                            item.type = "download"
                        } else if (item.dispUrl.indexOf('bbs') > -1 || item.dispUrl.indexOf('forum') > -1) {
                            item.flagIcon = 'flag_icon2'
                            item.tip = '论坛'
                            item.requestId = requestId
                            item.bizId = 104
                            item.type = "bbs"
                        } else if (item.dispUrl.indexOf('edu') > -1) {
                            item.flagIcon = 'flag_icon3'
                            item.tip = '学院'
                            item.requestId = requestId
                            item.bizId = 105
                            item.type = "course"
                        } else if (item.dispUrl.indexOf('ask') > -1) {
                            item.flagIcon = 'flag_icon4'
                            item.tip = '问答'
                            item.requestId = requestId
                            item.bizId = 106
                            item.type = "ask"
                        } else {
                            item.flagIcon = ''
                            item.tip = ''
                            item.requestId = ''
                            item.bizId = -1
                            item.type = ''
                        }
                    }

                    var strategy = item['strategy'] ? item['strategy'].toUpperCase() : ("2~" + type + "~sobaiduweb~default")

                    var utm_medium = 'distribute.pc_aggpage_search_result.none-task-' + item.type + '-' + strategy + "-" + i + "-" + _fn.getIdByUrl(item.linkUrl);
                    var objs = {
                        mod: "popu_876",
                        dest: item.linkUrl.indexOf('?') > -1 ? item.linkUrl + '&utm_term=' + utm_term + '&utm_medium=' + utm_medium : item.linkUrl + '?utm_term=' + utm_term + '&utm_medium=' + utm_medium + '&spm=3001.4430',
                        strategy: strategy,
                        index: i + "",
                        spm: "3001.4430",
                        request_id: item['requestId'] + "",
                        biz_id: item['bizId'] + "",
                        ops_request_misc: ""
                    }
                    var jsonData = JSON.stringify(objs);
                    var data_report_query = ' data-report-query=""';
                    var data_report_click = ' data-report-click=' + jsonData;
                    var data_report_view = ' data-report-view=' + jsonData;
                    if (item.title) {
                        var htmlStr = '<li class="baidu_list">\n' +
                            '                    <div class="blog-title">\n' +
                            '                         <a href="' + objs.dest + '"  ' + data_report_query + data_report_click + data_report_view + ' target="_blank" class="blog-title">\n' +
                            '                                    <h2>' + item.title + '</h2>\n' +
                            '                                </a>\n' +
                            '                                <span class="update-time"></span>\n' +
                            '                     </div>\n' +
                            '                     <div class="description">\n' +
                            '                          <div class="desc-detail digest">' + item.abstract + '</div>\n' +
                            '                          <div class="desc-detail content">\n' +
                            '                                <div class="article_content">\n' +
                            '                                        <pre class="preclass">' + item.abstract + '</pre>\n' +
                            '                                </div>\n' +
                            '                          </div>\n' +
                            '                      </div>\n' +
                            '           </li>'
                    }
                    domArr.push($(htmlStr))
                }
                return {
                    data: $data,
                    dom: _fn.group(domArr, num, true)
                };
            },
            /**
             * 百度数据穿插
             * @param {Array} baiduData 百度的数据集合
             * @param {Number} num 每组多少个
             * @param {Boolean} flag 排序方式，是否百度数据在前
             */
            insertFunc: function (pagerNum, baiduData, num, flag, type, keyword) {
                // 处理后的百度数据
                var $baiduData = _fn.baiduDOMHandle(pagerNum, baiduData, num, type, keyword)
                var $csdnDl = $(".wrap-main-list li[data-page='" + pagerNum + "']").not('.baidu_list')
                var getLastChild = function () {
                    var searchList = $(".wrap-main-list li[data-page='" + pagerNum + "']")
                    // 防止出现 678 12345, 应该为
                    if (searchList.eq($csdnDl.length).length) {
                        return searchList.eq(searchList.length - 1)
                    } else {
                        return $csdnDl.eq($csdnDl.length - 1)
                    }
                }
                $baiduData.dom && $baiduData.dom.forEach(function (curBaiduGroup, index) {
                    var insertDom = null;
                    if (flag) { // 百度在前
                        if (index === 0) {
                            curBaiduGroup.children().insertBefore($csdnDl.eq(0));
                        } else {
                            insertDom = $csdnDl.length - num * index >= 0 ? $csdnDl.eq(index * num - 1) : getLastChild()
                            curBaiduGroup.children().insertAfter(insertDom);
                        }
                    } else { // CSDN在前
                        insertDom = $csdnDl.length - (index + 1) * num >= 0 ? $csdnDl.eq((index + 1) * num - 1) : getLastChild();
                        curBaiduGroup.children().insertAfter(insertDom);
                    }
                })
            },
            baiduDataInsert: function (pageNo, data, keyword, type) {
                type = type || "all";
                data = data.filter(function (item) {
                    if (!item.dispUrl) {
                        item.dispUrl = ''
                    }
                    return item.dispUrl.indexOf('m.blog.csdn.net') === -1
                });
                _fn.insertFunc(pageNo, data, _fn._CFG.js_insert_count, _fn._CFG.js_insert_first, type, keyword)

                setTimeout(function () {
                    window.csdn && window.csdn.report && window.csdn.report.viewCheck()
                }, 100)
            },
            loadBaiduData: function (keyword, domain, pageNo) {
                if (isContainBaidu != "10") {
                    _fn.setCsdnDataObj();
                    console.log("baiduSearch: pageNo:" + pageNo + " keyword:" + keyword + " domain:" + domain);
                    if (domain) {
                        domain = _types[domain];
                    } else {
                        domain = null;
                    }
                    try {
                        csdn.baiduSearch(keyword, function (data) {
                            console.log(data);
                            var newData = _fn.distinct(data);
                            newData = _fn.uniqueBaidu(newData);
                            _fn.baiduDataInsert(pageNo, newData, keyword, domain);
                        }, domain, pageNo);
                    } catch (e) {
                        //console.log(e)
                    }
                }
            },
            setCsdnDataObj: function () {
                $('.wrap-main-list .main-ul li').each(function () {
                    var url = $(this).find('a.blog-title').attr('href')
                    if (url) {
                        _csdnDataObj[url] = true;
                    }
                    window.listUrls = _csdnDataObj;
                });
            }
        }


        let timer = null;
        let count = 0;
        timer = setInterval(function () {
            count++;
            let _cse = csdn.cse;
            let _ssr = csdn.cse.setSearchRange;
            if ((_cse && _ssr) || count > 10) {
                clearInterval(timer)
                _fn.loadBaiduData(mod_keywords, '', mod_page);
            }
        }, 1000)

        window.fillColor = function (color) {
            color = color || '#44ff8f'
            $("li.baidu_list").each(function (index, item) {
                $(item).css("background", color)
            })
        }


    })

</script>
</body>
</html>